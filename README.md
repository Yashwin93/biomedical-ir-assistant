# Biomedical IR Assistant (BM25 vs RM3 vs MeSH)

## ðŸŽ¯ Objective
This project compares three biomedical document retrieval pipelines â€” BM25, RM3 pseudo-relevance feedback, and MeSH-based semantic expansion â€” using PyTerrier and a Streamlit interface. It demonstrates how semantic enrichment improves retrieval effectiveness and provides a reproducible evaluation workflow.

## Features
- BM25 baseline and RM3 expansion pipelines
- MeSH-based query enrichment using biomedical vocabulary
- Metadata-aware retrieval (docno, title, abstract)
- Streamlit UI with method toggle, top-10 abstract display, and Excel download
- Evaluation metrics: MAP, Precision@10, nDCG@10
- Modular codebase with clear separation of concerns

## Quick start (<= 6 minutes)
1. Clone repo:
   git clone https://github.com/Yashwin93/biomedical-ir-assistant.git
   cd biomedical-ir-assistant

2. Create virtual environment (Windows):
   python -m venv .venv
   .\.venv\Scripts\activate

3. Install dependencies:
   pip install -r requirements.txt

4. Build tiny demo index (first run):
   python -m src.indexing

5. Run the app:
   streamlit run src/app.py

## Code structure
BIOMEDICAL-IR-ASSISTANT/
â”‚
â”œâ”€â”€ .venv/                        # Virtual environment (excluded from GitHub)
â”‚
â”œâ”€â”€ app.py                        # Streamlit UI (outside src)
â”‚
â”œâ”€â”€ data/                         # Raw and processed data
â”‚   â”œâ”€â”€ xml/                      # Original XML files (optional)
â”‚   â”œâ”€â”€ pmc_oa.tar.gz             # Compressed PMC OA dataset
â”‚   â”œâ”€â”€ pmc_subset.json           # Parsed biomedical documents
â”‚   â”œâ”€â”€ queries.csv               # Query set for evaluation
â”‚   â”œâ”€â”€ qrels_completed.csv       # Relevance judgments
|   â”œâ”€â”€ README.md                 # Data Documentation â€” Biomedical IR Assistant  
â”‚
â”œâ”€â”€ index_dir/                    # Terrier index files
â”‚   â”œâ”€â”€ data.direct.bf
â”‚   â”œâ”€â”€ data.document.fsarrayfile
â”‚   â”œâ”€â”€ data.inverted.bf
â”‚   â”œâ”€â”€ data.lexicon.fsomapfile
â”‚   â”œâ”€â”€ data.meta.idx
â”‚   â””â”€â”€ ...                       # Other index artifacts
â”‚
â”œâ”€â”€ results/                      # Evaluation outputs
â”‚   â”œâ”€â”€ evaluation_metrics.csv
â”‚   â”œâ”€â”€ evaluation_chart.png
â”‚
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate.py               # Evaluation logic
â”‚   â”œâ”€â”€ indexing.py               # Index builder
â”‚   â”œâ”€â”€ main.py                   # BM25, RM3, MeSH pipelines
â”‚   â”œâ”€â”€ utils.py                  # Path helpers
â”‚
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ requirements.txt              # Python dependencies


## Data
All data files are stored in the data/ folder.
- pmc_subset.json contains biomedical abstracts and metadata.
- queries.csv defines search intents.
- qrels_completed.csv contains relevance judgments.
See data/README.md for schema and provenance details

## Evaluation
To run evaluation:
python src/evaluate.py

This will compute MAP, Precision@10, and nDCG@10 across all pipelines and save results to results/evaluation_metrics.csv. A chart is also generated for visual comparison.

- Full datasets and large indexes are excluded. Regenerate locally via indexing.py.
- RM3 expands queries using top-ranked BM25 documents, so results will differ by design.
- MeSH expansion enriches queries with biomedical vocabulary for improved recall.

## References
See project_report.pdf for academic references and visuals supporting this implementation.


## Notes
- Full datasets and large indexes are excluded. Regenerate locally via indexing.py.
- RM3 reorders results by expanding queries using top-ranked BM25 docs â€” so BM25 and RM3 will differ by design.

