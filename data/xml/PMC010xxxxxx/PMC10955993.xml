<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Npj Ment Health Res</journal-id><journal-id journal-id-type="iso-abbrev">Npj Ment Health Res</journal-id><journal-title-group><journal-title>NPJ Mental Health Research</journal-title></journal-title-group><issn pub-type="epub">2731-4251</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">38609509</article-id><article-id pub-id-type="pmc">PMC10955993</article-id>
<article-id pub-id-type="publisher-id">40</article-id><article-id pub-id-type="doi">10.1038/s44184-023-00040-z</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>A systematic review on automated clinical depression diagnosis</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5637-6635</contrib-id><name><surname>Mao</surname><given-names>Kaining</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6527-2221</contrib-id><name><surname>Wu</surname><given-names>Yuqi</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7925-3729</contrib-id><name><surname>Chen</surname><given-names>Jie</given-names></name><address><email>jc65@ualberta.ca</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0160cpw27</institution-id><institution-id institution-id-type="GRID">grid.17089.37</institution-id><institution>Department of Electrical and Computer Engineering, </institution><institution>University of Alberta, </institution></institution-wrap>Edmonton, AB T6G 2R3 Canada </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>2</volume><elocation-id>20</elocation-id><history><date date-type="received"><day>10</day><month>3</month><year>2023</year></date><date date-type="accepted"><day>27</day><month>9</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2023</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Assessing mental health disorders and determining treatment can be difficult for a number of reasons, including access to healthcare providers. Assessments and treatments may not be continuous and can be limited by the unpredictable nature of psychiatric symptoms. Machine-learning models using data collected in a clinical setting can improve diagnosis and treatment. Studies have used speech, text, and facial expression analysis to identify depression. Still, more research is needed to address challenges such as the need for multimodality machine-learning models for clinical use. We conducted a review of studies from the past decade that utilized speech, text, and facial expression analysis to detect depression, as defined by the Diagnostic and Statistical Manual of Mental Disorders (DSM-5), using the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guideline. We provide information on the number of participants, techniques used to assess clinical outcomes, speech-eliciting tasks, machine-learning algorithms, metrics, and other important discoveries for each study. A total of 544 studies were examined, 264 of which satisfied the inclusion criteria. A database has been created containing the query results and a summary of how different features are used to detect depression. While machine learning shows its potential to enhance mental health disorder evaluations, some obstacles must be overcome, especially the requirement for more transparent machine-learning models for clinical purposes. Considering the variety of datasets, feature extraction techniques, and metrics used in this field, guidelines have been provided to collect data and train machine-learning models to guarantee reproducibility and generalizability across different contexts.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Psychiatric disorders</kwd><kwd>Biomedical engineering</kwd><kwd>Electrical and electronic engineering</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004543</institution-id><institution>China Scholarship Council</institution></institution-wrap></funding-source><award-id>202000810031</award-id><award-id>202308180002</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2023</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Major depressive disorder is one of the most common diseases globally. It is estimated that 3.8% of the population is impacted by depression, and ~280 million people suffer from depression. The economic impact of depression is significant, even when compared to other medical conditions, such as cancer, cardiovascular diseases, diabetes, and respiratory diseases<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Conventional methods for assessing and monitoring depression involve semi-structured interviews between patients and healthcare workers, which can be subjective and affected by bias (i.e., deemphasized or exaggerated symptoms), cognitive limitations (i.e., memory errors) and social stigma. The need for objective depression diagnosis, routine symptom monitoring, and timely treatment is widely recognized in the medical community. However, many people with depression have difficulty accessing psychological healthcare services due to geographical and financial barriers. As the World Health Organization (WHO) has reported, over 75% of individuals with depression in low and mid-income countries do not receive qualified psychotherapy<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. The shortage of well-trained healthcare professionals and the social stigma surrounding depression are major barriers patients face when seeking help. An automated depression assessment tool would assist in objective diagnosis and remote care to improve the quality of mental healthcare.</p><p id="Par3">One potential solution to improve the objectivity of depression assessments and the quality of mental health services is to enhance mental health-related data collection and analysis via sensors and advanced machine-learning algorithms. Sensors that measure biological signals such as electrocardiogram, electroencephalogram, heart rate, and skin conductance may also be used to monitor biomarkers of depression<sup><xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR15">15</xref></sup>. Researchers have developed models to detect depression and other psychological disorders by extracting features from interview videos, or audio recordings<sup><xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR28">28</xref></sup>. Toolkits such as OpenFace can extract facial landmarks, action units, face orientation, and eye gaze<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Other modalities, such as neuroimaging data, have been used to predict the presence of Schizophrenia<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Still, here we will focus only on non-neuroimaging modalities, such as acoustic, semantic, and facial modalities. Lastly, features extracted from social media and transcribed audio recordings have also been used to detect depression and stress<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. These previous studies provide valuable insights into new approaches for improved depression assessment.</p><p id="Par4">A machine-learning algorithm for automated depression assessment can provide several benefits, including (a) supporting clinicians in making accurate diagnoses and providing effective treatment; (b) identifying individuals at risk for depression before they seek treatment from mental healthcare clinics and (c) tracking symptoms over time, both during and after treatment. These benefits are further discussed in the following:</p><p id="Par5">One of the primary advantages of automated depression assessment is that it can help overcome barriers preventing people from receiving proper diagnoses and treatment for mental health issues in a timely manner. Mohr et al. reported that the main barriers patients face when seeking help are stigma, lack of motivation, time or availability constraints, and cost<sup><xref ref-type="bibr" rid="CR33">33</xref>&#x02013;<xref ref-type="bibr" rid="CR35">35</xref></sup>. Automated diagnostic tools could allow depression patients who have not yet sought help from healthcare professionals to evaluate their mental states remotely and receive online support from healthcare workers. In addition, these tools can be designed to customize treatment based on an individual&#x02019;s specific symptoms, improving treatment efficacy<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>. These systems can also be utilized for mental disorder screening in various settings, such as universities, the military, and basic healthcare facilities.</p><p id="Par6">Automated depression assessment systems can play an important role in helping doctors diagnose and make decisions related to depression. Diagnosing depression can be difficult because symptoms may be episodic and multiple disorders may occur simultaneously, as demonstrated by the low inter-rater reliability<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> and test&#x02013;retest reliability scores<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> in major depressive disorder diagnosis. In addition, it is also challenging to train a model to recognize any mental disorder, given that patients may have multiple disorders that present overlapping symptoms simultaneously. For example, over 50% of depression or anxiety cases co-occur with drug abuse or post-traumatic stress disorder (PTSD)<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Because of these challenges, researchers have proposed prediction tools for suicidal thoughts and behaviors across many mental disorders (for a review, see ref. <sup><xref ref-type="bibr" rid="CR41">41</xref></sup>). The Research Domain Criteria, created by the National Institute of Mental Health, assists in distinguishing diagnoses and symptoms<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Consequently, we can train models to predict the probability of different mental disorders to assist in both diagnosis and early interventions. Furthermore, prediction tools can enable customized treatment plans based on multimodality features (genetic, behavioral, neuroimaging)<sup><xref ref-type="bibr" rid="CR43">43</xref>&#x02013;<xref ref-type="bibr" rid="CR45">45</xref></sup>. As a result, automated depression assessment models improve the efficiency of the healthcare systems, lower costs, and make treatment plans more customizable.</p><p id="Par7">Automated depression assessment models can also enhance mental healthcare by allowing more frequent monitoring of symptoms, even in real time. Real-time monitoring enables individuals at risk for depression to be reminded to seek mental healthcare. Additionally, depression symptoms can change between healthcare appointments<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Real-time monitoring can detect important signs related to suicidal or self-harm thoughts and conduct online psychotherapy. With real-time monitoring, patients and clinicians can monitor symptoms, conduct early intervention, and tailor treatment plans in a more personalized and timely manner.</p><p id="Par8">Despite the potential benefits, these advantages have yet to be fully realized. Previous studies have relied on small, unrepresentative and non-clinical datasets, which are inadequate as they do not reflect real-world clinical situations. Most clinical datasets are limited in size and data type, for example, recorded in noise-controlled rooms, spoken in English, and restricted to adult participants. Moreover, models can be biased toward differences in the demographic characteristics of patients and healthy controls. For example, the model may tend to assign lower depression likelihood in male subjects because fewer male subjects have depression in the training set<sup><xref ref-type="bibr" rid="CR47">47</xref>&#x02013;<xref ref-type="bibr" rid="CR51">51</xref></sup>. Another significant issue in developing high-performance models is that their output depression probabilities are difficult to explain, limiting their clinical applications. This is because machine-learning models are often considered &#x0201c;black boxes&#x0201d;, meaning that their decision-making process is not easily interpretable by humans. While high-performing models can achieve high accuracy in depression prediction, the lack of transparency in how they made such decisions can be a barrier to clinical use. Thus, an explanation of the output probabilities of high-performance models is crucial for developing interpretable automated depression assessment models.</p><p id="Par9">The study of speech patterns has been a research topic in identifying indicators of mental disorders since the 1920s. Emil Kraepelin, the founder of modern scientific psychiatry, reported that the voices of depressed patients were lower in pitch, sound intensity, and speech rate, and instead tend to be monotonous and hesitant, with shuttering and whispering<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. Moreover, acoustic features can be extracted across different languages, which is important for languages without pre-trained natural language processing models. In addition, speech recordings can be easily collected with smartphones and laptop computers instead of complex and costly equipment. With the advancements in speech recognition, especially its application for electronic medical records, speech recording will become more accessible for research purposes.</p><p id="Par10">In research settings, depression is conventionally assessed using clinical depression rating scales. However, in clinical practice, the official psychiatric diagnosis is typically determined through a clinical interview, which may be semi-structured, with rating scales used as supplementary data to inform the diagnosis. However, these rating scales have limitations, as responses can be influenced by factors such as the patient&#x02019;s emotional state, relationship with the clinician, and patient self-bias (e.g., participants may be more likely to exaggerate their symptoms)<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. With the advancement of machine-learning applied to text data from social media, new methods have emerged to address these limitations. Social media such as Twitter, Facebook, and Reddit provide a wealth of information about individuals&#x02019; feelings, thoughts and activities. Machine learning, especially text mining and sentiment analysis techniques, have become more accurate and intelligent, aiding mental healthcare providers in detecting depression. For instance, Pirina et al. and Yates et al. proposed machine-learning models to screen depression symptoms based on text data from Reddit<sup><xref ref-type="bibr" rid="CR54">54</xref>,<xref ref-type="bibr" rid="CR55">55</xref></sup>. Researchers have recently developed models that utilize semantic features and other information extracted from social media platforms for detecting depression<sup><xref ref-type="bibr" rid="CR56">56</xref>&#x02013;<xref ref-type="bibr" rid="CR60">60</xref></sup>.</p><p id="Par11">Several studies have been published on depression detection via facial image processing. The Audio/Visual Emotion Challenge and Workshop (AVEC) depression sub-challenge published papers with encouraging results on depression detection. Jan et al. achieved plausible results with a Motion History Histogram to extract Local Binary Pattern (LBP) and Edge Orientation Histogram (EOH) for depression identification<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. Other features, such as head posture, blink rate, and eye gaze, are also reported to be effective in depression detection<sup><xref ref-type="bibr" rid="CR62">62</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup>. Depressive individuals tend to display less nodding, avoid eye contact, and lower their heads more frequently than healthy individuals. Alghowinem et al. developed a support vector machine (SVM) to recognize depression and concluded that the head movements of depressed individuals are different from those of healthy controls<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>.</p><p id="Par12">This paper reviews recent research on the use of computational methods to predict major depressive disorder using acoustic, semantic, and facial features. This review is unique in that it uses the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines for extensive and rigorous evaluation of the latest research findings. By synthesizing and analyzing recently published data, this review offers important insights into the current state of the field and identifies key areas for future research. We would like to show how multimodal features could enhance mental healthcare, which provides insights into establishing connections between psychiatry and computing. Consequently, this review aims to (a) summarize results from previous publications using acoustic, semantic, and facial features to detect major depressive disorder; (b) characterize psychiatric disorders by identifying significant differences in acoustic, semantic, and visual features; (c) associate these multimodal features to depression symptoms and behaviors; and (d) summarize the challenges of automated depression assessment, make suggestions for future data collection and model training with better reproducibility and performance. Therefore, we propose that artificial intelligence may be a key tool for improving the assessment and treatment of depression through automated approaches.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Inclusion criteria and literature search</title><p id="Par13">The PRISMA guidelines were followed in this literature review, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Our goal was to search for articles published in the last ten years that included artificial intelligence methods for predicting the presence or severity of major depressive disorder by analyzing acoustic, semantic, and facial landmarks. Google Scholar was used as the search engine for articles from 2012 to the present, queried between July 20, 2022, and May 20, 2023, excluding case studies, studies that solely used perceptual evaluation of speech, studies without a control group or clinical depression rating scales, non-peer-reviewed preprint and theses, and articles published before 2022 and having fewer citations than years of publication (e.g., articles published in 2019 with three citations, or articles published in 2017 with five citations would be included). We excluded certain articles that lacked comprehensive methodology or detailed results. In addition, we encountered cases where articles were published in both journals and conference proceedings, covering similar topics, methods, and results. Furthermore, some articles only focused on proposing methods for feature extraction without incorporating the training of models for depression detection. The search terms used to find relevant articles were: &#x0201c;allintitle: ((&#x0201c;depression&#x0201d; OR &#x0201c;major depressive disorder&#x0201d;) + (acoustic OR acoustical OR speech OR voice OR vocal OR audio OR pitch OR prosody OR prosodic OR vowel) + (automated OR behavioral OR measures OR diagnosis)).&#x0201d; Articles related to depression caused by Parkinson&#x02019;s Disease, autism, and substance overdose disorders were excluded. Replacement of the acoustic feature with the semantic feature and facial landmarks in the command resulted in the following search term with associated features: &#x0201c;allintitle: ((&#x0201c;depression&#x0201d; OR &#x0201c;major depressive disorder&#x0201d;) + (semantic OR text OR interview OR transcript OR social media) + (automated OR behavioral OR measures OR diagnosis))&#x0201d; and &#x0201c;allintitle: ((&#x0201c;depression&#x0201d; OR &#x0201c;major depressive disorder&#x0201d;) + (facial expression OR visual OR facial features OR facial landmarks OR facial muscles)+ (automated OR behavioral OR measures OR diagnosis)).&#x0201d;<fig id="Fig1"><label>Fig. 1</label><caption><p>PRISMA flow diagram of study inclusion and exclusion criteria in this review.</p></caption><graphic xlink:href="44184_2023_40_Fig1_HTML" id="d33e349"/></fig></p><p id="Par14">Information extraction was performed by reading the title, abstract, and conclusion. The following information was synthesized from each article: mental disorders, number of subjects, age range, optimal model, best metrics, type of validation, and predictive features.</p></sec><sec id="Sec4"><title>Reporting summary</title><p id="Par15">Further information on research design is available in the <xref rid="MOESM1" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p></sec></sec><sec id="Sec5" sec-type="results"><title>Results</title><sec id="Sec6"><title>Summary of results</title><p id="Par16">In total, 264 studies were included in the review. Table <xref rid="Tab1" ref-type="table">1</xref> summarizes the search results. Synthesized information can be found online <ext-link ext-link-type="uri" xlink:href="https://bit.ly/3DBQtZk">https://bit.ly/3DBQtZk</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://bit.ly/43Q6Yvy">https://bit.ly/43Q6Yvy</ext-link>, and <ext-link ext-link-type="uri" xlink:href="https://bit.ly/44IKaPv">https://bit.ly/44IKaPv</ext-link>, which can be extended by adding new studies on a blank row or fields on a blank column. Previous review and datasets-only articles were included in this study but are not included in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Summary of literature review results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Modality</th><th>Articles</th><th>Median dataset size (range)</th><th>Clinical assessment</th><th>Predictive models</th></tr></thead><tbody><tr><td>Acoustic</td><td>140</td><td>189</td><td>36</td><td>140</td></tr><tr><td>Semantic</td><td>99</td><td>1046</td><td>2</td><td>81</td></tr><tr><td>Facial landmarks</td><td>25</td><td>49</td><td>16</td><td>21</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec7"><title>Predictive acoustic features in major depressive disorder</title><p id="Par17">With the publicly available code provided by Low et al.<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>, we created Fig. <xref rid="Fig2" ref-type="fig">2</xref>, which provides a synthesis of the acoustic features investigated using machine learning. The table shows the acoustic features found to be statistically different between the group with a mental disorder and the healthy control group or highly correlated with a diagnostic rating scale. Each cell in Fig. <xref rid="Fig2" ref-type="fig">2</xref> represents the correlation between a specific acoustic feature and depression. For example, an acoustic feature that correlates positively with the disorder severity would be marked with a red dot, a negative correlation with a blue dot, and a nonsignificant feature with a gray dot.<fig id="Fig2"><label>Fig. 2</label><caption><title>Synthesis of acoustic feature analysis in major depressive disorder.</title><p>Acoustic features are sorted, such as vocal fold source features (blue), vocal tract filter features (red), spectral features (purple), and features related to prosody or melody (black). Features that are significantly higher in a psychiatric group than healthy controls or that correlate positively with the depression level receive a score of 1 (red), features that are lower or correlate negatively receive a score of -1 (blue), and nonsignificant or contradicting findings receive a score of 0 (gray). Features not studied in any studies are blank.</p></caption><graphic xlink:href="44184_2023_40_Fig2_HTML" id="d33e462"/></fig></p><p id="Par18">Table <xref rid="Tab2" ref-type="table">2</xref> provides an overview of the key findings from previous studies on automated depression detection using acoustic features. One common finding among the studies is the relationship between acoustic volume and depression. Cummins et al.<sup><xref ref-type="bibr" rid="CR66">66</xref>,<xref ref-type="bibr" rid="CR67">67</xref></sup> found that as the level of depression increases, the acoustic volume significantly decreases, indicating a potential acoustic marker for depression. Similarly, Harati et al.<sup><xref ref-type="bibr" rid="CR68">68</xref></sup> reported that individuals with depression tend to have lower voices compared to the control group.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Predictive acoustic features in prior research publications.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Study</th><th>Key finding</th></tr></thead><tbody><tr><td>Cummins et al.<sup><xref ref-type="bibr" rid="CR66">66</xref></sup></td><td>Decreased acoustic volume</td></tr><tr><td/><td>More concentrated MFCC space</td></tr><tr><td>Cummins et al.<sup><xref ref-type="bibr" rid="CR69">69</xref></sup></td><td>Gender-dependent formant features</td></tr><tr><td>Morales et al.<sup><xref ref-type="bibr" rid="CR70">70</xref></sup></td><td>Fundamental frequency</td></tr><tr><td/><td>Pronoun use and negatively valenced words</td></tr><tr><td>Scherer et al.<sup><xref ref-type="bibr" rid="CR118">118</xref></sup></td><td>Tenser voice</td></tr><tr><td>Vicsi et al.<sup><xref ref-type="bibr" rid="CR71">71</xref></sup></td><td>Jitter and shimmer values of vowels</td></tr><tr><td/><td>First and second formant frequencies</td></tr><tr><td>Marmor et al.<sup><xref ref-type="bibr" rid="CR150">150</xref></sup></td><td>Seeking care for voice problem</td></tr><tr><td>Cummins et al.<sup><xref ref-type="bibr" rid="CR67">67</xref></sup></td><td>Acoustic volume</td></tr><tr><td/><td>Probabilistic acoustic volume slope</td></tr><tr><td>Harati et al.<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></td><td>Lower voices</td></tr><tr><td/><td>Variance in voice pitch</td></tr><tr><td>Kiss et al.<sup><xref ref-type="bibr" rid="CR72">72</xref></sup></td><td>Articulation rate</td></tr><tr><td/><td>Speech rate</td></tr><tr><td/><td>Pause lengths</td></tr><tr><td/><td>Formant frequency</td></tr><tr><td>Stasak et al.<sup><xref ref-type="bibr" rid="CR73">73</xref></sup></td><td>Use phonemes that require less effort</td></tr><tr><td/><td>Articulatory precision</td></tr></tbody></table></table-wrap></p><p id="Par19">Another notable finding is the influence of gender on acoustic features related to depression. Cummins et al.<sup><xref ref-type="bibr" rid="CR69">69</xref></sup> proposed gender-dependent formant features that outperformed acoustic-only features in depression detection. This suggests that gender-specific acoustic characteristics may play a role in accurately detecting depression. Fundamental frequency (<italic>F</italic><sub>0</sub>) was found to be negatively correlated with depression level by Morales et al.<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>. This finding indicates that changes in <italic>F</italic><sub>0</sub> could serve as an indicator of depression. In addition, Vicsi et al.<sup><xref ref-type="bibr" rid="CR71">71</xref></sup> discovered that depressed individuals exhibited higher jitter and shimmer values in vowel production, along with lower first and second formant frequencies. These acoustic features may serve as potential biomarkers for depression detection. Kiss et al.<sup><xref ref-type="bibr" rid="CR72">72</xref></sup> highlighted the importance of speech rate, articulation rate, pause lengths, and formant frequency in detecting depression. They found that these acoustic features differed between individuals with depression and the control group, suggesting potential utility in automated depression detection. Stasak et al.<sup><xref ref-type="bibr" rid="CR73">73</xref></sup> proposed that depressed individuals tend to use phonemes that require less effort and demonstrate decreased articulatory precision. These findings indicate that analyzing articulatory characteristics could provide valuable insights for depression detection.</p><p id="Par20">It is important to note that these findings are based on previous studies and should be interpreted within the context of their respective methodologies and limitations. Further research is needed to validate and refine the use of these acoustic features for automated depression detection.</p></sec><sec id="Sec8"><title>Predictive semantic features in major depressive disorder</title><p id="Par21">This summary reviews studies on automated depression detection using language cues. Previous studies identified depression based on clinician diagnosis, patient self-reported mental status and online forum memberships. Clinician diagnosis means that depression levels were determined by a clinician based on interview transcripts or online posts. Among the 99 studies using language cues, only two identified depression based on the clinician diagnosis<sup><xref ref-type="bibr" rid="CR74">74</xref>,<xref ref-type="bibr" rid="CR75">75</xref></sup>, while 77 studies used self-reported depression rating scales. The remaining 20 studies did not report which criteria were used to determine depression levels.</p><sec id="Sec9"><title>Social media and depression</title><p id="Par22">Table <xref rid="Tab3" ref-type="table">3</xref> provides a summary of key findings from various studies examining the relationship between social media usage and depression. These studies employ a range of techniques and models to improve the detection and understanding of depression based on social media data. Several studies highlight increased social media usage among individuals with depression. Various advanced models, such as GRU models with knowledge-aware dot-product attention<sup><xref ref-type="bibr" rid="CR76">76</xref></sup> and DeepBoSE<sup><xref ref-type="bibr" rid="CR77">77</xref></sup>, demonstrate improved performance in depression detection compared to conventional methods. In addition, semantic mapping of emoticons<sup><xref ref-type="bibr" rid="CR78">78</xref></sup> and the application of semantic role labeling<sup><xref ref-type="bibr" rid="CR79">79</xref></sup> are proposed as techniques to enhance detection accuracy. These findings highlight the potential of leveraging machine learning and natural language processing techniques to gain insights into mental health conditions through social media data.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Exploring the predictive relationship between social media usage and depression.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Author</th><th>Main findings</th></tr></thead><tbody><tr><td>Hartanto et al.<sup><xref ref-type="bibr" rid="CR151">151</xref></sup></td><td>Social media usage increases among depressive individuals</td></tr><tr><td>Figueredo et al.<sup><xref ref-type="bibr" rid="CR78">78</xref></sup></td><td>Semantic mapping of emoticons improves the performance.</td></tr><tr><td>Stankevich et al.<sup><xref ref-type="bibr" rid="CR79">79</xref></sup></td><td>Future work needs to involve applying semantic role labeling to obtain better results.</td></tr><tr><td>Lara et al.<sup><xref ref-type="bibr" rid="CR77">77</xref></sup></td><td>DeepBoSE outperforms conventional Bag-of-Features(BoF) representations.</td></tr><tr><td>Hussain et al.<sup><xref ref-type="bibr" rid="CR152">152</xref></sup></td><td>Proposed depression lexicons that distinguish depressive individuals.</td></tr><tr><td>Ramiandrisoa et al.<sup><xref ref-type="bibr" rid="CR86">86</xref></sup></td><td>Analyzing users&#x02019; social signals could be considered for further analysis.</td></tr><tr><td>Liaw et al.<sup><xref ref-type="bibr" rid="CR153">153</xref></sup></td><td>Topic modeling features such as liked tweets can be useful.</td></tr><tr><td>Guo et al.<sup><xref ref-type="bibr" rid="CR84">84</xref></sup></td><td>Fused the lexical features using a correlation-based metric to enhance prediction effectiveness.</td></tr><tr><td>Cui et al.<sup><xref ref-type="bibr" rid="CR83">83</xref></sup></td><td>Capture deep emotional information from the input embeddings with a pre-trained TextCNN.</td></tr><tr><td>Zogan et al.<sup><xref ref-type="bibr" rid="CR87">87</xref></sup></td><td>The model captures semantic features from user timelines for depression detection.</td></tr><tr><td>Tlatelpa et al.<sup><xref ref-type="bibr" rid="CR80">80</xref></sup></td><td>User characteristics and sentiment analysis improved depression detection performance.</td></tr><tr><td>Cha et al.<sup><xref ref-type="bibr" rid="CR82">82</xref></sup></td><td>Proposed lexicon features for depression detection.</td></tr><tr><td>Primack et al.<sup><xref ref-type="bibr" rid="CR154">154</xref></sup></td><td>Using multiple social media platforms is associated with depression.</td></tr><tr><td>Primack et al.<sup><xref ref-type="bibr" rid="CR155">155</xref></sup></td><td>Social media use is associated with the development of depression.</td></tr><tr><td>Vedula et al.<sup><xref ref-type="bibr" rid="CR156">156</xref></sup></td><td>Depressed users exhibit reduced online activities, increased negative sentiment, and self-focused pronoun usage.</td></tr><tr><td>Nesi et al.<sup><xref ref-type="bibr" rid="CR157">157</xref></sup></td><td>More frequent negative emotional reactions to social media are linked to more severe depression symptoms, especially among female subjects.</td></tr><tr><td>Thorisdottir et al.<sup><xref ref-type="bibr" rid="CR158">158</xref></sup></td><td>Time spent on social media has a stronger relationship with emotional distress among female subjects.</td></tr><tr><td>Ghosh et al.<sup><xref ref-type="bibr" rid="CR159">159</xref></sup></td><td>Depressed users frequently use negative words and mostly post late at night, in addition to increased use of personal pronouns and sharing personal events.</td></tr><tr><td>Aragon et al.<sup><xref ref-type="bibr" rid="CR160">160</xref></sup></td><td>Representations based on fine-grained emotions can more comprehensively capture users experiencing depression.</td></tr><tr><td>Puukko et al.<sup><xref ref-type="bibr" rid="CR161">161</xref></sup></td><td>Depressed individuals increasingly use active social media during early and late adolescence.</td></tr><tr><td>Robinson et al.<sup><xref ref-type="bibr" rid="CR162">162</xref></sup></td><td>Depressed individuals are more likely to compare themselves to others and dislike being tagged in self-perceived unflattering pictures.</td></tr><tr><td>Choudhury et al.<sup><xref ref-type="bibr" rid="CR163">163</xref></sup></td><td>Social media can provide valuable indicators of depression onset, including decreased social activities, increased negative emotions, focus on personal and medical issues, and more frequent expressions of religious involvement.</td></tr></tbody></table></table-wrap></p><p id="Par23">Furthermore, the studies presented in the table emphasize the importance of considering multimodal data, user characteristics, and sentiment analysis for a comprehensive understanding of depression<sup><xref ref-type="bibr" rid="CR80">80</xref>,<xref ref-type="bibr" rid="CR81">81</xref></sup>. They also propose the use of lexicon features and emotional information capture to improve depression detection<sup><xref ref-type="bibr" rid="CR82">82</xref>,<xref ref-type="bibr" rid="CR83">83</xref></sup>. The fusion of lexical features and the development of bipolar feature vectors demonstrate promising results in enhancing prediction effectiveness<sup><xref ref-type="bibr" rid="CR84">84</xref>,<xref ref-type="bibr" rid="CR85">85</xref></sup>. In addition, the studies suggest the potential of analyzing social signals and user timelines to capture semantic features for depression detection<sup><xref ref-type="bibr" rid="CR86">86</xref>,<xref ref-type="bibr" rid="CR87">87</xref></sup>.</p><p id="Par24">It is important to note that while these studies offer valuable insights, there are still ethical considerations that need to be addressed. Responsible data usage, privacy protection, and potential biases are crucial aspects that should be carefully considered when developing AI-based depression detection tools using social media data. Furthermore, future research should focus on the validation and generalization of these findings across diverse populations and cultures.</p><p id="Par25">By synthesizing these findings (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>), we have gained a deeper understanding of the potential of social media data in detecting and understanding depression. These insights can inform the development of effective mental health interventions, improve clinical practice, and contribute to the responsible and ethical usage of AI in this domain.<fig id="Fig3"><label>Fig. 3</label><caption><title>Synthesis of semantic feature analysis in major depressive disorder.</title><p>Features that are significantly higher in a psychiatric group than healthy controls or that correlate positively with the depression level receive a score of 1 (red), features that are lower or correlate negatively receive a score of &#x02212;1 (blue), and findings without reporting their changes receive a score of 0 (gray). Features not studied in any studies are blank.</p></caption><graphic xlink:href="44184_2023_40_Fig3_HTML" id="d33e947"/></fig></p></sec><sec id="Sec10"><title>Problematic social media use</title><p id="Par26">Table <xref rid="Tab4" ref-type="table">4</xref> provides a summary of the main findings from studies examining the association between problematic social media use and depression, including several studies<sup><xref ref-type="bibr" rid="CR88">88</xref>&#x02013;<xref ref-type="bibr" rid="CR90">90</xref></sup> which consistently found a strong correlation. In addition, findings from refs. <sup><xref ref-type="bibr" rid="CR91">91</xref>&#x02013;<xref ref-type="bibr" rid="CR93">93</xref></sup> indicate that over-sharing and stressed posting on social media is associated with depression. These findings collectively highlight the importance of recognizing problematic social media use as a potential risk factor for depressive symptoms. The results underscore the need for interventions and guidelines to promote healthier social media habits, particularly in vulnerable populations such as adolescents and university students. It is important to acknowledge the limitations of the studies summarized in Table <xref rid="Tab4" ref-type="table">4</xref>, including factors like sample size, study design, and generalizability. Future research should consider longitudinal studies to examine the long-term effects of excessive social media use on depression and explore the underlying mechanisms of this association.<table-wrap id="Tab4"><label>Table 4</label><caption><p>The impact of problematic social media use on mental health.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Author</th><th>Main findings</th></tr></thead><tbody><tr><td>Cunningham et al.<sup><xref ref-type="bibr" rid="CR88">88</xref></sup></td><td>Future research should focus on individuals with problematic social media use.</td></tr><tr><td>Shensa et al.<sup><xref ref-type="bibr" rid="CR89">89</xref></sup></td><td>Problematic social media use is strongly associated with depressive symptoms.</td></tr><tr><td>Woods et al.<sup><xref ref-type="bibr" rid="CR90">90</xref></sup></td><td>Adolescents who abused social media have a higher depression risk.</td></tr><tr><td>Radovic et al.<sup><xref ref-type="bibr" rid="CR91">91</xref></sup></td><td>Over-sharing and stressed posting is associated with depression.</td></tr><tr><td>Ivie et al.<sup><xref ref-type="bibr" rid="CR92">92</xref></sup></td><td>Significant positive correlation between social media use and depressive symptoms.</td></tr><tr><td>Raudsepp et al.<sup><xref ref-type="bibr" rid="CR93">93</xref></sup></td><td>Excessive social media use was correlated with increased symptoms of depression.</td></tr><tr><td>Zhong et al.<sup><xref ref-type="bibr" rid="CR164">164</xref></sup></td><td>Breaks from social media use could have mitigated mental health trauma during the pandemic.</td></tr><tr><td>Haand et al.<sup><xref ref-type="bibr" rid="CR165">165</xref></sup></td><td>Addiction to social media is positively linked to depression.</td></tr><tr><td>Brailovskaia et al.<sup><xref ref-type="bibr" rid="CR166">166</xref></sup></td><td>Depressed individuals often intensively use social media to escape negative moods.</td></tr><tr><td>Jeri-Yabar et al.<sup><xref ref-type="bibr" rid="CR167">167</xref></sup></td><td>Excessive use of social media is associated with depressive symptoms.</td></tr><tr><td>Kircaburun et al.<sup><xref ref-type="bibr" rid="CR168">168</xref></sup></td><td>Social media addiction indirectly affects depression levels.</td></tr></tbody></table></table-wrap></p><p id="Par27">Overall, the findings contribute to our understanding of the complex relationship between social media use and depression, providing valuable insights for mental health promotion and clinical practice.</p></sec><sec id="Sec11"><title>Machine-learning models</title><p id="Par28">Automated depression detection algorithms have been a subject of study by various researchers. Table <xref rid="Tab5" ref-type="table">5</xref> provides a summary of these studies, highlighting machine-learning models and performance metrics employed. Salas et al. conducted a comprehensive review of previous studies that utilized language cues and found that word embedding was the most commonly used linguistic feature extraction method, while the support vector machine was the most prominent machine-learning model<sup><xref ref-type="bibr" rid="CR94">94</xref></sup>. Liu et al. summarized the findings of studies focused on using machine-learning methods to detect depressive symptoms in social media data, highlighting their potential as complementary tools in public mental health practice<sup><xref ref-type="bibr" rid="CR95">95</xref></sup>. Yazdavar et al. developed a semi-supervised statistical model to assess the alignment between depressive symptoms expressed on Twitter and medical findings reported via the Patient Health Questionnaire (PHQ-9), achieving 68% accuracy and 72% precision in identifying clinical depressive symptoms<sup><xref ref-type="bibr" rid="CR74">74</xref></sup>. Zogan et al. introduced a computational approach for automatically identifying depression using a hybrid extractive summarization technique applied to tweets, achieving high recall, precision, and <italic>F</italic><sub>1</sub> score<sup><xref ref-type="bibr" rid="CR75">75</xref></sup>. Arag&#x000f3;n et al. proposed a novel representation called bag of sub-emotions (BoSE) that improved the detection of depression using fine-grained emotions, demonstrating competitive results compared to existing approaches<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. These studies collectively emphasize the significance of linguistic features, word embeddings, and machine-learning models in automated depression detection.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Analyzing the efficacy of machine-learning models in detecting depression through social media data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Author</th><th>Main findings</th></tr></thead><tbody><tr><td>Yazdavar et al.<sup><xref ref-type="bibr" rid="CR74">74</xref></sup></td><td>Achieved 68% accuracy and 72% precision in identifying clinical depressive symptoms using a semi-supervised statistical model.</td></tr><tr><td>Zogan et al.<sup><xref ref-type="bibr" rid="CR75">75</xref></sup></td><td>Proposed a new computational model and achieved a recall of 0.904, precision of 0.909, and F1 score of 0.912.</td></tr><tr><td>Arag&#x000f3;n et al.<sup><xref ref-type="bibr" rid="CR57">57</xref></sup></td><td>Using fine-grained emotions to obtain competitive results in comparison to state-of-the-art approaches.</td></tr><tr><td>Paul et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td>AdaBoost classifier outperformed other methods for depression likelihood assessment.</td></tr><tr><td>Ricard et al.<sup><xref ref-type="bibr" rid="CR169">169</xref></sup></td><td>Leveraging community-generated content from social media can be informative for automated depression assessment.</td></tr><tr><td>Peng et al.<sup><xref ref-type="bibr" rid="CR170">170</xref></sup></td><td>Demonstrated that a multi-kernel support vector machine is the most appropriate approach to identifying depression in individuals using social media.</td></tr><tr><td>Aldarwish et al.<sup><xref ref-type="bibr" rid="CR171">171</xref></sup></td><td>Trained a support vector machine based on term frequency to classify depression levels.</td></tr><tr><td>Chiong et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td>The proposed model effectively determines depression presence via social media posts, even when the training datasets do not contain depression-related words.</td></tr><tr><td>Burdisso et al.<sup><xref ref-type="bibr" rid="CR172">172</xref></sup></td><td>Introduced a general framework for early depression detection with less computational cost and higher interpretability.</td></tr><tr><td>Smys et al.<sup><xref ref-type="bibr" rid="CR173">173</xref></sup></td><td>A machine-learning model consisting of a support vector machine and a naive Bayes model can predict depression in its early stages.</td></tr><tr><td>Bucur et al.<sup><xref ref-type="bibr" rid="CR174">174</xref></sup></td><td>Latent semantic analysis shows a significant difference in writing topics depending on users&#x02019; mental health.</td></tr><tr><td>Kayalvizhi et al.<sup><xref ref-type="bibr" rid="CR175">175</xref></sup></td><td>A word2vec pre-trained word embedding and random forest classifier achieved their best performance with a 0.877 <italic>F</italic><sub>1</sub> score.</td></tr><tr><td>Mann et al.<sup><xref ref-type="bibr" rid="CR176">176</xref></sup></td><td>Fusion model can detect moderate depression or higher with 0.92 recall and 0.69 precision.</td></tr><tr><td>Sadeque et al.<sup><xref ref-type="bibr" rid="CR177">177</xref></sup></td><td>Proposed a system to effectively detect depression using social media content with an accuracy of 88% and <italic>F</italic><sub>1</sub> score of 93%.</td></tr><tr><td>Hussain et al.<sup><xref ref-type="bibr" rid="CR152">152</xref></sup></td><td>Application accurately identifies indicators of depression in Facebook users with 94% accuracy.</td></tr><tr><td>Tadesse et al.<sup><xref ref-type="bibr" rid="CR58">58</xref></sup></td><td>Achieved 91% accuracy and <italic>F</italic><sub>1</sub> score of 93% with a multi-layer perceptron algorithm and combined features.</td></tr><tr><td>Fatima et al.<sup><xref ref-type="bibr" rid="CR178">178</xref></sup></td><td>Achieved an accuracy, recall, and precision of 91.7% using a combination of text-based features and machine-learning techniques.</td></tr><tr><td>Katchapakirin et al.<sup><xref ref-type="bibr" rid="CR179">179</xref></sup></td><td>Facebook behaviors can be used to predict depression levels with an accuracy of 85% and <italic>F</italic><sub>1</sub> score of 88.9%.</td></tr><tr><td>Shen et al.<sup><xref ref-type="bibr" rid="CR180">180</xref></sup></td><td>The model outperformed several baselines by 3% to 10% with an <italic>F</italic><sub>1</sub> score of 85%.</td></tr><tr><td>Li et al.<sup><xref ref-type="bibr" rid="CR181">181</xref></sup></td><td>Proposed a correlation explanation learning algorithm to detect COVID-19-related stress symptoms.</td></tr><tr><td>Lin et al.<sup><xref ref-type="bibr" rid="CR182">182</xref></sup></td><td>Social media use is significantly associated with increased depression risk.</td></tr></tbody></table></table-wrap></p><p id="Par29">However, some weaknesses and variations in the literature have been raised. McCrae et al. conducted a review of studies examining the relationship between social media use and depression symptoms, highlighting the need for comparative analysis due to variations in methods, sample sizes, and results across studies<sup><xref ref-type="bibr" rid="CR96">96</xref></sup>. They also suggested that future research should incorporate longitudinal analysis, as most studies were cross-sectional. Similarly, Heffer et al. found no predictive association between social media use and depressive symptoms over time, challenging the assumption that social media use leads to depressive symptoms<sup><xref ref-type="bibr" rid="CR97">97</xref></sup>. These contrasting perspectives call for further investigation and highlight the complexity of the relationship between social media use and depression.</p><p id="Par30">In conclusion, while automated depression detection algorithms show promise, the field still faces challenges in terms of standardization, methodological variations, and the need for longitudinal analysis. Future research should address these limitations, conduct a comparative analysis, and explore the intricate mechanisms underlying the relationship between social media use and depression. In addition, ethical considerations and the potential impact of using social media data for mental health assessment should be carefully examined. Advancements in this field can contribute to the development of effective and reliable tools for early detection and intervention in depression.</p></sec><sec id="Sec12"><title>Privacy issues and other factors</title><p id="Par31">Ford et al. investigated social media users&#x02019; opinions on providing mental healthcare services for depression-vulnerable individuals by analyzing social media content. Their survey indicated that social media users post negative content during low moods. They could see the benefits of identifying depression using social media content but did not believe that the risks of privacy breaches outweighed these benefits. In this survey, most participants consider identifying depression symptoms using social media content is intrusive and would not grant permission to researchers to conduct linguistic analysis<sup><xref ref-type="bibr" rid="CR98">98</xref></sup>.</p><p id="Par32">Gender also affects text patterns and must be accounted for when developing depression detection models. Hou et al. investigated the gender differences in depression and explored associated factors during the COVID-19 pandemic among Chinese social media users. Their findings showed an increased prevalence of depression and anxiety in the Chinese population, with females more likely to experience more severe symptoms than males<sup><xref ref-type="bibr" rid="CR99">99</xref></sup>.</p><p id="Par33">Several studies have built text corpora from social media, which were used to train baseline datasets, thus allowing other researchers to develop more efficient prediction models. Choudhury et al. built a large collection of tweets from individuals clinically diagnosed with depression. They developed a support vector machine with a radial basis function kernel to characterize depression levels in populations<sup><xref ref-type="bibr" rid="CR100">100</xref></sup>. Narynov et al. presented a dataset collected from social network platforms that are commonly used by the youth of the Commonwealth of Independent countries. They demonstrated that the dataset has high validity and can be used for further research in mental health<sup><xref ref-type="bibr" rid="CR101">101</xref></sup>. These studies contribute to our understanding of social media as a tool for mental health analysis and the importance of gender differences in depression research.</p></sec></sec><sec id="Sec13"><title>Predictive facial features in major depressive disorder</title><p id="Par34">Table <xref rid="Tab6" ref-type="table">6</xref> provides a summary of previous studies on automated depression detection using facial features. The studies examined various aspects of facial expressions and their relationship to depression. Among the 18 studies that utilized facial landmark features, 13 studies identified depression based on clinician diagnosis, while four studies used self-report depression rating scales. One study did not report the specific criterion used to determine depression levels.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Exploring the predictive relationship between facial expressions and depression.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Author</th><th>Main findings</th></tr></thead><tbody><tr><td>Li et al.<sup><xref ref-type="bibr" rid="CR102">102</xref></sup></td><td>A deep residual regression model to evaluate depression levels using enhancement techniques can reduce the influence of external factors on the image, significantly improving prediction performance.</td></tr><tr><td>Wang et al.<sup><xref ref-type="bibr" rid="CR62">62</xref></sup></td><td>Facial analysis is effective in automated depression diagnosis with an accuracy of 78%, recall of 80%, and F1 score of 79%.</td></tr><tr><td>Hao et al.<sup><xref ref-type="bibr" rid="CR103">103</xref></sup></td><td>A bidirectional LSTM network with an attention mechanism achieved an accuracy of 82% and F1 score of 81%.</td></tr><tr><td>Hunter et al.<sup><xref ref-type="bibr" rid="CR63">63</xref></sup></td><td>Individuals with depressive symptomatology showed a different eye-tracking pattern in processing emotional expressions.</td></tr><tr><td>Jan et al.<sup><xref ref-type="bibr" rid="CR61">61</xref></sup></td><td>The linear regression method applied to the AVEC 2014 dataset can predict BDI score using natural facial expressions.</td></tr><tr><td>Mohan et al.<sup><xref ref-type="bibr" rid="CR183">183</xref></sup></td><td>The proposed LSTM had the highest accuracy compared to other baselines.</td></tr><tr><td>Lee et al.<sup><xref ref-type="bibr" rid="CR184">184</xref></sup></td><td>An accessible depression diagnosis system using real-time object recognition and facial expressions obtained with a smartphone camera.</td></tr><tr><td>Liu et al.<sup><xref ref-type="bibr" rid="CR104">104</xref></sup></td><td>Proposed Part-and-Relation Attention Network for depression recognition, which outperforms state-of-the-art models with smaller prediction errors and higher stability.</td></tr><tr><td>Hamid et al.<sup><xref ref-type="bibr" rid="CR105">105</xref></sup></td><td>Designed a model for depression detection using electroencephalogram (EEG) and facial features. A hybrid model is proposed, outperforming existing diagnosis systems.</td></tr><tr><td>Nasir et al.<sup><xref ref-type="bibr" rid="CR106">106</xref></sup></td><td>A multimodal classification system for depression detection using geometrical facial features. The proposed visual feature sets show potential for robust and knowledge-driven depression classification.</td></tr><tr><td>Dai et al.<sup><xref ref-type="bibr" rid="CR107">107</xref></sup></td><td>A multimodal model with high performance on the AVEC 2013, AVEC 2014, and Emotion-Gait datasets. They concluded that the visual model is accurate.</td></tr><tr><td>Shangguan et al.<sup><xref ref-type="bibr" rid="CR108">108</xref></sup></td><td>An aggregation method which achieved comparable performance to 3D models with fewer parameters. The study suggests that video stimuli can be used for automatic depression detection.</td></tr><tr><td>Sumali et al.<sup><xref ref-type="bibr" rid="CR185">185</xref></sup></td><td>Significant differences were observed in facial landmark features (e.g., average right nose (speed), median left ear top (speed), and left pupil-right pupil positions) between healthy and depressive volunteers.</td></tr><tr><td>Dadiz et al.<sup><xref ref-type="bibr" rid="CR186">186</xref></sup></td><td>The uniformed local binary pattern extracted from videos for depression detection focuses on specific facial areas.</td></tr></tbody></table></table-wrap></p><p id="Par35">Li et al.<sup><xref ref-type="bibr" rid="CR102">102</xref></sup> proposed a deep residual regression model that evaluated depression levels. Their findings indicated that enhancing techniques can significantly improve prediction performance by reducing the influence of external factors, such as lighting and head pose. Wang et al.<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> analyzed facial expressions in videos for automated depression diagnosis. Their study demonstrated the effectiveness of facial analysis, achieving an accuracy of 78%, recall of 80%, and an F1 score of 79%. Hao et al.<sup><xref ref-type="bibr" rid="CR103">103</xref></sup> investigated optimal methods of depression detection using contextual temporal information. Their proposed bidirectional long-short-term memory network (LSTM) with an attention mechanism achieved an accuracy of 82% and an F1 score of 81%. Hunter et al.<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> evaluated the eye-tracking patterns of individuals with non-clinical depressive symptomatology in processing emotional expressions, revealing distinct differences compared to healthy individuals.</p><p id="Par36">In addition to the studies mentioned in the original paragraph, several more recent studies provide valuable insights into automated depression detection using facial features. For example, Liu et al.<sup><xref ref-type="bibr" rid="CR104">104</xref></sup> proposed the Part-and-Relation Attention Network, which outperformed state-of-the-art models with smaller prediction errors and higher stability. Hamid et al.<sup><xref ref-type="bibr" rid="CR105">105</xref></sup> designed a hybrid model that integrates electroencephalogram (EEG) data and facial features, surpassing existing diagnosis systems. Nasir et al.<sup><xref ref-type="bibr" rid="CR106">106</xref></sup> explored multimodal classification systems using geometrical facial features, indicating potential for robust and knowledge-driven depression classification. Dai et al.<sup><xref ref-type="bibr" rid="CR107">107</xref></sup> proposed a multimodal model that achieved superior performance on multiple datasets, emphasizing the accuracy of the visual model. Shangguan et al.<sup><xref ref-type="bibr" rid="CR108">108</xref></sup> demonstrated that video stimuli and an aggregation method can be effective for automatic depression detection.</p><p id="Par37">Overall, the summarized studies (see Fig. <xref rid="Fig4" ref-type="fig">4</xref>) highlight the significance of facial expressions in automated depression detection. They showcase various approaches, including deep learning models, multimodal techniques, and analysis of specific facial features. These findings contribute to the understanding of how facial expressions can serve as valuable indicators for detecting and diagnosing depression.<fig id="Fig4"><label>Fig. 4</label><caption><title>Synthesis of visual feature analysis in major depressive disorder.</title><p>Features that are significantly higher in a psychiatric group than healthy controls or that correlate positively with the depression level receive a score of 1 (red), features that are lower or correlate negatively receive a score of &#x02212;1 (blue), and findings without reporting their changes receive a score of 0 (gray). Features not studied in any studies are blank.</p></caption><graphic xlink:href="44184_2023_40_Fig4_HTML" id="d33e1587"/></fig></p></sec></sec><sec id="Sec14" sec-type="discussion"><title>Discussion</title><p id="Par38">Most studies in this literature review adopted automated speech feature extraction to assess major depressive disorder. This is probably due to the Audio/Visual Emotion Challenge Workshop (AVEC) competitions, which provide automated extracted audio and video features to predict the severity of these conditions. Many other studies then used the public datasets in competitions like Distress Analysis Interview Corpus Wizard of Oz (DAIC-WOZ). Of the 264 studies in this review, 39% used DAIC-WOZ or AVEC datasets.</p><p id="Par39">Most of the studies used some form of cross-validation for evaluating the performance of the trained models. However, only some studies used held-out test sets, which means that most models&#x02019; reported performance may not generalize well. Without a held-out test set, performance may drop from the development set to the test set, as has been observed in AVEC competitions<sup><xref ref-type="bibr" rid="CR109">109</xref>&#x02013;<xref ref-type="bibr" rid="CR111">111</xref></sup>. In contrast, models that used held-out test sets generally performed better on the test set<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. In addition, Zhang et al. achieved performance close to the state-of-the-art on the AVEC-2019 dataset (mean absolute error = 5.77 on the test set) by using automated extracted features and a random forest classifier<sup><xref ref-type="bibr" rid="CR111">111</xref></sup>. This suggests that the performance of a depression detection model is dependent on the dataset size, preprocessing strategy, feature engineering, and the model itself, which are all determined by the dataset used for training. Different models applied to different datasets can lead to different complexity-accuracy tradeoffs, and there is no universal best model.</p><p id="Par40">When studying the significance of acoustic features for major depressive disorder, many automated extracted features have been found to be predictive, and their correlations may be useful for making depression predictions<sup><xref ref-type="bibr" rid="CR70">70</xref>,<xref ref-type="bibr" rid="CR106">106</xref>,<xref ref-type="bibr" rid="CR112">112</xref>,<xref ref-type="bibr" rid="CR113">113</xref></sup>. As a result, we examine the connection between these automated extracted features and the observable symptoms of major depressive disorder.</p><sec id="Sec15"><title>Associating acoustic features to depression symptoms</title><p id="Par41">Many studies have reported that individuals with depression have lower values of the fundamental frequency <italic>F</italic><sub>0</sub> and its range, which indicates that their speech becomes monotonous<sup><xref ref-type="bibr" rid="CR70">70</xref>,<xref ref-type="bibr" rid="CR112">112</xref>,<xref ref-type="bibr" rid="CR114">114</xref></sup>. In addition, acoustic features such as jitter and shimmer<sup><xref ref-type="bibr" rid="CR115">115</xref>,<xref ref-type="bibr" rid="CR116">116</xref></sup> have been observed to increase with the severity of depression, which may be a result of slower thought, reaction, and physical movement in people with depression.</p></sec><sec id="Sec16"><title>Review for data collection</title><p id="Par42">The datasets used in automated depression detection studies vary greatly in size, participants&#x02019; demographics, depression rating scales, the task used to elicit emotion, and the interview environment. Therefore, the performance of a detection model can be misleading if the dataset used for training is not representative of the studied population. In this review, we discussed the data collection strategies used in these studies to elicit emotions, record videos, and maintain participant privacy while avoiding confounding factors such as clinician questions and patient responses.</p></sec><sec id="Sec17"><title>Identifying the presence of comorbidity</title><p id="Par43">Many previous studies have not reported comorbidities<sup><xref ref-type="bibr" rid="CR117">117</xref></sup> which presents additional challenges when developing automated depression detection models. However, Scherer et al. discovered a strong association (Pearson&#x02019;s <italic>r</italic>&#x02009;&#x0003e;&#x02009;0.8) between scores for depression on the PHQ-9 scale and scores for PTSD on the PTSD Checklist-Civilian Version (PCL-C) in the DAIC-WOZ dataset<sup><xref ref-type="bibr" rid="CR118">118</xref></sup>. Only a few articles stated that they excluded individuals with comorbidities from their study, such as ref. <sup><xref ref-type="bibr" rid="CR119">119</xref></sup>. To improve the data quality through consideration of comorbidities, researchers should use multiple mental disorder rating scales when collecting data. Additionally, researchers should develop and compare models trained with and without comorbidities to better understand the impact on the model performance.</p></sec><sec id="Sec18"><title>Factors to consider in recruiting control groups</title><p id="Par44">When selecting control groups for depression studies, it is important to ensure that individuals in the control group do not match any diagnostic criteria for other pathological conditions. For example, an individual may not be assessed as having depression based on their depression rating scale, but they may be assessed as suffering from PTSD that affects their speech patterns. Age, gender, first language, comorbidities, brain injury, respiratory disorders, and drug abuse can also affect speech and facial landmark patterns. In addition, variables such as education level, race, medication, and gender<sup><xref ref-type="bibr" rid="CR120">120</xref>&#x02013;<xref ref-type="bibr" rid="CR122">122</xref></sup> can also affect speech patterns. Hert et al. have reported that antipsychotic therapies may lead to dyskinesia, an involuntary movement of facial muscles that affects speech and facial landmarks<sup><xref ref-type="bibr" rid="CR123">123</xref></sup>. Therefore, individuals with a history of antidepressant medication should be excluded or reported in depression detection studies. Other variables such as gender, age, and education level can be adjusted via propensity score matching if they are statistically different between the depressive and healthy control groups.</p></sec><sec id="Sec19"><title>Self-report depression rating scales: pros and cons in depression diagnosis</title><p id="Par45">The traditional method for diagnosing depression is via a clinical evaluation by a registered psychologist, which is considered the gold standard compared to self-report depression rating scales. However, clinical diagnosis can be costly and subject to the experience and expertise of the clinician, leading to lower inter-rater reliability<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Most studies included in this review relied on self-reporting depression rating scales instead of clinical evaluation, such as AVEC<sup><xref ref-type="bibr" rid="CR121">121</xref>,<xref ref-type="bibr" rid="CR122">122</xref></sup> and DAIC-WOZ<sup><xref ref-type="bibr" rid="CR120">120</xref></sup>. When using these self-rating scales, the task becomes predicting the self-report rating scale rather than a clinical diagnosis, which may not align with a clinician&#x02019;s evaluation. On the other hand, using open-source datasets for research can improve reproducibility and objectively compare model performance.</p></sec><sec id="Sec20"><title>Eliciting emotions in depression diagnosis</title><p id="Par46">Choosing appropriate tasks for eliciting emotions is crucial, as specific features may be linked to certain depression rating scales but not others. In Table <xref rid="Tab7" ref-type="table">7</xref>, we summarize the tasks used in previous articles and their advantages. Kane et al. proposed that sustained vowels are optimal for estimating glottal source features because it can be difficult to identify voiced sections in free speech<sup><xref ref-type="bibr" rid="CR124">124</xref></sup>. Scherer et al. demonstrated that the voices of participants with moderate to severe depression are tenser than those of healthy participants<sup><xref ref-type="bibr" rid="CR118">118</xref></sup>. Alghowinem et al. proposed that spontaneous speech leads to better results for most features than reading speech and that the first few seconds of speech perform better than the entire recording<sup><xref ref-type="bibr" rid="CR125">125</xref></sup>. Another interesting approach to emotion elicitation is to use virtual agents for interviews, which can reduce data collection costs and can be less stressful for participants when discussing their symptoms. Multiple articles have reported successes in developing virtual interviewers<sup><xref ref-type="bibr" rid="CR126">126</xref>&#x02013;<xref ref-type="bibr" rid="CR128">128</xref></sup>, and the widely used AVEC challenges have also adopted virtual interviewers.<table-wrap id="Tab7"><label>Table 7</label><caption><p>A comparative study of different speech-eliciting tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2">Task and examples</th><th>Advantages</th></tr></thead><tbody><tr><td>Constrained</td><td>Repeating &#x0201c;PATAKA&#x0201d;<sup><xref ref-type="bibr" rid="CR187">187</xref>,<xref ref-type="bibr" rid="CR188">188</xref></sup></td><td>Capture speech sequencing; a proxy for lung capacity</td></tr><tr><td/><td>Sustained vowel<sup><xref ref-type="bibr" rid="CR189">189</xref></sup></td><td>Measure muscle weakness and aspects of move control</td></tr><tr><td/><td>Counting<sup><xref ref-type="bibr" rid="CR190">190</xref></sup></td><td>Counting from 1 to 10 allows mroe control over acoustic patterns</td></tr><tr><td/><td>Reading</td><td/></tr><tr><td/><td>&#x02022; The &#x0201c;Nordwind&#x0201d; passage<sup><xref ref-type="bibr" rid="CR191">191</xref>,<xref ref-type="bibr" rid="CR192">192</xref></sup></td><td>Paragraphs frequently used in the gathering of depression-related speech</td></tr><tr><td/><td>&#x02022; Rainbow passage<sup><xref ref-type="bibr" rid="CR189">189</xref></sup></td><td>Includes all the sounds used in English and reflects normal speech patterns</td></tr><tr><td/><td>&#x02022; Emotion-evoking movie clips<sup><xref ref-type="bibr" rid="CR193">193</xref></sup></td><td>Greater ability to regulate emotions that are provoked</td></tr><tr><td>Free speech</td><td>Monologue</td><td/></tr><tr><td/><td>&#x02022; Describing, memory recalling<sup><xref ref-type="bibr" rid="CR194">194</xref></sup></td><td>More spontaneous than reading speech</td></tr><tr><td/><td>Dialogue</td><td/></tr><tr><td/><td>&#x02022; Semi-structured interviews<sup><xref ref-type="bibr" rid="CR120">120</xref></sup></td><td>Frequently used in medical facilities</td></tr><tr><td/><td>&#x02022; Phone conversations<sup><xref ref-type="bibr" rid="CR195">195</xref></sup></td><td>Only the interviewee is recorded; no need to identify the speaker</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec21"><title>Diarization of speech segments in interview recordings: methods and considerations</title><p id="Par47">It is common practice to separate the speech segments of the participants from interview recordings to train depression prediction models. This process is commonly referred to as diarization. The participant&#x02019;s speech can be extracted using a microphone next to each speaker. If participants have headsets with lapel microphones during the interview, their voiced sections can be easily extracted, which may make some participants uncomfortable. Desk microphones can also be used, but they can introduce confounds because they are not targeted and make it difficult to separate participants&#x02019; speech in the data processing. We suggest using two desk microphones next to each speaker with a sound barrier between them. It is important to record all metadata after the interview in a separate spreadsheet, such as participant ID, group, task, and other demographic information.</p></sec><sec id="Sec22"><title>Ensuring privacy in interview recordings</title><p id="Par48">Clinicians should obtain verbal or written consent from the participants before interviewing. Participants must be informed that their interview recordings and demographic information may be distributed, pre-processed, and used for training machine-learning models for academic research purposes. Even if participants grant permission for their data to be further processed, researchers must minimize the risk of data leakage because the interview audio (or video) recordings may contain sensitive information. To address this, researchers can share only the automated extracted speech and facial features rather than the raw interview recordings. If hackers were to gain access to the interview data, it would be impossible to reconstruct the original interview recordings using only the automated extracted features. Additionally, researchers can train the depression prediction model in real-time or use bone conduction microphones, which only record acoustic features without speech content<sup><xref ref-type="bibr" rid="CR129">129</xref></sup>, but this limits researchers to training semantic models. Edge computing can also be a solution to improve privacy by allowing computation to be performed on the participants&#x02019; devices, with only the trained models being returned to the researchers, not the data.</p></sec><sec id="Sec23"><title>Review of machine-learning models</title><p id="Par49">We believe that a well-trained depression prediction model can accurately detect the disorder in a randomly selected individual, regardless of the environment in which the individual is being interviewed. The participant may be of a different age and use a different language or accent, but the depression prediction model should still be able to generalize and be robust to these environmental factors. However, most previous studies have only been designed to detect mental disorders in new individuals collected in similar settings. Our suggestions for future studies to improve robustness and generalization are as follows.</p></sec><sec id="Sec24"><title>Data preprocessing</title><p id="Par50">Automatic speech recognition (ASR) can transcribe speech into transcripts for training semantic-based depression prediction models. ASR can also filter unvoiced sections and noise in the interview audio (or video) recordings. In most in-person interviews, two speakers are present, and the clinicians&#x02019; segments can be discarded if the ASR system includes automatic diarization. To prevent overfitting, techniques like dimensionality reduction or feature selection should be applied to the training and test sets during preprocessing. This will help ensure that the model is not overly influenced by the specific characteristics of the training data and can be generalized to new data.</p></sec><sec id="Sec25"><title>Automated feature extraction</title><p id="Par51">The most commonly used automated feature extraction tools in the studies we reviewed were openSMILE, COVAREP, pyAudioAnalysis, and openEAR. As shown in Table <xref rid="Tab8" ref-type="table">8</xref>, some features were found to be predictive in multiple studies. To ensure the deep learning models converge, it is recommended to standardize or normalize features as they may be in different scales. Before training the model, we recommend performing exploratory data analysis or visualization to better understand how these features characterize mental disorders. This can help inform the selection and preprocessing of features, as well as the design of the model.<table-wrap id="Tab8"><label>Table 8</label><caption><p>An overview of acoustic features. for more details, see the cooperative voice analysis repository (COVAREP).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Acoustic feature</th><th>Description</th></tr></thead><tbody><tr><td>Source features</td><td>Features reflecting airflow from the lungs through the glottis (i.e., glottal features) or vocal fold vibrations (i.e., voice quality features), which is the sound source later filtered by the vocal tract following the source-filter theory of speech production.</td></tr><tr><td>Jitter (%)</td><td>Deviations in the consecutive lengths of the <italic>f</italic><sub>0</sub> period, which suggests irregular and uneven vocal fold vibrations.</td></tr><tr><td>Shimmer (%)</td><td>The variation in the peak amplitudes of consecutive <italic>f</italic><sub>0</sub> periods, which implies unevenness in voice loudness.</td></tr><tr><td>Tremor (Hz)</td><td>The number of occurrences of the most powerful low-frequency fundamental frequency-modulating element within a defined examination range.</td></tr><tr><td>Harmonics-to-noise ratio (HNR) (dB)</td><td>Ratio between <italic>f</italic><sub>0</sub> and noise components, which indirectly correlates with perceived aspiration.</td></tr><tr><td>Frequency disturbance ratio (FDR) (%)</td><td>The average relative value of the frequency variation over 5 to 5 cycles (calculated using an average of five data points).</td></tr><tr><td>Amplitude disturbance ratio (ADR) (%)</td><td>Relative mean amplitude value over a set of windows.</td></tr><tr><td>Quasi-open quotient</td><td>Ratio of the vocal folds opening time. Functional dysphonias often reduce QOQ range.</td></tr><tr><td>Normalized amplitude quotient (NAQ)</td><td>A measurement that compares the amplitude between the highest and lowest points of the differentiated flow glottogram to the amplitude of the negative peak and normalizing it with respect to the period time. It can be used as an approximation of glottal adduction.</td></tr><tr><td>Peak slope</td><td>Slope of the regression line that is fit to log10 of the maxima of each frame.</td></tr><tr><td>Filter features</td><td>The resonant properties of the vocal and nasal tracts filter the sound source from the vocal folds: the filter attenuates certain frequencies and strengthens others by the shape of the vocal and nasal tracts.</td></tr><tr><td><italic>F</italic><sub>1</sub> mean (Hz)</td><td>First peak in the spectrum that results from a resonance of the human vocal tract.</td></tr><tr><td><italic>F</italic><sub>2</sub> mean (Hz)</td><td>Second peak in the spectrum that results from a resonance of the human vocal tract.</td></tr><tr><td><italic>F</italic><sub>1</sub> variability (Hz)</td><td>Measures of dispersion of <italic>F</italic><sub>1</sub> (variance, standard deviation).</td></tr><tr><td><italic>F</italic><sub>2</sub> variability (Hz)</td><td>Measures of dispersion of <italic>F</italic><sub>2</sub> (variance, standard deviation).</td></tr><tr><td><italic>F</italic><sub>1</sub> range (Hz)</td><td>Difference between the lowest and highest <italic>F</italic><sub>1</sub> values.</td></tr><tr><td>Vowel space</td><td><italic>F</italic><sub>1</sub> and <italic>F</italic><sub>2</sub> 2D space for the vowels.</td></tr><tr><td>Linear predictive coding (LPC) coefficients</td><td>Coefficients that best predict the values of the next time point of the audio signal using the values from the previous n time points, which is used to reconstruct filter properties.</td></tr><tr><td>Spectral features</td><td>Features characterizing the frequency distribution of the speech signal at a particular moment in time.</td></tr><tr><td>Mel-frequency cepstral coefficients (MFCCs)</td><td>The coefficients derived by analyzing the Mel-spectrum of the log-magnitude of an audio segment.</td></tr><tr><td>Prosodic features</td><td>Changes over longer segments of time, which is perceived in the rhythm, stress, and intonation of speech.</td></tr><tr><td><italic>f</italic><sub>0</sub> mean (Hz)</td><td>Fundamental frequency: lowest frequency of the speech signal, perceived as pitch (mean, median).</td></tr><tr><td><italic>f</italic><sub>0</sub> variability (Hz)</td><td>Measures of dispersion of <italic>f</italic><sub>0</sub> (variance, standard deviation).</td></tr><tr><td><italic>f</italic><sub>0</sub> range (Hz)</td><td>Difference between the lowest and highest <italic>f</italic><sub>0</sub>.</td></tr><tr><td>Intensity (dB)</td><td>Defined as the acoustic intensity (i.e., power carried by sound per unit area in a direction perpendicular to that area in decibels relative to a reference value, perceived as loudness).</td></tr><tr><td>Intensity variability (dB)</td><td>Measures of dispersion of intensity (variance, standard deviation).</td></tr><tr><td>Energy velocity</td><td>Measured as the mean-squared central difference across frames and may correlate with motor coordination.</td></tr><tr><td>Maximum phonation time (s)</td><td>The mean of three attempts of the following measure is taken: the maximum time during which phonation of a vowel is sustained as long as possible with an upright position, deep breath, and a comfortable pitch and loudness.</td></tr><tr><td>Speech rate</td><td>Number of speech utterances per second over the duration of the speech sample (including pauses).</td></tr><tr><td>Articulation rate</td><td>Number of speech units per second throughout the speech sample (excluding pauses).</td></tr><tr><td>Time talking (s)</td><td>Sum of the duration of all speech segments.</td></tr><tr><td>Utterance duration mean (s)</td><td>Mean duration of utterance length.</td></tr><tr><td>Pause duration mean (s)</td><td>Mean duration of pause length.</td></tr><tr><td>Pause variability (s)</td><td>Measures of dispersion of pause duration (variance, standard deviation).</td></tr><tr><td>Pause rate (s)</td><td>Total length of pauses divided by the total length of speech (including pauses).</td></tr><tr><td>Pause total (s)</td><td>Total duration of pauses.</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec26"><title>Evaluate models with small datasets: bootstrapping and K-fold cross-validation</title><p id="Par52">To avoid overfitting the model on the test set, we typically evaluate the trained model on the held-out test set only once. However, when training a model for predicting depression using a small dataset (e.g., around 100 data points), which is commonly seen in the medical field, a 20% held-out test set or K-fold cross-validation can decrease the number of samples available for training. In addition, a small test set is unlikely to represent the entire population accurately. As a result, we suggest using repeated bootstrapping to evaluate the depression prediction model, which provides a distribution of performance metrics with mean and standard deviation. However, given the computational complexity of deep learning models, the bootstrapping method may not be feasible. When working with a small dataset, K-fold cross-validation can be a viable alternative to bootstrapping, as deep learning models tend to need a large number of data points which reduces the need for bootstrapping.</p></sec><sec id="Sec27"><title>Evaluating the performance of depression prediction: best practices and considerations</title><p id="Par53">Performing better than chance does not indicate the model learned from the training data, and the resulting metrics must be generalizable and statistically significant for clinical use. Aloshban et al. demonstrated that their accuracy is always better than chance to a statistically significant extent<sup><xref ref-type="bibr" rid="CR130">130</xref></sup>. However, to further prove generalizability, we suggest performing a permutation test in future works, where models are trained on permutated labels to evaluate the model&#x02019;s performance based on mistaken labels, which is often better than chance. A statistical test can then determine if the difference between the permuted and non-permuted scores is statistically significant. On the other hand, clinical datasets can be imbalanced, with a greater number of healthy cases compared to the population of individuals with depression. In this case, using the accuracy of the classification model as the sole metric to evaluate its performance may not be objective since it will be biased towards predicting every sample as negative. To evaluate model performance more objectively, metrics such as the <italic>F</italic><sub>1</sub> score, precision, recall, and area under the curve (AUC) should be considered. These metrics account for class imbalance and provide a more balanced view of model performance. Saito et al. have shown that the precision-recall curve is more useful than the receiver operating characteristic curve when evaluating binary classifiers on imbalanced datasets<sup><xref ref-type="bibr" rid="CR131">131</xref></sup>. In addition to the precision-recall curve, metrics such as root mean square error (RMSE), mean square error (MSE), and the coefficient of determination (<italic>r</italic><sup>2</sup>) are commonly used to evaluate the performance of regression models for predicting depression scores. In the AVEC competition, the performance of baseline models was evaluated using the concordance correlation coefficient (CCC), which takes into account changes in scale and includes measures of both precision and accuracy<sup><xref ref-type="bibr" rid="CR132">132</xref></sup>. It is generally helpful for other researchers to see a range of metrics when evaluating the performance of a model, as this allows for more objective comparison. A model&#x02019;s performance must be generalizable and statistically significant to be truly useful in a clinical setting.</p></sec><sec id="Sec28"><title>Explainable depression detection model</title><p id="Par54">Recent evidence suggests that individuals may lack trust in black box models and that these models may cause harm in high-stakes decision-making processes<sup><xref ref-type="bibr" rid="CR133">133</xref>&#x02013;<xref ref-type="bibr" rid="CR135">135</xref></sup>. As a result, researchers are exploring ways to explain the decision-making processes of algorithms better<sup><xref ref-type="bibr" rid="CR136">136</xref>,<xref ref-type="bibr" rid="CR137">137</xref></sup> through publications and software packages implement explainable machine-learning models<sup><xref ref-type="bibr" rid="CR138">138</xref>,<xref ref-type="bibr" rid="CR139">139</xref></sup>. By providing explanations of a model&#x02019;s feature contributions, clinicians can gain a better understanding of depression and improve the model itself. Lundberg et al. proposed an additive method for evaluating feature contributions, which calculates the difference in the model&#x02019;s output when a given feature is considered versus not considered<sup><xref ref-type="bibr" rid="CR139">139</xref></sup>. This can provide valuable insights into the factors influencing the model&#x02019;s predictions. Once high-impact features have been identified, we can retrain the model using only these features to evaluate their performance. In some of the reviewed articles, we observed that while the studies presented excellent feature engineering for distinguishing between groups, they lacked quantitative analysis to support their findings. In addition, we suggest linking changes in the automated extraction of features to mental disorder symptoms to provide a more comprehensive view of the model&#x02019;s performance. By combining these approaches, we can better understand the factors influencing the model&#x02019;s predictions and improve its performance. On the other hand, there are arguments on whether a complex, hard-to-explain model with good performance should be discarded in favor of a simpler, easy-to-interpret but lower-performing model<sup><xref ref-type="bibr" rid="CR140">140</xref></sup>. From our perspective, since complex but high-sensitivity examination methods would not surpass the use of less sensitive models, these models would not be abandoned if they are proven effective in clinical trials. Thus, validating and explaining complex models will be important in this field.</p></sec><sec id="Sec29"><title>Ensuring reproducibility in automated depression detection</title><p id="Par55">Reproducibility is a critical issue in machine learning, particularly when artificial intelligence is applied to healthcare<sup><xref ref-type="bibr" rid="CR141">141</xref>,<xref ref-type="bibr" rid="CR142">142</xref></sup>. One obstacle to reproducing previous studies is that clinical datasets are not always available for redistribution. As we mentioned in Section Ensuring privacy in interview recordings, automated extracted features can be shared without violating privacy concerns. However, sharing the code used for training and evaluating the model is also important. Even when the code and data are publicly available, other researchers may still have difficulty reproducing the results due to differences in the software environment. To address this issue, we suggest that researchers use containers, such as Docker, which include the data, code, and environment in one package that can be easily redistributed. This will make it easier for other researchers to reproduce the results, ultimately accelerating the advancement of automated depression detection.</p></sec><sec id="Sec30"><title>Evaluating models from competitions in automated depression diagnosis</title><p id="Par56">About 39% of the studies included in this review were developed during or after the AVEC. Some of these studies introduced innovative approaches to feature engineering and model architecture. However, since teams were allowed to submit their models multiple times, overfitting the test set is a risk. The test set provided in the AVEC competition is relatively small, which means that the state-of-the-art model can outperform the second-best model by chance due to overfitting. Therefore, we cannot simply assume that a model that performs slightly better on the test set will also generalize well to new data. We believe that multiple comparison corrections should be applied to evaluate the performance of the models, and simpler models should be prioritized<sup><xref ref-type="bibr" rid="CR143">143</xref></sup>. This will help to ensure that the results are more reliable and can be more confidently applied in a clinical setting.</p><p id="Par57">Due to the limited number of studies we were able to review and include in this review, we only searched for keywords in the titles of articles rather than in other sections. The screening process of the articles involved reading the title and abstract. Only articles that were relevant to using machine learning to detect depression and had &#x0201c;machine learning" or related terms in the title were included, and the others were excluded. Our study may not have captured all relevant articles on this topic, and other studies not focused specifically on automated depression diagnosis using machine-learning methods may have been missed.</p></sec><sec id="Sec31"><title>Future work</title><sec id="Sec32"><title>Cross-cultural generalization</title><p id="Par58">While machine-learning models are optimized to work well on the training data, it is not always clear how well they will generalize to new sample sets where different ages, languages, socioeconomic and education levels are reported. Alghowinem et al. developed a cross-cultural depression recognition model and evaluated it on datasets in English and German<sup><xref ref-type="bibr" rid="CR144">144</xref></sup>. Some previous studies have also investigated the effects of different smartphones on the quality of acoustic features, which can impact the accuracy of depression prediction<sup><xref ref-type="bibr" rid="CR145">145</xref>,<xref ref-type="bibr" rid="CR146">146</xref></sup>. Mitra et al. examined the effects of noise and reverberation on depression detection using speech and found that spontaneous speech performed better than read speech<sup><xref ref-type="bibr" rid="CR147">147</xref></sup>. These studies highlight the importance of considering factors that may affect the performance of machine-learning models for automated depression diagnosis.</p></sec><sec id="Sec33"><title>Ethical considerations in automated depression detection: addressing risks and ensuring responsible use</title><p id="Par59">Automated depression detection can benefit society by reducing the workload of the healthcare system, preventing suicidal or self-harm behaviors, and enabling law enforcement authorities to track abnormal behaviors. However, the use of automated depression detection also raises some ethical concerns. For example, insurance companies and employers may use the results to evaluate candidates without their knowledge or consent and reject them if a mental disorder is present or likely to develop in the future. In addition, it can be difficult for individuals to fully understand the implications of consent forms, which can further complicate the ethical considerations surrounding automated depression detection<sup><xref ref-type="bibr" rid="CR148">148</xref></sup>. To ensure that automated depression detection is used ethically in clinical settings, researchers should provide clear and understandable explanations of how the collected data will be used. Participants should also have the right to revoke permission to use their data at any time. Like other developing technologies, these systems may be vulnerable to abuse and have unexpected side effects. As researchers, engineers, and clinicians, it is our responsibility to educate the public and policymakers about the potential benefits and harms of automated depression detection to both prevent abuse and further advance these techniques, which have the potential to help many people.</p></sec><sec id="Sec34"><title>Leveraging machine learning for advancing psychiatry</title><p id="Par60">With this review paper, we aim to demonstrate the potential for psychiatry to benefit from advances in machine learning. Many individuals have difficulty accessing qualified mental healthcare or may be hesitant to seek psychotherapy due to stigmatization<sup><xref ref-type="bibr" rid="CR149">149</xref></sup>. Automated depression detection models can provide an accessible and efficient method for early screening, which can help individuals determining that they may need professional healthcare. In addition, psychiatric visits often include interviews that can be recorded in video or audio format, which provides a wealth of data that can be used to associate mental health assessments with acoustic, semantic, and facial features. By following the guidelines outlined in this paper for collecting and analyzing this data, we hope to enable new collaborations between clinicians and machine-learning engineers to advance our understanding of mental health disorders.</p></sec></sec></sec><sec id="Sec35" sec-type="conclusion"><title>Conclusion</title><p id="Par61">We reviewed 264 studies that measure acoustic, semantic, and facial landmark features to distinguish between individuals with and without mental health disorders using either null hypothesis testing or predictive machine-learning models. Our synthesis includes significant and nonsignificant features across audio, text, and facial modalities, as well as those correlated with the severity of depression. We also provide guidelines on collecting data, preventing confounding factors, protecting privacy, selecting speech-eliciting tasks, and improving machine-learning model generalizability and reproducibility. We also found a few studies have been conducted on post-traumatic stress disorder, bipolar disorder, and postpartum depression, thanks to open-access research datasets provided by the AVEC and DAIC. Competitions provide a useful framework for comparing innovations under the same conditions, such as using the same dataset and metrics. This approach enables researchers to evaluate the model&#x02019;s performance using a held-out test set and estimate overfitting; however, overfitting is still a concern in such competitions. In addition, these competitions make it possible for future studies to be conducted using the same dataset, which facilitates comparisons and helps advance the field. Based on their proven effectiveness, we encourage the collection of open datasets, particularly distributing datasets through competitions. These are highly productive in advancing research in various fields. While productivity is important, reproducibility is also critical. Since the studies in this review involve building computational models, the associated data and code should be shared, ideally through containers. This allows others to test the claims made by these studies and contribute to the development of these models in a collaborative manner. Moreover, conducting more research on multiple datasets may help enhance the models&#x02019; generalizability and reconcile conflicting results regarding crucial and predictive features. This approach could lead to more robust and reliable conclusions about the nature of these disorders and their diagnosis and treatment. Using multimodality features to train machine-learning models holds promise for enhancing mental health evaluations and treatment. This approach aligns with the principles of preventive and personalized diagnosis and treatment and could lead to better outcomes for individuals with mental health conditions.</p></sec><sec id="Sec36" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="44184_2023_40_MOESM1_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="44184_2023_40_MOESM2_ESM.docx"><caption><p>Checklist item</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s44184-023-00040-z.</p></sec><ack><title>Acknowledgements</title><p>We would like to thank the funding support of MITACS for this research. This research is also supported by the China Scholarship Council (CSC) No. 202000810031 and No. 202308180002.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>K.M. was a major contributor to writing the manuscript. K.M. and Y.W. performed the literature review. J.C. supervised the project and provided funds to support the project. All authors have given approval for the final version of the manuscript. All authors reviewed the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The data supporting this review paper are openly available and can be accessed at the following URLs: <ext-link ext-link-type="uri" xlink:href="https://bit.ly/3DBQtZk">https://bit.ly/3DBQtZk</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://bit.ly/43Q6Yvy">https://bit.ly/43Q6Yvy</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://bit.ly/44IKaPv">https://bit.ly/44IKaPv</ext-link>.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code associated with this review paper is openly available and can be accessed at the following GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/uofabinarylab/ADDReview">https://github.com/uofabinarylab/ADDReview</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par62">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>MJ</given-names></name></person-group><article-title>Depression is the leading cause of disability around the world</article-title><source>J. Am. Med. Assoc.</source><year>2017</year><volume>317</volume><fpage>1517</fpage><lpage>1517</lpage></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Friedrich, M. J. Depression is the leading cause of disability around the world. <italic>J. Am. Med. Assoc.</italic><bold>317</bold>, 1517&#x02013;1517 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Evans-Lacko</surname><given-names>S</given-names></name><etal/></person-group><article-title>Socio-economic variations in the mental health treatment gap for people with anxiety, mood, and substance use disorders: results from the WHO world mental health (WMH) surveys</article-title><source>Psychol. Med.</source><year>2018</year><volume>48</volume><fpage>1560</fpage><lpage>1571</lpage><pub-id pub-id-type="doi">10.1017/S0033291717003336</pub-id><pub-id pub-id-type="pmid">29173244</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Evans-Lacko, S. et al. Socio-economic variations in the mental health treatment gap for people with anxiety, mood, and substance use disorders: results from the WHO world mental health (WMH) surveys. <italic>Psychol. Med.</italic><bold>48</bold>, 1560&#x02013;1571 (2018).<pub-id pub-id-type="pmid">29173244</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Cai, H., Sha, X., Han, X., Wei, S. &#x00026; Hu, B. Pervasive eeg diagnosis of depression using deep belief network with three-electrodes EEG collector. In <italic>2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</italic>, 1239&#x02013;1246 (IEEE, 2016).</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Hosseinifard</surname><given-names>B</given-names></name><name><surname>Moradi</surname><given-names>MH</given-names></name><name><surname>Rostami</surname><given-names>R</given-names></name></person-group><article-title>Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from EEG signal</article-title><source>Comput. Methods Programs Biomed.</source><year>2013</year><volume>109</volume><fpage>339</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2012.10.008</pub-id><pub-id pub-id-type="pmid">23122719</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Hosseinifard, B., Moradi, M. H. &#x00026; Rostami, R. Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from EEG signal. <italic>Comput. Methods Programs Biomed.</italic><bold>109</bold>, 339&#x02013;345 (2013).<pub-id pub-id-type="pmid">23122719</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Sano, A. &#x00026; Picard, R. W. Stress recognition using wearable sensors and mobile phones. In <italic>2013 Humaine Association Conference on Affective Computing and Intelligent Interaction</italic>, 671&#x02013;676 (IEEE, 2013).</mixed-citation></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>UR</given-names></name><etal/></person-group><article-title>Computer-aided diagnosis of depression using EEG signals</article-title><source>Eur. Neurol.</source><year>2015</year><volume>73</volume><fpage>329</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1159/000381950</pub-id><pub-id pub-id-type="pmid">25997732</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Acharya, U. R. et al. Computer-aided diagnosis of depression using EEG signals. <italic>Eur. Neurol.</italic><bold>73</bold>, 329&#x02013;336 (2015).<pub-id pub-id-type="pmid">25997732</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Hartmann</surname><given-names>R</given-names></name><name><surname>Schmidt</surname><given-names>FM</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Hegerl</surname><given-names>U</given-names></name></person-group><article-title>Heart rate variability as indicator of clinical state in depression</article-title><source>Front. Psychiatry</source><year>2019</year><volume>9</volume><fpage>735</fpage><pub-id pub-id-type="doi">10.3389/fpsyt.2018.00735</pub-id><pub-id pub-id-type="pmid">30705641</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Hartmann, R., Schmidt, F. M., Sander, C. &#x00026; Hegerl, U. Heart rate variability as indicator of clinical state in depression. <italic>Front. Psychiatry</italic><bold>9</bold>, 735 (2019).<pub-id pub-id-type="pmid">30705641</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Kan, D. P. X. &#x00026; Lee, P. F. Decrease alpha waves in depression: an electroencephalogram (EEG) study. In <italic>2015 International Conference on BioSignal Analysis, Processing and Systems (ICBAPS)</italic>, 156&#x02013;161 (IEEE, 2015).</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Ay</surname><given-names>B</given-names></name><etal/></person-group><article-title>Automated depression detection using deep representation and sequence learning with EEG signals</article-title><source>J. Medi. Syst.</source><year>2019</year><volume>43</volume><fpage>1</fpage><lpage>12</lpage></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Ay, B. et al. Automated depression detection using deep representation and sequence learning with EEG signals. <italic>J. Medi. Syst.</italic><bold>43</bold>, 1&#x02013;12 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>UR</given-names></name><etal/></person-group><article-title>Automated EEG-based screening of depression using deep convolutional neural network</article-title><source>Comput. Methods Programs Biomed.</source><year>2018</year><volume>161</volume><fpage>103</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2018.04.012</pub-id><pub-id pub-id-type="pmid">29852953</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Acharya, U. R. et al. Automated EEG-based screening of depression using deep convolutional neural network. <italic>Comput. Methods Programs Biomed.</italic><bold>161</bold>, 103&#x02013;113 (2018).<pub-id pub-id-type="pmid">29852953</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>UR</given-names></name><etal/></person-group><article-title>A novel depression diagnosis index using nonlinear features in EEG signals</article-title><source>Eur. Neurol.</source><year>2015</year><volume>74</volume><fpage>79</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1159/000438457</pub-id><pub-id pub-id-type="pmid">26303033</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Acharya, U. R. et al. A novel depression diagnosis index using nonlinear features in EEG signals. <italic>Eur. Neurol.</italic><bold>74</bold>, 79&#x02013;83 (2015).<pub-id pub-id-type="pmid">26303033</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Mohan, Y., Chee, S. S., Xin, D. K. P. &#x00026; Foong, L. P. Artificial neural network for classification of depressive and normal in EEG. In <italic>2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES)</italic>, 286&#x02013;290 (IEEE, 2016).</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Mumtaz</surname><given-names>W</given-names></name><etal/></person-group><article-title>Electroencephalogram (EEG)-based computer-aided technique to diagnose major depressive disorder (MDD)</article-title><source>Biomed. Signal Process. Control</source><year>2017</year><volume>31</volume><fpage>108</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/j.bspc.2016.07.006</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Mumtaz, W. et al. Electroencephalogram (EEG)-based computer-aided technique to diagnose major depressive disorder (MDD). <italic>Biomed. Signal Process. Control</italic><bold>31</bold>, 108&#x02013;115 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Akbari</surname><given-names>H</given-names></name><etal/></person-group><article-title>Depression recognition based on the reconstruction of phase space of EEG signals and geometrical features</article-title><source>Appl. Acoust.</source><year>2021</year><volume>179</volume><fpage>108078</fpage><pub-id pub-id-type="doi">10.1016/j.apacoust.2021.108078</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Akbari, H. et al. Depression recognition based on the reconstruction of phase space of EEG signals and geometrical features. <italic>Appl. Acoust.</italic><bold>179</bold>, 108078 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>AY</given-names></name><etal/></person-group><article-title>Skin conductance responses in major depressive disorder (MDD) under mental arithmetic stress</article-title><source>PLoS ONE</source><year>2019</year><volume>14</volume><fpage>e0213140</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0213140</pub-id><pub-id pub-id-type="pmid">30943195</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Kim, A. Y. et al. Skin conductance responses in major depressive disorder (MDD) under mental arithmetic stress. <italic>PLoS ONE</italic><bold>14</bold>, e0213140 (2019).<pub-id pub-id-type="pmid">30943195</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Williamson, J. R. et al. Detecting depression using vocal, facial and semantic communication cues. In <italic>Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</italic>, 11&#x02013;18 (Association for Computing Machinery (ACM), 2016).</mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Jan</surname><given-names>A</given-names></name><name><surname>Meng</surname><given-names>H</given-names></name><name><surname>Gaus</surname><given-names>YFBA</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name></person-group><article-title>Artificial intelligent system for automatic depression level analysis through visual and vocal expressions</article-title><source>IEEE Trans. Cogn. Dev. Syst.</source><year>2017</year><volume>10</volume><fpage>668</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1109/TCDS.2017.2721552</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Jan, A., Meng, H., Gaus, Y. F. B. A. &#x00026; Zhang, F. Artificial intelligent system for automatic depression level analysis through visual and vocal expressions. <italic>IEEE Trans. Cogn. Dev. Syst.</italic><bold>10</bold>, 668&#x02013;680 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Daros</surname><given-names>AR</given-names></name><name><surname>Zakzanis</surname><given-names>KK</given-names></name><name><surname>Ruocco</surname><given-names>A</given-names></name></person-group><article-title>Facial emotion recognition in borderline personality disorder</article-title><source>Psychol. Med.</source><year>2013</year><volume>43</volume><fpage>1953</fpage><lpage>1963</lpage><pub-id pub-id-type="doi">10.1017/S0033291712002607</pub-id><pub-id pub-id-type="pmid">23149223</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Daros, A. R., Zakzanis, K. K. &#x00026; Ruocco, A. Facial emotion recognition in borderline personality disorder. <italic>Psychol. Med.</italic><bold>43</bold>, 1953&#x02013;1963 (2013).<pub-id pub-id-type="pmid">23149223</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Early perceptual anomaly of negative facial expression in depression: an event-related potential study</article-title><source>Neurophysiologie Clinique/Clin. Neurophysiol.</source><year>2015</year><volume>45</volume><fpage>435</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1016/j.neucli.2015.09.011</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Zhao, Q. et al. Early perceptual anomaly of negative facial expression in depression: an event-related potential study. <italic>Neurophysiologie Clinique/Clin. Neurophysiol.</italic><bold>45</bold>, 435&#x02013;443 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Seneviratne, N., Williamson, J. R., Lammert, A. C., Quatieri, T. F. &#x00026; Espy-Wilson, C. Y. Extended study on the use of vocal tract variables to quantify neuromotor coordination in depression. <italic>INTERSPEECH</italic> 4551&#x02013;4555 (2020).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Zhao, Z. et al. Hybrid network feature extraction for depression assessment from speech. <italic>Interspeech</italic><ext-link ext-link-type="uri" xlink:href="https://api.semanticscholar.org/CorpusID:226203252">https://api.semanticscholar.org/CorpusID:226203252</ext-link> (2020).</mixed-citation></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Kiss</surname><given-names>G</given-names></name><name><surname>Vicsi</surname><given-names>K</given-names></name></person-group><article-title>Mono-and multi-lingual depression prediction based on speech processing</article-title><source>Int. J. Speech Technol.y</source><year>2017</year><volume>20</volume><fpage>919</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1007/s10772-017-9455-8</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Kiss, G. &#x00026; Vicsi, K. Mono-and multi-lingual depression prediction based on speech processing. <italic>Int. J. Speech Technol.y</italic><bold>20</bold>, 919&#x02013;935 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Dham, S., Sharma, A. &#x00026; Dhall, A. Depression scale recognition from audio, visual and text analysis. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1709.05865">https://arxiv.org/abs/1709.05865</ext-link> (2017).</mixed-citation></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Rejaibi</surname><given-names>E</given-names></name><name><surname>Komaty</surname><given-names>A</given-names></name><name><surname>Meriaudeau</surname><given-names>F</given-names></name><name><surname>Agrebi</surname><given-names>S</given-names></name><name><surname>Othmani</surname><given-names>A</given-names></name></person-group><article-title>Mfcc-based recurrent neural network for automatic clinical depression recognition and assessment from speech</article-title><source>Biomed. Signal Process. Control</source><year>2022</year><volume>71</volume><fpage>103107</fpage><pub-id pub-id-type="doi">10.1016/j.bspc.2021.103107</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Rejaibi, E., Komaty, A., Meriaudeau, F., Agrebi, S. &#x00026; Othmani, A. Mfcc-based recurrent neural network for automatic clinical depression recognition and assessment from speech. <italic>Biomed. Signal Process. Control</italic><bold>71</bold>, 103107 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Jiang, H. et al. Detecting depression using an ensemble logistic regression model based on multiple speech features. <italic>Comput. Math. Methods Med</italic>. <bold>2018</bold>, (2018).</mixed-citation></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Sardari</surname><given-names>S</given-names></name><name><surname>Nakisa</surname><given-names>B</given-names></name><name><surname>Rastgoo</surname><given-names>MN</given-names></name><name><surname>Eklund</surname><given-names>P</given-names></name></person-group><article-title>Audio based depression detection using convolutional autoencoder</article-title><source>Expert Syst. Appl.</source><year>2022</year><volume>189</volume><fpage>116076</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2021.116076</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Sardari, S., Nakisa, B., Rastgoo, M. N. &#x00026; Eklund, P. Audio based depression detection using convolutional autoencoder. <italic>Expert Syst. Appl.</italic><bold>189</bold>, 116076 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Pampouchidou, A. et al. Facial geometry and speech analysis for depression detection. In <italic>2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</italic>, 1433&#x02013;1436 (IEEE, 2017).</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Maxhuni</surname><given-names>A</given-names></name><etal/></person-group><article-title>Classification of bipolar disorder episodes based on analysis of voice and motor activity of patients</article-title><source>Pervasive Mobile Comput.g</source><year>2016</year><volume>31</volume><fpage>50</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.pmcj.2016.01.008</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Maxhuni, A. et al. Classification of bipolar disorder episodes based on analysis of voice and motor activity of patients. <italic>Pervasive Mobile Comput.g</italic><bold>31</bold>, 50&#x02013;66 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Amos, B., Ludwiczuk, B. &#x00026; Satyanarayanan, M. <italic>Openface: A General-purpose Face Recognition Library With Mobile Applications</italic>. Tech. Rep., CMU-CS-16-118 (CMU School of Computer Science, 2016).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Rahaman, M. A. et al. Multi-modal deep learning of functional and structural neuroimaging and genomic data to predict mental illness. In <italic>2021 43rd Annual International Conference of the IEEE Engineering in Medicine &#x00026; Biology Society (EMBC)</italic>, 3267&#x02013;3272 (IEEE, 2021).</mixed-citation></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Chiong</surname><given-names>R</given-names></name><name><surname>Budhi</surname><given-names>GS</given-names></name><name><surname>Dhakal</surname><given-names>S</given-names></name><name><surname>Chiong</surname><given-names>F</given-names></name></person-group><article-title>A textual-based featuring approach for depression detection using machine learning classifiers and social media texts</article-title><source>Comput. Biol. Med.</source><year>2021</year><volume>135</volume><fpage>104499</fpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104499</pub-id><pub-id pub-id-type="pmid">34174760</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Chiong, R., Budhi, G. S., Dhakal, S. &#x00026; Chiong, F. A textual-based featuring approach for depression detection using machine learning classifiers and social media texts. <italic>Comput. Biol. Med.</italic><bold>135</bold>, 104499 (2021).<pub-id pub-id-type="pmid">34174760</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Paul, S., Jandhyala, S. K. &#x00026; Basu, T. Early detection of signs of anorexia and depression over social media using effective machine learning frameworks. In <italic>Working Notes of {CLEF} 2018 - Conference and Labs of the Evaluation Forum, Avignon, France</italic>, Vol. 2125 (eds. Cappellato, L., Ferro, N., Nie, J.-Y. &#x00026; Soulier, L.) (CEUR-WS.org, 2018). <ext-link ext-link-type="uri" xlink:href="https://dblp.org/rec/conf/clef/PaulJB18.bib">https://dblp.org/rec/conf/clef/PaulJB18.bib</ext-link>.</mixed-citation></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Mohr</surname><given-names>DC</given-names></name><etal/></person-group><article-title>Perceived barriers to psychological treatments and their relationship to depression</article-title><source>J. Clin. Psychol.</source><year>2010</year><volume>66</volume><fpage>394</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1002/jclp.20659</pub-id><pub-id pub-id-type="pmid">20127795</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Mohr, D. C. et al. Perceived barriers to psychological treatments and their relationship to depression. <italic>J. Clin. Psychol.</italic><bold>66</bold>, 394&#x02013;409 (2010).<pub-id pub-id-type="pmid">20127795</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Docherty</surname><given-names>JP</given-names></name></person-group><article-title>Barriers to the diagnosis of depression in primary care</article-title><source>J. Clin. Psychiatry</source><year>1997</year><volume>58</volume><fpage>5</fpage><lpage>10</lpage><pub-id pub-id-type="pmid">9054902</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Docherty, J. P. Barriers to the diagnosis of depression in primary care. <italic>J. Clin. Psychiatry</italic><bold>58</bold>, 5&#x02013;10 (1997).<pub-id pub-id-type="pmid">9054902</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Byatt</surname><given-names>N</given-names></name><name><surname>Simas</surname><given-names>TAM</given-names></name><name><surname>Lundquist</surname><given-names>RS</given-names></name><name><surname>Johnson</surname><given-names>JV</given-names></name><name><surname>Ziedonis</surname><given-names>DM</given-names></name></person-group><article-title>Strategies for improving perinatal depression treatment in North American outpatient obstetric settings</article-title><source>J. Psychosom. Obstet. Gynecol.</source><year>2012</year><volume>33</volume><fpage>143</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.3109/0167482X.2012.728649</pub-id></element-citation><mixed-citation id="mc-CR35" publication-type="journal">Byatt, N., Simas, T. A. M., Lundquist, R. S., Johnson, J. V. &#x00026; Ziedonis, D. M. Strategies for improving perinatal depression treatment in North American outpatient obstetric settings. <italic>J. Psychosom. Obstet. Gynecol.</italic><bold>33</bold>, 143&#x02013;161 (2012).</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Chekroud</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Cross-trial prediction of treatment outcome in depression: a machine learning approach</article-title><source>Lancet Psychiatry</source><year>2016</year><volume>3</volume><fpage>243</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/S2215-0366(15)00471-X</pub-id><pub-id pub-id-type="pmid">26803397</pub-id>
</element-citation><mixed-citation id="mc-CR36" publication-type="journal">Chekroud, A. M. et al. Cross-trial prediction of treatment outcome in depression: a machine learning approach. <italic>Lancet Psychiatry</italic><bold>3</bold>, 243&#x02013;250 (2016).<pub-id pub-id-type="pmid">26803397</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Mira</surname><given-names>A</given-names></name><etal/></person-group><article-title>An internet-based program for depressive symptoms using human and automated support: a randomized controlled trial</article-title><source>Neuropsych. Dis. Treat.</source><year>2017</year><volume>13</volume><fpage>987</fpage><pub-id pub-id-type="doi">10.2147/NDT.S130994</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Mira, A. et al. An internet-based program for depressive symptoms using human and automated support: a randomized controlled trial. <italic>Neuropsych. Dis. Treat.</italic><bold>13</bold>, 987 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>R</given-names></name><etal/></person-group><article-title>The initial field trials of DSM-5: new blooms and old thorns</article-title><source>Am. J. Psychiatry</source><year>2013</year><volume>170</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.2012.12091189</pub-id><pub-id pub-id-type="pmid">23288382</pub-id>
</element-citation><mixed-citation id="mc-CR38" publication-type="journal">Freedman, R. et al. The initial field trials of DSM-5: new blooms and old thorns. <italic>Am. J. Psychiatry</italic><bold>170</bold>, 1&#x02013;5 (2013).<pub-id pub-id-type="pmid">23288382</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Regier</surname><given-names>DA</given-names></name><etal/></person-group><article-title>Dsm-5 field trials in the United States and Canada, part ii: test-retest reliability of selected categorical diagnoses</article-title><source>Am. J. Psychiatry</source><year>2013</year><volume>170</volume><fpage>59</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.2012.12070999</pub-id><pub-id pub-id-type="pmid">23111466</pub-id>
</element-citation><mixed-citation id="mc-CR39" publication-type="journal">Regier, D. A. et al. Dsm-5 field trials in the United States and Canada, part ii: test-retest reliability of selected categorical diagnoses. <italic>Am. J. Psychiatry</italic><bold>170</bold>, 59&#x02013;70 (2013).<pub-id pub-id-type="pmid">23111466</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Yehuda</surname><given-names>R</given-names></name></person-group><article-title>Post-traumatic stress disorder</article-title><source>New Engl. J. Med.</source><year>2002</year><volume>346</volume><fpage>108</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1056/NEJMra012941</pub-id><pub-id pub-id-type="pmid">11784878</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Yehuda, R. Post-traumatic stress disorder. <italic>New Engl. J. Med.</italic><bold>346</bold>, 108&#x02013;114 (2002).<pub-id pub-id-type="pmid">11784878</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Cummins</surname><given-names>N</given-names></name><etal/></person-group><article-title>A review of depression and suicide risk assessment using speech analysis</article-title><source>Speech Commun.</source><year>2015</year><volume>71</volume><fpage>10</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.specom.2015.03.004</pub-id></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Cummins, N. et al. A review of depression and suicide risk assessment using speech analysis. <italic>Speech Commun.</italic><bold>71</bold>, 10&#x02013;49 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Insel</surname><given-names>TR</given-names></name></person-group><article-title>The Nimh research domain criteria (RDOC) project: precision medicine for psychiatry</article-title><source>Am. J. Psychiatry</source><year>2014</year><volume>171</volume><fpage>395</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.2014.14020138</pub-id><pub-id pub-id-type="pmid">24687194</pub-id>
</element-citation><mixed-citation id="mc-CR42" publication-type="journal">Insel, T. R. The Nimh research domain criteria (RDOC) project: precision medicine for psychiatry. <italic>Am. J. Psychiatry</italic><bold>171</bold>, 395&#x02013;397 (2014).<pub-id pub-id-type="pmid">24687194</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Bzdok</surname><given-names>D</given-names></name><name><surname>Meyer-Lindenberg</surname><given-names>A</given-names></name></person-group><article-title>Machine learning for precision psychiatry: opportunities and challenges</article-title><source>Biol. Psychiatry Cogn. Neurosci. Neuroimag.</source><year>2018</year><volume>3</volume><fpage>223</fpage><lpage>230</lpage></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Bzdok, D. &#x00026; Meyer-Lindenberg, A. Machine learning for precision psychiatry: opportunities and challenges. <italic>Biol. Psychiatry Cogn. Neurosci. Neuroimag.</italic><bold>3</bold>, 223&#x02013;230 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Vieira</surname><given-names>S</given-names></name><name><surname>Pinaya</surname><given-names>WH</given-names></name><name><surname>Mechelli</surname><given-names>A</given-names></name></person-group><article-title>Using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders: methods and applications</article-title><source>Neurosci. Biobehav. Rev.</source><year>2017</year><volume>74</volume><fpage>58</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.01.002</pub-id><pub-id pub-id-type="pmid">28087243</pub-id>
</element-citation><mixed-citation id="mc-CR44" publication-type="journal">Vieira, S., Pinaya, W. H. &#x00026; Mechelli, A. Using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders: methods and applications. <italic>Neurosci. Biobehav. Rev.</italic><bold>74</bold>, 58&#x02013;75 (2017).<pub-id pub-id-type="pmid">28087243</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Heinsfeld</surname><given-names>AS</given-names></name><name><surname>Franco</surname><given-names>AR</given-names></name><name><surname>Craddock</surname><given-names>RC</given-names></name><name><surname>Buchweitz</surname><given-names>A</given-names></name><name><surname>Meneguzzi</surname><given-names>F</given-names></name></person-group><article-title>Identification of autism spectrum disorder using deep learning and the abide dataset</article-title><source>NeuroImage Clin.</source><year>2018</year><volume>17</volume><fpage>16</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2017.08.017</pub-id><pub-id pub-id-type="pmid">29034163</pub-id>
</element-citation><mixed-citation id="mc-CR45" publication-type="journal">Heinsfeld, A. S., Franco, A. R., Craddock, R. C., Buchweitz, A. &#x00026; Meneguzzi, F. Identification of autism spectrum disorder using deep learning and the abide dataset. <italic>NeuroImage Clin.</italic><bold>17</bold>, 16&#x02013;23 (2018).<pub-id pub-id-type="pmid">29034163</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Nahum-Shani</surname><given-names>I</given-names></name><etal/></person-group><article-title>Just-in-time adaptive interventions (jitais) in mobile health: key components and design principles for ongoing health behavior support</article-title><source>Annal. Behav. Med.</source><year>2018</year><volume>52</volume><fpage>446</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1007/s12160-016-9830-8</pub-id></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Nahum-Shani, I. et al. Just-in-time adaptive interventions (jitais) in mobile health: key components and design principles for ongoing health behavior support. <italic>Annal. Behav. Med.</italic><bold>52</bold>, 446&#x02013;462 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Andrade</surname><given-names>L</given-names></name><etal/></person-group><article-title>The epidemiology of major depressive episodes: results from the international consortium of psychiatric epidemiology (ICPE) surveys</article-title><source>Int. J. Methods Psychiatric Res.</source><year>2003</year><volume>12</volume><fpage>3</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1002/mpr.138</pub-id></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Andrade, L. et al. The epidemiology of major depressive episodes: results from the international consortium of psychiatric epidemiology (ICPE) surveys. <italic>Int. J. Methods Psychiatric Res.</italic><bold>12</bold>, 3&#x02013;21 (2003).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name><surname>Girgus</surname><given-names>JS</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Ferri</surname><given-names>CV</given-names></name></person-group><article-title>The gender difference in depression: are elderly women at greater risk for depression than elderly men?</article-title><source>Geriatrics</source><year>2017</year><volume>2</volume><fpage>35</fpage><pub-id pub-id-type="doi">10.3390/geriatrics2040035</pub-id><pub-id pub-id-type="pmid">31011045</pub-id>
</element-citation><mixed-citation id="mc-CR48" publication-type="journal">Girgus, J. S., Yang, K. &#x00026; Ferri, C. V. The gender difference in depression: are elderly women at greater risk for depression than elderly men? <italic>Geriatrics</italic><bold>2</bold>, 35 (2017).<pub-id pub-id-type="pmid">31011045</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name><surname>Schuch</surname><given-names>JJ</given-names></name><name><surname>Roest</surname><given-names>AM</given-names></name><name><surname>Nolen</surname><given-names>WA</given-names></name><name><surname>Penninx</surname><given-names>BW</given-names></name><name><surname>De Jonge</surname><given-names>P</given-names></name></person-group><article-title>Gender differences in major depressive disorder: results from the Netherlands study of depression and anxiety</article-title><source>J. Affect. Disord.</source><year>2014</year><volume>156</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.jad.2013.12.011</pub-id><pub-id pub-id-type="pmid">24388685</pub-id>
</element-citation><mixed-citation id="mc-CR49" publication-type="journal">Schuch, J. J., Roest, A. M., Nolen, W. A., Penninx, B. W. &#x00026; De Jonge, P. Gender differences in major depressive disorder: results from the Netherlands study of depression and anxiety. <italic>J. Affect. Disord.</italic><bold>156</bold>, 156&#x02013;163 (2014).<pub-id pub-id-type="pmid">24388685</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>W</given-names></name><name><surname>Ping</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name></person-group><article-title>Gender differences in depression, anxiety, and stress among college students: a longitudinal study from China</article-title><source>J. Affect. Disord.</source><year>2020</year><volume>263</volume><fpage>292</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.jad.2019.11.121</pub-id><pub-id pub-id-type="pmid">31818792</pub-id>
</element-citation><mixed-citation id="mc-CR50" publication-type="journal">Gao, W., Ping, S. &#x00026; Liu, X. Gender differences in depression, anxiety, and stress among college students: a longitudinal study from China. <italic>J. Affect. Disord.</italic><bold>263</bold>, 292&#x02013;300 (2020).<pub-id pub-id-type="pmid">31818792</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Albert</surname><given-names>PR</given-names></name></person-group><article-title>Why is depression more prevalent in women?</article-title><source>J. Psychiatry Neurosci. JPN</source><year>2015</year><volume>40</volume><fpage>219</fpage><pub-id pub-id-type="doi">10.1503/jpn.150205</pub-id><pub-id pub-id-type="pmid">26107348</pub-id>
</element-citation><mixed-citation id="mc-CR51" publication-type="journal">Albert, P. R. Why is depression more prevalent in women? <italic>J. Psychiatry Neurosci. JPN</italic><bold>40</bold>, 219 (2015).<pub-id pub-id-type="pmid">26107348</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Kraepelin, E. <italic>Manic-Depressive Insanity and Paranoia</italic> (E. &#x00026; S. Livingstone, 1921).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Kumar, M., Dredze, M., Coppersmith, G. &#x00026; De Choudhury, M. Detecting changes in suicide content manifested in social media following celebrity suicides. In <italic>Proceedings of the 26th ACM Conference on Hypertext &#x00026; Social Media</italic>, 85&#x02013;94 (ACM, 2015).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Pirina, I. &#x00026; &#x000c7;&#x000f6;ltekin, &#x000c7;. Identifying depression on Reddit: the effect of training data. In <italic>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &#x00026; Shared Task</italic>, 9&#x02013;12 (Association for Computational Linguistics (ACL), 2018).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Yates, A., Cohan, A. &#x00026; Goharian, N. Depression and self-harm risk assessment in online forums. In <italic>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</italic>. 2968&#x02013;2978 (Association for Computational Linguistics, Copenhagen, Denmark, 2017). 10.18653/v1/D17-1322.</mixed-citation></ref><ref id="CR56"><label>56.</label><citation-alternatives><element-citation id="ec-CR56" publication-type="journal"><person-group person-group-type="author"><name><surname>Guntuku</surname><given-names>SC</given-names></name><name><surname>Yaden</surname><given-names>DB</given-names></name><name><surname>Kern</surname><given-names>ML</given-names></name><name><surname>Ungar</surname><given-names>LH</given-names></name><name><surname>Eichstaedt</surname><given-names>JC</given-names></name></person-group><article-title>Detecting depression and mental illness on social media: an integrative review</article-title><source>Curr. Opin. Behav. Sci.</source><year>2017</year><volume>18</volume><fpage>43</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2017.07.005</pub-id></element-citation><mixed-citation id="mc-CR56" publication-type="journal">Guntuku, S. C., Yaden, D. B., Kern, M. L., Ungar, L. H. &#x00026; Eichstaedt, J. C. Detecting depression and mental illness on social media: an integrative review. <italic>Curr. Opin. Behav. Sci.</italic><bold>18</bold>, 43&#x02013;49 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Arag&#x000f3;n, M. E., L&#x000f3;pez-Monroy, A. P., Gonz&#x000e1;lez-Gurrola, L. C. &#x00026; Montes, M. Detecting depression in social media using fine-grained emotions. In <italic>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</italic> 1481&#x02013;1486 (Association for Computational Linguistics (ACL), 2019).</mixed-citation></ref><ref id="CR58"><label>58.</label><citation-alternatives><element-citation id="ec-CR58" publication-type="journal"><person-group person-group-type="author"><name><surname>Tadesse</surname><given-names>MM</given-names></name><name><surname>Lin</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Yang</surname><given-names>L</given-names></name></person-group><article-title>Detection of depression-related posts in Reddit social media forum</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>44883</fpage><lpage>44893</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2909180</pub-id></element-citation><mixed-citation id="mc-CR58" publication-type="journal">Tadesse, M. M., Lin, H., Xu, B. &#x00026; Yang, L. Detection of depression-related posts in Reddit social media forum. <italic>IEEE Access</italic><bold>7</bold>, 44883&#x02013;44893 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">De Choudhury, M. &#x00026; De, S. Mental health discourse on Reddit: self-disclosure, social support, and anonymity. In <italic>Eighth International AAAI Conference on Weblogs and Social Media</italic> (MIT Press, 2014).</mixed-citation></ref><ref id="CR60"><label>60.</label><citation-alternatives><element-citation id="ec-CR60" publication-type="journal"><person-group person-group-type="author"><name><surname>Alghamdi</surname><given-names>NS</given-names></name><name><surname>Mahmoud</surname><given-names>HAH</given-names></name><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Alanazi</surname><given-names>SA</given-names></name><name><surname>Garc&#x000ed;a-Hern&#x000e1;ndez</surname><given-names>L</given-names></name></person-group><article-title>Predicting depression symptoms in an Arabic psychological forum</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>57317</fpage><lpage>57334</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2981834</pub-id></element-citation><mixed-citation id="mc-CR60" publication-type="journal">Alghamdi, N. S., Mahmoud, H. A. H., Abraham, A., Alanazi, S. A. &#x00026; Garc&#x000ed;a-Hern&#x000e1;ndez, L. Predicting depression symptoms in an Arabic psychological forum. <italic>IEEE Access</italic><bold>8</bold>, 57317&#x02013;57334 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Jan, A., Meng, H., Gaus, Y. F. A., Zhang, F. &#x00026; Turabzadeh, S. Automatic depression scale prediction using facial expression dynamics and regression. In <italic>Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</italic>, 73&#x02013;80 (Association for Computing Machinery (ACM), 2014).</mixed-citation></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name></person-group><article-title>Facial expression video analysis for depression detection in Chinese patients</article-title><source>J. Vis. Commun. Image Represent.</source><year>2018</year><volume>57</volume><fpage>228</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1016/j.jvcir.2018.11.003</pub-id></element-citation><mixed-citation id="mc-CR62" publication-type="journal">Wang, Q., Yang, H. &#x00026; Yu, Y. Facial expression video analysis for depression detection in Chinese patients. <italic>J. Vis. Commun. Image Represent.</italic><bold>57</bold>, 228&#x02013;233 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Hunter, L., Roland, L. &#x00026; Ferozpuri, A. Emotional expression processing and depressive symptomatology: eye-tracking reveals differential importance of lower and middle facial areas of interest. <italic>Depression Res. Treatment</italic><bold>2020</bold>, (2020).</mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Alghowinem, S., Goecke, R., Wagner, M., Parkerx, G. &#x00026; Breakspear, M. Head pose and movement analysis as an indicator of depression. In <italic>2013 Humaine Association Conference on Affective Computing and Intelligent Interaction</italic>, 283&#x02013;288 (IEEE, 2013).</mixed-citation></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>Low</surname><given-names>DM</given-names></name><name><surname>Bentley</surname><given-names>KH</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><article-title>Automated assessment of psychiatric disorders using speech: a systematic review</article-title><source>Laryngoscope Investig. Otolaryngol.</source><year>2020</year><volume>5</volume><fpage>96</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1002/lio2.354</pub-id><pub-id pub-id-type="pmid">32128436</pub-id>
</element-citation><mixed-citation id="mc-CR65" publication-type="journal">Low, D. M., Bentley, K. H. &#x00026; Ghosh, S. S. Automated assessment of psychiatric disorders using speech: a systematic review. <italic>Laryngoscope Investig. Otolaryngol.</italic><bold>5</bold>, 96&#x02013;116 (2020).<pub-id pub-id-type="pmid">32128436</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name><surname>Cummins</surname><given-names>N</given-names></name><name><surname>Sethu</surname><given-names>V</given-names></name><name><surname>Epps</surname><given-names>J</given-names></name><name><surname>Schnieder</surname><given-names>S</given-names></name><name><surname>Krajewski</surname><given-names>J</given-names></name></person-group><article-title>Analysis of acoustic space variability in speech affected by depression</article-title><source>Speech Commun.</source><year>2015</year><volume>75</volume><fpage>27</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.specom.2015.09.003</pub-id></element-citation><mixed-citation id="mc-CR66" publication-type="journal">Cummins, N., Sethu, V., Epps, J., Schnieder, S. &#x00026; Krajewski, J. Analysis of acoustic space variability in speech affected by depression. <italic>Speech Commun.</italic><bold>75</bold>, 27&#x02013;49 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Cummins, N., Sethu, V., Epps, J. &#x00026; Krajewski, J. Probabilistic acoustic volume analysis for speech affected by depression. In <italic>Fifteenth Annual Conference of the International Speech Communication Association.</italic> (International Speech Communication Association, 2014).</mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Harati, S., Crowell, A., Mayberg, H. &#x00026; Nemati, S. Depression severity classification from speech emotion. In <italic>2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</italic>, 5763&#x02013;5766 (IEEE, 2018).</mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Cummins, N., Vlasenko, B., Sagha, H. &#x00026; Schuller, B. Enhancing speech-based depression detection through gender dependent vowel-level formant features. In <italic>Conference on Artificial Intelligence in Medicine in Europe</italic>, 209&#x02013;214 (Springer, 2017).</mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="other">Morales, M. R. &#x00026; Levitan, R. Speech vs. text: a comparative analysis of features for depression detection systems. In <italic>2016 IEEE Spoken Language Technology Workshop (SLT)</italic>, 136&#x02013;143 (IEEE, 2016).</mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Vicsi, K., Sztah&#x000f3;, D. &#x00026; Kiss, G. Examination of the sensitivity of acoustic-phonetic parameters of speech to depression. In <italic>2012 IEEE 3rd International Conference on Cognitive Infocommunications (CogInfoCom)</italic>, 511&#x02013;515 (IEEE, 2012).</mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Kiss, G. &#x00026; Vicsi, K. Comparison of read and spontaneous speech in case of automatic detection of depression. In <italic>2017 8th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)</italic>, 000213&#x02013;000218 (IEEE, 2017).</mixed-citation></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="other">Stasak, B., Epps, J. &#x00026; Goecke, R. Elicitation design for acoustic depression classification: an investigation of articulation effort, linguistic complexity, and word affect. <italic>INTERSPEECH</italic>, 834&#x02013;838 (2017).</mixed-citation></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="other">Yazdavar, A. H. et al. Semi-supervised approach to monitoring clinical depressive symptoms in social media. In <italic>Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017</italic>, 1191&#x02013;1198 (IEEE/ACM, 2017).</mixed-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="other">Zogan, H., Razzak, I., Jameel, S. &#x00026; Xu, G. Depressionnet: a novel summarization boosted deep framework for depression detection on social media. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2105.10878">https://arxiv.org/abs/2105.10878</ext-link> (2021).</mixed-citation></ref><ref id="CR76"><label>76.</label><citation-alternatives><element-citation id="ec-CR76" publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Ananiadou</surname><given-names>S</given-names></name></person-group><article-title>A mental state knowledge&#x02013;aware and contrastive network for early stress and depression detection on social media</article-title><source>Inf. Process. Manag.</source><year>2022</year><volume>59</volume><fpage>102961</fpage><pub-id pub-id-type="doi">10.1016/j.ipm.2022.102961</pub-id></element-citation><mixed-citation id="mc-CR76" publication-type="journal">Yang, K., Zhang, T. &#x00026; Ananiadou, S. A mental state knowledge&#x02013;aware and contrastive network for early stress and depression detection on social media. <italic>Inf. Process. Manag.</italic><bold>59</bold>, 102961 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="other">Lara, J. S., Arag&#x000f3;n, M. E., Gonz&#x000e1;lez, F. A. &#x00026; Montes-y G&#x000f3;mez, M. Deep bag-of-sub-emotions for depression detection in social media. In <italic>Text, Speech, and Dialogue: 24th International Conference, TSD 2021, Olomouc, Czech Republic, September 6&#x02013;9, 2021, Proceedings 24</italic>, 60&#x02013;72 (Springer International Publishing, 2021).</mixed-citation></ref><ref id="CR78"><label>78.</label><citation-alternatives><element-citation id="ec-CR78" publication-type="journal"><person-group person-group-type="author"><name><surname>Figuer&#x000ea;do</surname><given-names>JSL</given-names></name><name><surname>Maia</surname><given-names>ALL</given-names></name><name><surname>Calumby</surname><given-names>RT</given-names></name></person-group><article-title>Early depression detection in social media based on deep learning and underlying emotions</article-title><source>Online Soc. Netw. Media</source><year>2022</year><volume>31</volume><fpage>100225</fpage><pub-id pub-id-type="doi">10.1016/j.osnem.2022.100225</pub-id></element-citation><mixed-citation id="mc-CR78" publication-type="journal">Figuer&#x000ea;do, J. S. L., Maia, A. L. L. &#x00026; Calumby, R. T. Early depression detection in social media based on deep learning and underlying emotions. <italic>Online Soc. Netw. Media</italic><bold>31</bold>, 100225 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR79"><label>79.</label><mixed-citation publication-type="other">Stankevich, M., Isakov, V., Devyatkin, D. &#x00026; Smirnov, I. V. Feature engineering for depression detection in social media. <italic>ICPRAM</italic>, 426&#x02013;431 (2018).</mixed-citation></ref><ref id="CR80"><label>80.</label><citation-alternatives><element-citation id="ec-CR80" publication-type="journal"><person-group person-group-type="author"><name><surname>de Jes&#x000fa;s Titla-Tlatelpa</surname><given-names>J</given-names></name><name><surname>Ortega-Mendoza</surname><given-names>RM</given-names></name><name><surname>Montes-y G&#x000f3;mez</surname><given-names>M</given-names></name><name><surname>Villase&#x000f1;or-Pineda</surname><given-names>L</given-names></name></person-group><article-title>A profile-based sentiment-aware approach for depression detection in social media</article-title><source>EPJ Data Sci.</source><year>2021</year><volume>10</volume><fpage>54</fpage><pub-id pub-id-type="doi">10.1140/epjds/s13688-021-00309-3</pub-id></element-citation><mixed-citation id="mc-CR80" publication-type="journal">de Jes&#x000fa;s Titla-Tlatelpa, J., Ortega-Mendoza, R. M., Montes-y G&#x000f3;mez, M. &#x00026; Villase&#x000f1;or-Pineda, L. A profile-based sentiment-aware approach for depression detection in social media. <italic>EPJ Data Sci.</italic><bold>10</bold>, 54 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR81"><label>81.</label><citation-alternatives><element-citation id="ec-CR81" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Mha: a multimodal hierarchical attention model for depression detection in social media</article-title><source>Health Inf. Sci. Syst.</source><year>2023</year><volume>11</volume><fpage>6</fpage><pub-id pub-id-type="doi">10.1007/s13755-022-00197-5</pub-id><pub-id pub-id-type="pmid">36660408</pub-id>
</element-citation><mixed-citation id="mc-CR81" publication-type="journal">Li, Z. et al. Mha: a multimodal hierarchical attention model for depression detection in social media. <italic>Health Inf. Sci. Syst.</italic><bold>11</bold>, 6 (2023).<pub-id pub-id-type="pmid">36660408</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR82"><label>82.</label><citation-alternatives><element-citation id="ec-CR82" publication-type="journal"><person-group person-group-type="author"><name><surname>Cha</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Park</surname><given-names>E</given-names></name></person-group><article-title>A lexicon-based approach to examine depression detection in social media: the case of Twitter and university community</article-title><source>Humanit. Soc. Sci. Commun.</source><year>2022</year><volume>9</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1057/s41599-022-01313-2</pub-id></element-citation><mixed-citation id="mc-CR82" publication-type="journal">Cha, J., Kim, S. &#x00026; Park, E. A lexicon-based approach to examine depression detection in social media: the case of Twitter and university community. <italic>Humanit. Soc. Sci. Commun.</italic><bold>9</bold>, 1&#x02013;10 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR83"><label>83.</label><citation-alternatives><element-citation id="ec-CR83" publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>B</given-names></name><etal/></person-group><article-title>Emotion-based reinforcement attention network for depression detection on social media: algorithm development and validation</article-title><source>JMIR Med. Informatics</source><year>2022</year><volume>10</volume><fpage>e37818</fpage><pub-id pub-id-type="doi">10.2196/37818</pub-id></element-citation><mixed-citation id="mc-CR83" publication-type="journal">Cui, B. et al. Emotion-based reinforcement attention network for depression detection on social media: algorithm development and validation. <italic>JMIR Med. Informatics</italic><bold>10</bold>, e37818 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="other">Guo, Z., Ding, N., Zhai, M., Zhang, Z. &#x00026; Li, Z. Leveraging domain knowledge to improve depression detection on Chinese social media. In <italic>IEEE Transactions on Computational Social Systems</italic> (IEEE, 2023).</mixed-citation></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="other">Hosseini-Saravani, S. H., Besharati, S., Calvo, H. &#x00026; Gelbukh, A. Depression detection in social media using a psychoanalytical technique for feature extraction and a cognitive based classifier. In <italic>Advances in Computational Intelligence: 19th Mexican International Conference on Artificial Intelligence, MICAI 2020, Mexico City, Mexico, October 12&#x02013;17, 2020, Proceedings, Part II</italic>, 282&#x02013;292 (Springer, 2020).</mixed-citation></ref><ref id="CR86"><label>86.</label><mixed-citation publication-type="other">Ramiandrisoa, F. &#x00026; Mothe, J. Early detection of depression and anorexia from social media: a machine learning approach. <italic>Circle</italic><bold>2621</bold>, 2020 (2020).</mixed-citation></ref><ref id="CR87"><label>87.</label><citation-alternatives><element-citation id="ec-CR87" publication-type="journal"><person-group person-group-type="author"><name><surname>Zogan</surname><given-names>H</given-names></name><name><surname>Razzak</surname><given-names>I</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Jameel</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>G</given-names></name></person-group><article-title>Explainable depression detection with multi-aspect features using a hybrid deep learning model on social media</article-title><source>World Wide Web</source><year>2022</year><volume>25</volume><fpage>281</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1007/s11280-021-00992-2</pub-id><pub-id pub-id-type="pmid">35106059</pub-id>
</element-citation><mixed-citation id="mc-CR87" publication-type="journal">Zogan, H., Razzak, I., Wang, X., Jameel, S. &#x00026; Xu, G. Explainable depression detection with multi-aspect features using a hybrid deep learning model on social media. <italic>World Wide Web</italic><bold>25</bold>, 281&#x02013;304 (2022).<pub-id pub-id-type="pmid">35106059</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR88"><label>88.</label><citation-alternatives><element-citation id="ec-CR88" publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>S</given-names></name><name><surname>Hudson</surname><given-names>CC</given-names></name><name><surname>Harkness</surname><given-names>K</given-names></name></person-group><article-title>Social media and depression symptoms: a meta-analysis</article-title><source>Res. Child Adolesc. Psychopathol.</source><year>2021</year><volume>49</volume><fpage>241</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1007/s10802-020-00715-7</pub-id><pub-id pub-id-type="pmid">33404948</pub-id>
</element-citation><mixed-citation id="mc-CR88" publication-type="journal">Cunningham, S., Hudson, C. C. &#x00026; Harkness, K. Social media and depression symptoms: a meta-analysis. <italic>Res. Child Adolesc. Psychopathol.</italic><bold>49</bold>, 241&#x02013;253 (2021).<pub-id pub-id-type="pmid">33404948</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR89"><label>89.</label><citation-alternatives><element-citation id="ec-CR89" publication-type="journal"><person-group person-group-type="author"><name><surname>Shensa</surname><given-names>A</given-names></name><etal/></person-group><article-title>Problematic social media use and depressive symptoms among us young adults: a nationally-representative study</article-title><source>Soc. Sci. Med.</source><year>2017</year><volume>182</volume><fpage>150</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1016/j.socscimed.2017.03.061</pub-id><pub-id pub-id-type="pmid">28446367</pub-id>
</element-citation><mixed-citation id="mc-CR89" publication-type="journal">Shensa, A. et al. Problematic social media use and depressive symptoms among us young adults: a nationally-representative study. <italic>Soc. Sci. Med.</italic><bold>182</bold>, 150&#x02013;157 (2017).<pub-id pub-id-type="pmid">28446367</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR90"><label>90.</label><citation-alternatives><element-citation id="ec-CR90" publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>HC</given-names></name><name><surname>Scott</surname><given-names>H</given-names></name></person-group><article-title># sleepyteens: social media use in adolescence is associated with poor sleep quality, anxiety, depression and low self-esteem</article-title><source>J. Adolesc.</source><year>2016</year><volume>51</volume><fpage>41</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.adolescence.2016.05.008</pub-id><pub-id pub-id-type="pmid">27294324</pub-id>
</element-citation><mixed-citation id="mc-CR90" publication-type="journal">Woods, H. C. &#x00026; Scott, H. # sleepyteens: social media use in adolescence is associated with poor sleep quality, anxiety, depression and low self-esteem. <italic>J. Adolesc.</italic><bold>51</bold>, 41&#x02013;49 (2016).<pub-id pub-id-type="pmid">27294324</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR91"><label>91.</label><citation-alternatives><element-citation id="ec-CR91" publication-type="journal"><person-group person-group-type="author"><name><surname>Radovic</surname><given-names>A</given-names></name><name><surname>Gmelin</surname><given-names>T</given-names></name><name><surname>Stein</surname><given-names>BD</given-names></name><name><surname>Miller</surname><given-names>E</given-names></name></person-group><article-title>Depressed adolescents&#x02019; positive and negative use of social media</article-title><source>J. Adolesc.</source><year>2017</year><volume>55</volume><fpage>5</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.adolescence.2016.12.002</pub-id><pub-id pub-id-type="pmid">27997851</pub-id>
</element-citation><mixed-citation id="mc-CR91" publication-type="journal">Radovic, A., Gmelin, T., Stein, B. D. &#x00026; Miller, E. Depressed adolescents&#x02019; positive and negative use of social media. <italic>J. Adolesc.</italic><bold>55</bold>, 5&#x02013;15 (2017).<pub-id pub-id-type="pmid">27997851</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR92"><label>92.</label><citation-alternatives><element-citation id="ec-CR92" publication-type="journal"><person-group person-group-type="author"><name><surname>Ivie</surname><given-names>EJ</given-names></name><name><surname>Pettitt</surname><given-names>A</given-names></name><name><surname>Moses</surname><given-names>LJ</given-names></name><name><surname>Allen</surname><given-names>NB</given-names></name></person-group><article-title>A meta-analysis of the association between adolescent social media use and depressive symptoms</article-title><source>J. Affect. Disord.</source><year>2020</year><volume>275</volume><fpage>165</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.jad.2020.06.014</pub-id><pub-id pub-id-type="pmid">32734903</pub-id>
</element-citation><mixed-citation id="mc-CR92" publication-type="journal">Ivie, E. J., Pettitt, A., Moses, L. J. &#x00026; Allen, N. B. A meta-analysis of the association between adolescent social media use and depressive symptoms. <italic>J. Affect. Disord.</italic><bold>275</bold>, 165&#x02013;174 (2020).<pub-id pub-id-type="pmid">32734903</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR93"><label>93.</label><citation-alternatives><element-citation id="ec-CR93" publication-type="journal"><person-group person-group-type="author"><name><surname>Raudsepp</surname><given-names>L</given-names></name><name><surname>Kais</surname><given-names>K</given-names></name></person-group><article-title>Longitudinal associations between problematic social media use and depressive symptoms in adolescent girls</article-title><source>Prev. Med. Rep.</source><year>2019</year><volume>15</volume><fpage>100925</fpage><pub-id pub-id-type="doi">10.1016/j.pmedr.2019.100925</pub-id><pub-id pub-id-type="pmid">31304081</pub-id>
</element-citation><mixed-citation id="mc-CR93" publication-type="journal">Raudsepp, L. &#x00026; Kais, K. Longitudinal associations between problematic social media use and depressive symptoms in adolescent girls. <italic>Prev. Med. Rep.</italic><bold>15</bold>, 100925 (2019).<pub-id pub-id-type="pmid">31304081</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR94"><label>94.</label><mixed-citation publication-type="other">Salas-Z&#x000e1;rate, R. et al. Detecting depression signs on social media: a systematic literature review. in <italic>Healthcare</italic>, Vol. 10, 291 (MDPI, 2022).</mixed-citation></ref><ref id="CR95"><label>95.</label><citation-alternatives><element-citation id="ec-CR95" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>D</given-names></name><etal/></person-group><article-title>Detecting and measuring depression on social media using a machine learning approach: systematic review</article-title><source>JMIR Mental Health</source><year>2022</year><volume>9</volume><fpage>e27244</fpage><pub-id pub-id-type="doi">10.2196/27244</pub-id><pub-id pub-id-type="pmid">35230252</pub-id>
</element-citation><mixed-citation id="mc-CR95" publication-type="journal">Liu, D. et al. Detecting and measuring depression on social media using a machine learning approach: systematic review. <italic>JMIR Mental Health</italic><bold>9</bold>, e27244 (2022).<pub-id pub-id-type="pmid">35230252</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR96"><label>96.</label><citation-alternatives><element-citation id="ec-CR96" publication-type="journal"><person-group person-group-type="author"><name><surname>McCrae</surname><given-names>N</given-names></name><name><surname>Gettings</surname><given-names>S</given-names></name><name><surname>Purssell</surname><given-names>E</given-names></name></person-group><article-title>Social media and depressive symptoms in childhood and adolescence: a systematic review</article-title><source>Adoles. Res. Rev.</source><year>2017</year><volume>2</volume><fpage>315</fpage><lpage>330</lpage></element-citation><mixed-citation id="mc-CR96" publication-type="journal">McCrae, N., Gettings, S. &#x00026; Purssell, E. Social media and depressive symptoms in childhood and adolescence: a systematic review. <italic>Adoles. Res. Rev.</italic><bold>2</bold>, 315&#x02013;330 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR97"><label>97.</label><citation-alternatives><element-citation id="ec-CR97" publication-type="journal"><person-group person-group-type="author"><name><surname>Heffer</surname><given-names>T</given-names></name><name><surname>Good</surname><given-names>M</given-names></name><name><surname>Daly</surname><given-names>O</given-names></name><name><surname>MacDonell</surname><given-names>E</given-names></name><name><surname>Willoughby</surname><given-names>T</given-names></name></person-group><article-title>The longitudinal association between social-media use and depressive symptoms among adolescents and young adults: an empirical reply to Twenge et al. (2018)</article-title><source>Clin. Psychol. Sci.</source><year>2019</year><volume>7</volume><fpage>462</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1177/2167702618812727</pub-id></element-citation><mixed-citation id="mc-CR97" publication-type="journal">Heffer, T., Good, M., Daly, O., MacDonell, E. &#x00026; Willoughby, T. The longitudinal association between social-media use and depressive symptoms among adolescents and young adults: an empirical reply to Twenge et al. (2018). <italic>Clin. Psychol. Sci.</italic><bold>7</bold>, 462&#x02013;470 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR98"><label>98.</label><citation-alternatives><element-citation id="ec-CR98" publication-type="journal"><person-group person-group-type="author"><name><surname>Ford</surname><given-names>E</given-names></name><name><surname>Curlewis</surname><given-names>K</given-names></name><name><surname>Wongkoblap</surname><given-names>A</given-names></name><name><surname>Curcin</surname><given-names>V</given-names></name><etal/></person-group><article-title>Public opinions on using social media content to identify users with depression and target mental health care advertising: mixed methods survey</article-title><source>JMIR Mental Health</source><year>2019</year><volume>6</volume><fpage>e12942</fpage><pub-id pub-id-type="doi">10.2196/12942</pub-id><pub-id pub-id-type="pmid">31719022</pub-id>
</element-citation><mixed-citation id="mc-CR98" publication-type="journal">Ford, E., Curlewis, K., Wongkoblap, A. &#x00026; Curcin, V. et al. Public opinions on using social media content to identify users with depression and target mental health care advertising: mixed methods survey. <italic>JMIR Mental Health</italic><bold>6</bold>, e12942 (2019).<pub-id pub-id-type="pmid">31719022</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR99"><label>99.</label><citation-alternatives><element-citation id="ec-CR99" publication-type="journal"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>F</given-names></name><name><surname>Bi</surname><given-names>F</given-names></name><name><surname>Jiao</surname><given-names>R</given-names></name><name><surname>Luo</surname><given-names>D</given-names></name><name><surname>Song</surname><given-names>K</given-names></name></person-group><article-title>Gender differences of depression and anxiety among social media users during the covid-19 outbreak in China: a cross-sectional study</article-title><source>BMC Public Health</source><year>2020</year><volume>20</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1186/s12889-020-09738-7</pub-id><pub-id pub-id-type="pmid">31898494</pub-id>
</element-citation><mixed-citation id="mc-CR99" publication-type="journal">Hou, F., Bi, F., Jiao, R., Luo, D. &#x00026; Song, K. Gender differences of depression and anxiety among social media users during the covid-19 outbreak in China: a cross-sectional study. <italic>BMC Public Health</italic><bold>20</bold>, 1&#x02013;11 (2020).<pub-id pub-id-type="pmid">31898494</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR100"><label>100.</label><mixed-citation publication-type="other">De Choudhury, M., Counts, S. &#x00026; Horvitz, E. Social media as a measurement tool of depression in populations. In <italic>Proceedings of the 5th Annual ACM Web Science Conference</italic>, 47&#x02013;56 (ACM, 2013).</mixed-citation></ref><ref id="CR101"><label>101.</label><citation-alternatives><element-citation id="ec-CR101" publication-type="journal"><person-group person-group-type="author"><name><surname>Narynov</surname><given-names>S</given-names></name><name><surname>Mukhtarkhanuly</surname><given-names>D</given-names></name><name><surname>Omarov</surname><given-names>B</given-names></name></person-group><article-title>Dataset of depressive posts in Russian language collected from social media</article-title><source>Data Brief</source><year>2020</year><volume>29</volume><fpage>105195</fpage><pub-id pub-id-type="doi">10.1016/j.dib.2020.105195</pub-id><pub-id pub-id-type="pmid">32083154</pub-id>
</element-citation><mixed-citation id="mc-CR101" publication-type="journal">Narynov, S., Mukhtarkhanuly, D. &#x00026; Omarov, B. Dataset of depressive posts in Russian language collected from social media. <italic>Data Brief</italic><bold>29</bold>, 105195 (2020).<pub-id pub-id-type="pmid">32083154</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR102"><label>102.</label><mixed-citation publication-type="other">Li, X., Guo, W. &#x00026; Yang, H. Depression severity prediction from facial expression based on the drr_depressionnet network. In <italic>2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</italic>, 2757&#x02013;2764 (IEEE, 2020).</mixed-citation></ref><ref id="CR103"><label>103.</label><mixed-citation publication-type="other">Hao, Y., Cao, Y., Li, B. &#x00026; Rahman, M. Depression recognition based on text and facial expression. In <italic>International Symposium on Artificial Intelligence and Robotics 2021</italic>, vol. 11884, 513&#x02013;522 (SPIE, 2021).</mixed-citation></ref><ref id="CR104"><label>104.</label><citation-alternatives><element-citation id="ec-CR104" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Pra-net: Part-and-relation attention network for depression recognition from facial expression</article-title><source>Comput. Biol. Med.</source><year>2023</year><volume>157</volume><fpage>106589</fpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.106589</pub-id><pub-id pub-id-type="pmid">36934531</pub-id>
</element-citation><mixed-citation id="mc-CR104" publication-type="journal">Liu, Z. et al. Pra-net: Part-and-relation attention network for depression recognition from facial expression. <italic>Comput. Biol. Med.</italic><bold>157</bold>, 106589 (2023).<pub-id pub-id-type="pmid">36934531</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR105"><label>105.</label><citation-alternatives><element-citation id="ec-CR105" publication-type="journal"><person-group person-group-type="author"><name><surname>Hamid</surname><given-names>DSBA</given-names></name><name><surname>Goyal</surname><given-names>S</given-names></name><name><surname>Bedi</surname><given-names>P</given-names></name></person-group><article-title>Integration of deep learning for improved diagnosis of depression using eeg and facial features</article-title><source>Mater. Today Proc.</source><year>2023</year><volume>80</volume><fpage>1965</fpage><lpage>1969</lpage><pub-id pub-id-type="doi">10.1016/j.matpr.2021.05.659</pub-id></element-citation><mixed-citation id="mc-CR105" publication-type="journal">Hamid, D. S. B. A., Goyal, S. &#x00026; Bedi, P. Integration of deep learning for improved diagnosis of depression using eeg and facial features. <italic>Mater. Today Proc.</italic><bold>80</bold>, 1965&#x02013;1969 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR106"><label>106.</label><mixed-citation publication-type="other">Nasir, M., Jati, A., Shivakumar, P. G., Nallan Chakravarthula, S. &#x00026; Georgiou, P. Multimodal and multiresolution depression detection from speech and facial landmark features. In <italic>Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge</italic>, 43&#x02013;50 (Association for Computing Machinery (ACM), 2016).</mixed-citation></ref><ref id="CR107"><label>107.</label><mixed-citation publication-type="other">Dai, Z., Li, Q., Shang, Y. &#x00026; Wang, X. Depression detection based on facial expression, audio and gait. In <italic>2023 IEEE 6th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)</italic>, vol. 6, 1568&#x02013;1573 (IEEE, 2023).</mixed-citation></ref><ref id="CR108"><label>108.</label><mixed-citation publication-type="other">Shangguan, Z. et al. Dual-stream multiple instance learning for depression detection with facial expression videos. In <italic>IEEE Transactions on Neural Systems and Rehabilitation Engineering</italic> (IEEE, 2022).</mixed-citation></ref><ref id="CR109"><label>109.</label><mixed-citation publication-type="other">Rodrigues Makiuchi, M., Warnita, T., Uto, K. &#x00026; Shinoda, K. Multimodal fusion of bert-cnn and gated cnn representations for depression detection. In <italic>Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop</italic>, 55&#x02013;63 (Association for Computing Machinery (ACM), 2019).</mixed-citation></ref><ref id="CR110"><label>110.</label><mixed-citation publication-type="other">Yin, S., Liang, C., Ding, H. &#x00026; Wang, S. A multi-modal hierarchical recurrent neural network for depression detection. In <italic>Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop</italic>, 65&#x02013;71 (Association for Computing Machinery (ACM), 2019).</mixed-citation></ref><ref id="CR111"><label>111.</label><mixed-citation publication-type="other">Zhang, L., Driscol, J., Chen, X. &#x00026; Hosseini Ghomi, R. Evaluating acoustic and linguistic features of detecting depression sub-challenge dataset. In <italic>Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop</italic>, 47&#x02013;53 (Association for Computing Machinery (ACM), 2019).</mixed-citation></ref><ref id="CR112"><label>112.</label><citation-alternatives><element-citation id="ec-CR112" publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Fairbairn</surname><given-names>C</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><article-title>Detecting depression severity from vocal prosody</article-title><source>IEEE Trans. Affect. Comput.</source><year>2012</year><volume>4</volume><fpage>142</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2012.38</pub-id></element-citation><mixed-citation id="mc-CR112" publication-type="journal">Yang, Y., Fairbairn, C. &#x00026; Cohn, J. F. Detecting depression severity from vocal prosody. <italic>IEEE Trans. Affect. Comput.</italic><bold>4</bold>, 142&#x02013;150 (2012).</mixed-citation></citation-alternatives></ref><ref id="CR113"><label>113.</label><citation-alternatives><element-citation id="ec-CR113" publication-type="journal"><person-group person-group-type="author"><name><surname>McGinnis</surname><given-names>EW</given-names></name><etal/></person-group><article-title>Giving voice to vulnerable children: machine learning analysis of speech detects anxiety and depression in early childhood</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2019</year><volume>23</volume><fpage>2294</fpage><lpage>2301</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2019.2913590</pub-id><pub-id pub-id-type="pmid">31034426</pub-id>
</element-citation><mixed-citation id="mc-CR113" publication-type="journal">McGinnis, E. W. et al. Giving voice to vulnerable children: machine learning analysis of speech detects anxiety and depression in early childhood. <italic>IEEE J. Biomed. Health Inform.</italic><bold>23</bold>, 2294&#x02013;2301 (2019).<pub-id pub-id-type="pmid">31034426</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR114"><label>114.</label><mixed-citation publication-type="other">Sanchez, M. H. et al. Using prosodic and spectral features in detecting depression in elderly males. In <italic>Twelfth Annual Conference of the International Speech Communication Association</italic> (International Speech Communication Association, 2011).</mixed-citation></ref><ref id="CR115"><label>115.</label><mixed-citation publication-type="other">Silva, W. J., Lopes, L., Galdino, M. K. C. &#x00026; Almeida, A. A. Voice acoustic parameters as predictors of depression. <italic>J. Voice</italic> (2021).</mixed-citation></ref><ref id="CR116"><label>116.</label><citation-alternatives><element-citation id="ec-CR116" publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>M</given-names></name><name><surname>Dietrich</surname><given-names>BJ</given-names></name><name><surname>Bai</surname><given-names>E-w</given-names></name><name><surname>Bockholt</surname><given-names>HJ</given-names></name></person-group><article-title>Vocal pattern detection of depression among older adults</article-title><source>Int. J. Mental Health Nurs.</source><year>2020</year><volume>29</volume><fpage>440</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1111/inm.12678</pub-id></element-citation><mixed-citation id="mc-CR116" publication-type="journal">Smith, M., Dietrich, B. J., Bai, E.-w &#x00026; Bockholt, H. J. Vocal pattern detection of depression among older adults. <italic>Int. J. Mental Health Nurs.</italic><bold>29</bold>, 440&#x02013;449 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR117"><label>117.</label><mixed-citation publication-type="other">Asgari, M., Shafran, I. &#x00026; Sheeber, L. B. Inferring clinical depression from speech and spoken utterances. In <italic>2014 IEEE international workshop on Machine Learning for Signal Processing (MLSP)</italic>, 1&#x02013;5 (IEEE, 2014).</mixed-citation></ref><ref id="CR118"><label>118.</label><mixed-citation publication-type="other">Scherer, S., Stratou, G., Gratch, J. &#x00026; Morency, L.-P. Investigating voice quality as a speaker-independent indicator of depression and PTSD. <italic>Interspeech</italic> 847&#x02013;851 (2013).</mixed-citation></ref><ref id="CR119"><label>119.</label><citation-alternatives><element-citation id="ec-CR119" publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>W</given-names></name><etal/></person-group><article-title>Re-examining the robustness of voice features in predicting depression: compared with baseline of confounders</article-title><source>PLoS ONE</source><year>2019</year><volume>14</volume><fpage>e0218172</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0218172</pub-id><pub-id pub-id-type="pmid">31220113</pub-id>
</element-citation><mixed-citation id="mc-CR119" publication-type="journal">Pan, W. et al. Re-examining the robustness of voice features in predicting depression: compared with baseline of confounders. <italic>PLoS ONE</italic><bold>14</bold>, e0218172 (2019).<pub-id pub-id-type="pmid">31220113</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR120"><label>120.</label><mixed-citation publication-type="other">Gratch, J. et al. <italic>The Distress Analysis Interview Corpus of Human and Computer Interviews</italic>. Tech. Rep. (UNIVERSITY OF SOUTHERN CALIFORNIA LOS ANGELES, 2014).</mixed-citation></ref><ref id="CR121"><label>121.</label><mixed-citation publication-type="other">Valstar, M. et al. Avec 2014: 3d dimensional affect and depression recognition challenge. In <italic>Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</italic>, 3&#x02013;10 (Association for Computing Machinery (ACM), 2014).</mixed-citation></ref><ref id="CR122"><label>122.</label><mixed-citation publication-type="other">Valstar, M. et al. Avec 2013: the continuous audio/visual emotion and depression recognition challenge. In <italic>Proceedings of the 3rd ACM International Workshop on Audio/Visual Emotion Challenge</italic>, 3&#x02013;10 (ACM, 2013).</mixed-citation></ref><ref id="CR123"><label>123.</label><citation-alternatives><element-citation id="ec-CR123" publication-type="journal"><person-group person-group-type="author"><name><surname>De Hert</surname><given-names>M</given-names></name><name><surname>Detraux</surname><given-names>J</given-names></name><name><surname>Van Winkel</surname><given-names>R</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Correll</surname><given-names>CU</given-names></name></person-group><article-title>Metabolic and cardiovascular adverse effects associated with antipsychotic drugs</article-title><source>Nat. Rev. Endocrinol.</source><year>2012</year><volume>8</volume><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/nrendo.2011.156</pub-id></element-citation><mixed-citation id="mc-CR123" publication-type="journal">De Hert, M., Detraux, J., Van Winkel, R., Yu, W. &#x00026; Correll, C. U. Metabolic and cardiovascular adverse effects associated with antipsychotic drugs. <italic>Nat. Rev. Endocrinol.</italic><bold>8</bold>, 114&#x02013;126 (2012).</mixed-citation></citation-alternatives></ref><ref id="CR124"><label>124.</label><citation-alternatives><element-citation id="ec-CR124" publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname><given-names>J</given-names></name><name><surname>Aylett</surname><given-names>M</given-names></name><name><surname>Yanushevskaya</surname><given-names>I</given-names></name><name><surname>Gobl</surname><given-names>C</given-names></name></person-group><article-title>Phonetic feature extraction for context-sensitive glottal source processing</article-title><source>Speech Commun.</source><year>2014</year><volume>59</volume><fpage>10</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.specom.2013.12.003</pub-id></element-citation><mixed-citation id="mc-CR124" publication-type="journal">Kane, J., Aylett, M., Yanushevskaya, I. &#x00026; Gobl, C. Phonetic feature extraction for context-sensitive glottal source processing. <italic>Speech Commun.</italic><bold>59</bold>, 10&#x02013;21 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR125"><label>125.</label><mixed-citation publication-type="other">Alghowinem, S. et al. Detecting depression: a comparison between spontaneous and read speech. In <italic>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</italic>, 7547&#x02013;7551 (IEEE, 2013).</mixed-citation></ref><ref id="CR126"><label>126.</label><mixed-citation publication-type="other">DeVault, D. et al. Simsensei kiosk: a virtual human interviewer for healthcare decision support. In <italic>Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems</italic>, 1061&#x02013;1068 (Association for Computing Machinery (ACM), 2014).</mixed-citation></ref><ref id="CR127"><label>127.</label><mixed-citation publication-type="other">Hartholt, A. et al. All together now. In <italic>International Workshop on Intelligent Virtual Agents</italic>, 368&#x02013;381 (Springer, 2013).</mixed-citation></ref><ref id="CR128"><label>128.</label><citation-alternatives><element-citation id="ec-CR128" publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>C</given-names></name><etal/></person-group><article-title>Pilot randomised controlled trial of help4mood, an embodied virtual agent-based system to support treatment of depression</article-title><source>J. Telemed. Telecare</source><year>2016</year><volume>22</volume><fpage>348</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1177/1357633X15609793</pub-id><pub-id pub-id-type="pmid">26453910</pub-id>
</element-citation><mixed-citation id="mc-CR128" publication-type="journal">Burton, C. et al. Pilot randomised controlled trial of help4mood, an embodied virtual agent-based system to support treatment of depression. <italic>J. Telemed. Telecare</italic><bold>22</bold>, 348&#x02013;355 (2016).<pub-id pub-id-type="pmid">26453910</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR129"><label>129.</label><citation-alternatives><element-citation id="ec-CR129" publication-type="journal"><person-group person-group-type="author"><name><surname>Nemes</surname><given-names>V</given-names></name><name><surname>Nikolic</surname><given-names>D</given-names></name><name><surname>Barney</surname><given-names>A</given-names></name><name><surname>Garrard</surname><given-names>P</given-names></name></person-group><article-title>A feasibility study of speech recording using a contact microphone in patients with possible or probable Alzheimer&#x02019;s disease to detect and quantify repetitions in a natural setting</article-title><source>Alzheimer&#x02019;s Dementia</source><year>2012</year><volume>8</volume><fpage>P490</fpage><lpage>P491</lpage><pub-id pub-id-type="doi">10.1016/j.jalz.2012.05.1330</pub-id></element-citation><mixed-citation id="mc-CR129" publication-type="journal">Nemes, V., Nikolic, D., Barney, A. &#x00026; Garrard, P. A feasibility study of speech recording using a contact microphone in patients with possible or probable Alzheimer&#x02019;s disease to detect and quantify repetitions in a natural setting. <italic>Alzheimer&#x02019;s Dementia</italic><bold>8</bold>, P490&#x02013;P491 (2012).</mixed-citation></citation-alternatives></ref><ref id="CR130"><label>130.</label><citation-alternatives><element-citation id="ec-CR130" publication-type="journal"><person-group person-group-type="author"><name><surname>Aloshban</surname><given-names>N</given-names></name><name><surname>Esposito</surname><given-names>A</given-names></name><name><surname>Vinciarelli</surname><given-names>A</given-names></name></person-group><article-title>What you say or how you say it? depression detection through joint modeling of linguistic and acoustic aspects of speech</article-title><source>Cogn. Comput.</source><year>2022</year><volume>14</volume><fpage>1585</fpage><lpage>1598</lpage><pub-id pub-id-type="doi">10.1007/s12559-020-09808-3</pub-id></element-citation><mixed-citation id="mc-CR130" publication-type="journal">Aloshban, N., Esposito, A. &#x00026; Vinciarelli, A. What you say or how you say it? depression detection through joint modeling of linguistic and acoustic aspects of speech. <italic>Cogn. Comput.</italic><bold>14</bold>, 1585&#x02013;1598 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR131"><label>131.</label><citation-alternatives><element-citation id="ec-CR131" publication-type="journal"><person-group person-group-type="author"><name><surname>Saito</surname><given-names>T</given-names></name><name><surname>Rehmsmeier</surname><given-names>M</given-names></name></person-group><article-title>The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets</article-title><source>PLoS ONE</source><year>2015</year><volume>10</volume><fpage>e0118432</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0118432</pub-id><pub-id pub-id-type="pmid">25738806</pub-id>
</element-citation><mixed-citation id="mc-CR131" publication-type="journal">Saito, T. &#x00026; Rehmsmeier, M. The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets. <italic>PLoS ONE</italic><bold>10</bold>, e0118432 (2015).<pub-id pub-id-type="pmid">25738806</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR132"><label>132.</label><citation-alternatives><element-citation id="ec-CR132" publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>I</given-names></name><name><surname>Lin</surname><given-names>K</given-names></name></person-group><article-title>A concordance correlation coefficient to evaluate reproducibility</article-title><source>Biometrics</source><year>1989</year><volume>45</volume><fpage>255</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.2307/2532051</pub-id><pub-id pub-id-type="pmid">2720055</pub-id>
</element-citation><mixed-citation id="mc-CR132" publication-type="journal">Lawrence, I. &#x00026; Lin, K. A concordance correlation coefficient to evaluate reproducibility. <italic>Biometrics</italic><bold>45</bold>, 255&#x02013;268 (1989).<pub-id pub-id-type="pmid">2720055</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR133"><label>133.</label><citation-alternatives><element-citation id="ec-CR133" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Kaushal</surname><given-names>R</given-names></name><name><surname>Khullar</surname><given-names>D</given-names></name></person-group><article-title>Should health care demand interpretable artificial intelligence or accept &#x0201c;black box&#x0201d; medicine?</article-title><source>Ann. Intern. Med.</source><year>2020</year><volume>172</volume><fpage>59</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.7326/M19-2548</pub-id><pub-id pub-id-type="pmid">31842204</pub-id>
</element-citation><mixed-citation id="mc-CR133" publication-type="journal">Wang, F., Kaushal, R. &#x00026; Khullar, D. Should health care demand interpretable artificial intelligence or accept &#x0201c;black box&#x0201d; medicine? <italic>Ann. Intern. Med.</italic><bold>172</bold>, 59&#x02013;60 (2020).<pub-id pub-id-type="pmid">31842204</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR134"><label>134.</label><citation-alternatives><element-citation id="ec-CR134" publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>TP</given-names></name><name><surname>Jacobs</surname><given-names>S</given-names></name><name><surname>Senadeera</surname><given-names>M</given-names></name><name><surname>Le</surname><given-names>V</given-names></name><name><surname>Coghlan</surname><given-names>S</given-names></name></person-group><article-title>The three ghosts of medical AI: can the black-box present deliver?</article-title><source>Artif. Intell. Med.</source><year>2022</year><volume>124</volume><fpage>102158</fpage><pub-id pub-id-type="doi">10.1016/j.artmed.2021.102158</pub-id><pub-id pub-id-type="pmid">34511267</pub-id>
</element-citation><mixed-citation id="mc-CR134" publication-type="journal">Quinn, T. P., Jacobs, S., Senadeera, M., Le, V. &#x00026; Coghlan, S. The three ghosts of medical AI: can the black-box present deliver? <italic>Artif. Intell. Med.</italic><bold>124</bold>, 102158 (2022).<pub-id pub-id-type="pmid">34511267</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR135"><label>135.</label><mixed-citation publication-type="other">Sendak, M. et al. &#x0201c; the human body is a black box" supporting clinical decision-making with deep learning. In <italic>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</italic>, 99&#x02013;109 (Association for Computing Machinery (ACM), 2020).</mixed-citation></ref><ref id="CR136"><label>136.</label><citation-alternatives><element-citation id="ec-CR136" publication-type="journal"><person-group person-group-type="author"><name><surname>Lipton</surname><given-names>ZC</given-names></name></person-group><article-title>The mythos of model interpretability: in machine learning, the concept of interpretability is both important and slippery</article-title><source>Queue</source><year>2018</year><volume>16</volume><fpage>31</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1145/3236386.3241340</pub-id></element-citation><mixed-citation id="mc-CR136" publication-type="journal">Lipton, Z. C. The mythos of model interpretability: in machine learning, the concept of interpretability is both important and slippery. <italic>Queue</italic><bold>16</bold>, 31&#x02013;57 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR137"><label>137.</label><mixed-citation publication-type="other">Doshi-Velez, F. &#x00026; Kim, B. Towards a rigorous science of interpretable machine learning. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1702.08608">https://arxiv.org/abs/1702.08608</ext-link> (2017).</mixed-citation></ref><ref id="CR138"><label>138.</label><mixed-citation publication-type="other">Molnar, C. <italic>Interpretable Machine Learning</italic> (Lulu. com, 2020).</mixed-citation></ref><ref id="CR139"><label>139.</label><citation-alternatives><element-citation id="ec-CR139" publication-type="journal"><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>SM</given-names></name><name><surname>Lee</surname><given-names>S-I</given-names></name></person-group><article-title>A unified approach to interpreting model predictions</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>30</volume><fpage>4768</fpage><lpage>4777</lpage></element-citation><mixed-citation id="mc-CR139" publication-type="journal">Lundberg, S. M. &#x00026; Lee, S.-I. A unified approach to interpreting model predictions. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>30</bold>, 4768&#x02013;4777 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR140"><label>140.</label><citation-alternatives><element-citation id="ec-CR140" publication-type="journal"><person-group person-group-type="author"><name><surname>Rudin</surname><given-names>C</given-names></name></person-group><article-title>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</article-title><source>Nat. Mach. Intell.</source><year>2019</year><volume>1</volume><fpage>206</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/s42256-019-0048-x</pub-id><pub-id pub-id-type="pmid">35603010</pub-id>
</element-citation><mixed-citation id="mc-CR140" publication-type="journal">Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. <italic>Nat. Mach. Intell.</italic><bold>1</bold>, 206&#x02013;215 (2019).<pub-id pub-id-type="pmid">35603010</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR141"><label>141.</label><citation-alternatives><element-citation id="ec-CR141" publication-type="journal"><person-group person-group-type="author"><name><surname>Beam</surname><given-names>AL</given-names></name><name><surname>Manrai</surname><given-names>AK</given-names></name><name><surname>Ghassemi</surname><given-names>M</given-names></name></person-group><article-title>Challenges to the reproducibility of machine learning models in health care</article-title><source>J. Am. Med. Assoc.</source><year>2020</year><volume>323</volume><fpage>305</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1001/jama.2019.20866</pub-id></element-citation><mixed-citation id="mc-CR141" publication-type="journal">Beam, A. L., Manrai, A. K. &#x00026; Ghassemi, M. Challenges to the reproducibility of machine learning models in health care. <italic>J. Am. Med. Assoc.</italic><bold>323</bold>, 305&#x02013;306 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR142"><label>142.</label><citation-alternatives><element-citation id="ec-CR142" publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>MB</given-names></name><etal/></person-group><article-title>Reproducibility in machine learning for health research: still a ways to go</article-title><source>Sci. Transl. Med.</source><year>2021</year><volume>13</volume><fpage>eabb1655</fpage><pub-id pub-id-type="doi">10.1126/scitranslmed.abb1655</pub-id><pub-id pub-id-type="pmid">33762434</pub-id>
</element-citation><mixed-citation id="mc-CR142" publication-type="journal">McDermott, M. B. et al. Reproducibility in machine learning for health research: still a ways to go. <italic>Sci. Transl. Med.</italic><bold>13</bold>, eabb1655 (2021).<pub-id pub-id-type="pmid">33762434</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR143"><label>143.</label><mixed-citation publication-type="other">Blum, A. &#x00026; Hardt, M. The ladder: a reliable leaderboard for machine learning competitions. In <italic>International Conference on Machine Learning</italic>, 1006&#x02013;1014 (PMLR, 2015).</mixed-citation></ref><ref id="CR144"><label>144.</label><mixed-citation publication-type="other">Alghowinem, S., Goecke, R., Epps, J., Wagner, M. &#x00026; Cohn, J. F. Cross-cultural depression recognition from vocal biomarkers. <italic>Interspeech</italic>, 1943&#x02013;1947 (2016).</mixed-citation></ref><ref id="CR145"><label>145.</label><mixed-citation publication-type="other">Stasak, B. &#x00026; Epps, J. Differential performance of automatic speech-based depression classification across smartphones. In <italic>2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</italic>, 171&#x02013;175 (IEEE, 2017).</mixed-citation></ref><ref id="CR146"><label>146.</label><mixed-citation publication-type="other">Gideon, J., Provost, E. M. &#x00026; McInnis, M. Mood state prediction from speech of varying acoustic quality for individuals with bipolar disorder. In <italic>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 2359&#x02013;2363 (IEEE, 2016).</mixed-citation></ref><ref id="CR147"><label>147.</label><mixed-citation publication-type="other">Mitra, V. &#x00026; Shriberg, E. Effects of feature type, learning algorithm and speaking style for depression detection from speech. In <italic>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 4774&#x02013;4778 (IEEE, 2015).</mixed-citation></ref><ref id="CR148"><label>148.</label><citation-alternatives><element-citation id="ec-CR148" publication-type="journal"><person-group person-group-type="author"><name><surname>Custers</surname><given-names>B</given-names></name></person-group><article-title>Click here to consent forever: Expiry dates for informed consent</article-title><source>Big Data Soc.</source><year>2016</year><volume>3</volume><fpage>2053951715624935</fpage><pub-id pub-id-type="doi">10.1177/2053951715624935</pub-id></element-citation><mixed-citation id="mc-CR148" publication-type="journal">Custers, B. Click here to consent forever: Expiry dates for informed consent. <italic>Big Data Soc.</italic><bold>3</bold>, 2053951715624935 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR149"><label>149.</label><citation-alternatives><element-citation id="ec-CR149" publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>A</given-names></name><name><surname>Malik</surname><given-names>A</given-names></name><name><surname>Sikander</surname><given-names>S</given-names></name><name><surname>Roberts</surname><given-names>C</given-names></name><name><surname>Creed</surname><given-names>F</given-names></name></person-group><article-title>Cognitive behaviour therapy-based intervention by community health workers for mothers with depression and their infants in rural Pakistan: a cluster-randomised controlled trial</article-title><source>Lancet</source><year>2008</year><volume>372</volume><fpage>902</fpage><lpage>909</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(08)61400-2</pub-id><pub-id pub-id-type="pmid">18790313</pub-id>
</element-citation><mixed-citation id="mc-CR149" publication-type="journal">Rahman, A., Malik, A., Sikander, S., Roberts, C. &#x00026; Creed, F. Cognitive behaviour therapy-based intervention by community health workers for mothers with depression and their infants in rural Pakistan: a cluster-randomised controlled trial. <italic>Lancet</italic><bold>372</bold>, 902&#x02013;909 (2008).<pub-id pub-id-type="pmid">18790313</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR150"><label>150.</label><citation-alternatives><element-citation id="ec-CR150" publication-type="journal"><person-group person-group-type="author"><name><surname>Marmor</surname><given-names>S</given-names></name><name><surname>Horvath</surname><given-names>KJ</given-names></name><name><surname>Lim</surname><given-names>KO</given-names></name><name><surname>Misono</surname><given-names>S</given-names></name></person-group><article-title>Voice problems and depression among adults in the United States</article-title><source>Laryngoscope</source><year>2016</year><volume>126</volume><fpage>1859</fpage><lpage>1864</lpage><pub-id pub-id-type="doi">10.1002/lary.25819</pub-id><pub-id pub-id-type="pmid">26691195</pub-id>
</element-citation><mixed-citation id="mc-CR150" publication-type="journal">Marmor, S., Horvath, K. J., Lim, K. O. &#x00026; Misono, S. Voice problems and depression among adults in the United States. <italic>Laryngoscope</italic><bold>126</bold>, 1859&#x02013;1864 (2016).<pub-id pub-id-type="pmid">26691195</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR151"><label>151.</label><citation-alternatives><element-citation id="ec-CR151" publication-type="journal"><person-group person-group-type="author"><name><surname>Hartanto</surname><given-names>A</given-names></name><name><surname>Quek</surname><given-names>FY</given-names></name><name><surname>Tng</surname><given-names>GY</given-names></name><name><surname>Yong</surname><given-names>JC</given-names></name></person-group><article-title>Does social media use increase depressive symptoms? a reverse causation perspective</article-title><source>Front. Psychiatry</source><year>2021</year><volume>12</volume><fpage>641934</fpage><pub-id pub-id-type="doi">10.3389/fpsyt.2021.641934</pub-id><pub-id pub-id-type="pmid">33833700</pub-id>
</element-citation><mixed-citation id="mc-CR151" publication-type="journal">Hartanto, A., Quek, F. Y., Tng, G. Y. &#x00026; Yong, J. C. Does social media use increase depressive symptoms? a reverse causation perspective. <italic>Front. Psychiatry</italic><bold>12</bold>, 641934 (2021).<pub-id pub-id-type="pmid">33833700</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR152"><label>152.</label><citation-alternatives><element-citation id="ec-CR152" publication-type="journal"><person-group person-group-type="author"><name><surname>Hussain</surname><given-names>J</given-names></name><etal/></person-group><article-title>Exploring the dominant features of social media for depression detection</article-title><source>J. Inf. Sci.</source><year>2020</year><volume>46</volume><fpage>739</fpage><lpage>759</lpage><pub-id pub-id-type="doi">10.1177/0165551519860469</pub-id></element-citation><mixed-citation id="mc-CR152" publication-type="journal">Hussain, J. et al. Exploring the dominant features of social media for depression detection. <italic>J. Inf. Sci.</italic><bold>46</bold>, 739&#x02013;759 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR153"><label>153.</label><mixed-citation publication-type="other">Liaw, A. S. &#x00026; Chua, H. N. Depression detection on social media with user network and engagement features using machine learning methods. In <italic>2022 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)</italic>, 1&#x02013;6 (IEEE, 2022).</mixed-citation></ref><ref id="CR154"><label>154.</label><citation-alternatives><element-citation id="ec-CR154" publication-type="journal"><person-group person-group-type="author"><name><surname>Primack</surname><given-names>BA</given-names></name><etal/></person-group><article-title>Use of multiple social media platforms and symptoms of depression and anxiety: a nationally-representative study among us young adults</article-title><source>Comput. Hum. Behav.</source><year>2017</year><volume>69</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.chb.2016.11.013</pub-id></element-citation><mixed-citation id="mc-CR154" publication-type="journal">Primack, B. A. et al. Use of multiple social media platforms and symptoms of depression and anxiety: a nationally-representative study among us young adults. <italic>Comput. Hum. Behav.</italic><bold>69</bold>, 1&#x02013;9 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR155"><label>155.</label><citation-alternatives><element-citation id="ec-CR155" publication-type="journal"><person-group person-group-type="author"><name><surname>Primack</surname><given-names>BA</given-names></name><name><surname>Shensa</surname><given-names>A</given-names></name><name><surname>Sidani</surname><given-names>JE</given-names></name><name><surname>Escobar-Viera</surname><given-names>CG</given-names></name><name><surname>Fine</surname><given-names>MJ</given-names></name></person-group><article-title>Temporal associations between social media use and depression</article-title><source>Am. J. Prev. Med.</source><year>2021</year><volume>60</volume><fpage>179</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.amepre.2020.09.014</pub-id><pub-id pub-id-type="pmid">33309454</pub-id>
</element-citation><mixed-citation id="mc-CR155" publication-type="journal">Primack, B. A., Shensa, A., Sidani, J. E., Escobar-Viera, C. G. &#x00026; Fine, M. J. Temporal associations between social media use and depression. <italic>Am. J. Prev. Med.</italic><bold>60</bold>, 179&#x02013;188 (2021).<pub-id pub-id-type="pmid">33309454</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR156"><label>156.</label><mixed-citation publication-type="other">Vedula, N. &#x00026; Parthasarathy, S. Emotional and linguistic cues of depression from social media. In <italic>Proceedings of the 2017 International Conference on Digital Health</italic>, 127&#x02013;136 (Association for Computing Machinery (ACM), 2017).</mixed-citation></ref><ref id="CR157"><label>157.</label><mixed-citation publication-type="other">Nesi, J. et al. Emotional responses to social media experiences among adolescents: longitudinal associations with depressive symptoms. <italic>J. Clin. Child Adolesc. Psychol.</italic><bold>51</bold>, 907&#x02013;922 (2021).</mixed-citation></ref><ref id="CR158"><label>158.</label><citation-alternatives><element-citation id="ec-CR158" publication-type="journal"><person-group person-group-type="author"><name><surname>Thorisdottir</surname><given-names>IE</given-names></name><name><surname>Sigurvinsdottir</surname><given-names>R</given-names></name><name><surname>Asgeirsdottir</surname><given-names>BB</given-names></name><name><surname>Allegrante</surname><given-names>JP</given-names></name><name><surname>Sigfusdottir</surname><given-names>ID</given-names></name></person-group><article-title>Active and passive social media use and symptoms of anxiety and depressed mood among Icelandic adolescents</article-title><source>Cyberpsychol. Behav. Soc. Netw.</source><year>2019</year><volume>22</volume><fpage>535</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1089/cyber.2019.0079</pub-id><pub-id pub-id-type="pmid">31361508</pub-id>
</element-citation><mixed-citation id="mc-CR158" publication-type="journal">Thorisdottir, I. E., Sigurvinsdottir, R., Asgeirsdottir, B. B., Allegrante, J. P. &#x00026; Sigfusdottir, I. D. Active and passive social media use and symptoms of anxiety and depressed mood among Icelandic adolescents. <italic>Cyberpsychol. Behav. Soc. Netw.</italic><bold>22</bold>, 535&#x02013;542 (2019).<pub-id pub-id-type="pmid">31361508</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR159"><label>159.</label><citation-alternatives><element-citation id="ec-CR159" publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>S</given-names></name><name><surname>Anwar</surname><given-names>T</given-names></name></person-group><article-title>Depression intensity estimation via social media: a deep learning approach</article-title><source>IEEE Trans. Comput. Soc. Syst.</source><year>2021</year><volume>8</volume><fpage>1465</fpage><lpage>1474</lpage><pub-id pub-id-type="doi">10.1109/TCSS.2021.3084154</pub-id></element-citation><mixed-citation id="mc-CR159" publication-type="journal">Ghosh, S. &#x00026; Anwar, T. Depression intensity estimation via social media: a deep learning approach. <italic>IEEE Trans. Comput. Soc. Syst.</italic><bold>8</bold>, 1465&#x02013;1474 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR160"><label>160.</label><mixed-citation publication-type="other">Aragon, M. E., Lopez-Monroy, A. P., Gonzalez-Gurrola, L.-C. G. &#x00026; Montes, M. Detecting mental disorders in social media through emotional patterns-the case of anorexia and depression. In <italic>IEEE Transactions on Affective Computing</italic> (IEEE, 2021).</mixed-citation></ref><ref id="CR161"><label>161.</label><citation-alternatives><element-citation id="ec-CR161" publication-type="journal"><person-group person-group-type="author"><name><surname>Puukko</surname><given-names>K</given-names></name><name><surname>Hietaj&#x000e4;rvi</surname><given-names>L</given-names></name><name><surname>Maksniemi</surname><given-names>E</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Salmela-Aro</surname><given-names>K</given-names></name></person-group><article-title>Social media use and depressive symptoms-a longitudinal study from early to late adolescence</article-title><source>Int. J. Environ. Res. Public Health</source><year>2020</year><volume>17</volume><fpage>5921</fpage><pub-id pub-id-type="doi">10.3390/ijerph17165921</pub-id><pub-id pub-id-type="pmid">32824057</pub-id>
</element-citation><mixed-citation id="mc-CR161" publication-type="journal">Puukko, K., Hietaj&#x000e4;rvi, L., Maksniemi, E., Alho, K. &#x00026; Salmela-Aro, K. Social media use and depressive symptoms-a longitudinal study from early to late adolescence. <italic>Int. J. Environ. Res. Public Health</italic><bold>17</bold>, 5921 (2020).<pub-id pub-id-type="pmid">32824057</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR162"><label>162.</label><citation-alternatives><element-citation id="ec-CR162" publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>A</given-names></name><etal/></person-group><article-title>Social comparisons, social media addiction, and social interaction: an examination of specific social media behaviors related to major depressive disorder in a millennial population</article-title><source>J. Appl. Biobehav. Res.</source><year>2019</year><volume>24</volume><fpage>e12158</fpage><pub-id pub-id-type="doi">10.1111/jabr.12158</pub-id></element-citation><mixed-citation id="mc-CR162" publication-type="journal">Robinson, A. et al. Social comparisons, social media addiction, and social interaction: an examination of specific social media behaviors related to major depressive disorder in a millennial population. <italic>J. Appl. Biobehav. Res.</italic><bold>24</bold>, e12158 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR163"><label>163.</label><mixed-citation publication-type="other">De Choudhury, M., Gamon, M., Counts, S. &#x00026; Horvitz, E. Predicting depression via social media. In <italic>Seventh International AAAI Conference on Weblogs and Social Media</italic> (AAAI, 2013).</mixed-citation></ref><ref id="CR164"><label>164.</label><citation-alternatives><element-citation id="ec-CR164" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>B</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name></person-group><article-title>Mental health toll from the coronavirus: social media usage reveals Wuhan residents&#x02019; depression and secondary trauma in the covid-19 outbreak</article-title><source>Comput. Hum. Behav.</source><year>2021</year><volume>114</volume><fpage>106524</fpage><pub-id pub-id-type="doi">10.1016/j.chb.2020.106524</pub-id></element-citation><mixed-citation id="mc-CR164" publication-type="journal">Zhong, B., Huang, Y. &#x00026; Liu, Q. Mental health toll from the coronavirus: social media usage reveals Wuhan residents&#x02019; depression and secondary trauma in the covid-19 outbreak. <italic>Comput. Hum. Behav.</italic><bold>114</bold>, 106524 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR165"><label>165.</label><citation-alternatives><element-citation id="ec-CR165" publication-type="journal"><person-group person-group-type="author"><name><surname>Haand</surname><given-names>R</given-names></name><name><surname>Shuwang</surname><given-names>Z</given-names></name></person-group><article-title>The relationship between social media addiction and depression: a quantitative study among university students in khost, afghanistan</article-title><source>Int. J. Adolesc. Youth</source><year>2020</year><volume>25</volume><fpage>780</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1080/02673843.2020.1741407</pub-id></element-citation><mixed-citation id="mc-CR165" publication-type="journal">Haand, R. &#x00026; Shuwang, Z. The relationship between social media addiction and depression: a quantitative study among university students in khost, afghanistan. <italic>Int. J. Adolesc. Youth</italic><bold>25</bold>, 780&#x02013;786 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR166"><label>166.</label><citation-alternatives><element-citation id="ec-CR166" publication-type="journal"><person-group person-group-type="author"><name><surname>Brailovskaia</surname><given-names>J</given-names></name><name><surname>Margraf</surname><given-names>J</given-names></name></person-group><article-title>Relationship between depression symptoms, physical activity, and addictive social media use</article-title><source>Cyberpsychol. Behav. Soc. Netw.</source><year>2020</year><volume>23</volume><fpage>818</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1089/cyber.2020.0255</pub-id><pub-id pub-id-type="pmid">32813562</pub-id>
</element-citation><mixed-citation id="mc-CR166" publication-type="journal">Brailovskaia, J. &#x00026; Margraf, J. Relationship between depression symptoms, physical activity, and addictive social media use. <italic>Cyberpsychol. Behav. Soc. Netw.</italic><bold>23</bold>, 818&#x02013;822 (2020).<pub-id pub-id-type="pmid">32813562</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR167"><label>167.</label><citation-alternatives><element-citation id="ec-CR167" publication-type="journal"><person-group person-group-type="author"><name><surname>Jeri-Yabar</surname><given-names>A</given-names></name><etal/></person-group><article-title>Association between social media use (Twitter, Instagram, Facebook) and depressive symptoms: are Twitter users at higher risk?</article-title><source>Int. J. Soc. Psychiatry</source><year>2019</year><volume>65</volume><fpage>14</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1177/0020764018814270</pub-id><pub-id pub-id-type="pmid">30497315</pub-id>
</element-citation><mixed-citation id="mc-CR167" publication-type="journal">Jeri-Yabar, A. et al. Association between social media use (Twitter, Instagram, Facebook) and depressive symptoms: are Twitter users at higher risk? <italic>Int. J. Soc. Psychiatry</italic><bold>65</bold>, 14&#x02013;19 (2019).<pub-id pub-id-type="pmid">30497315</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR168"><label>168.</label><citation-alternatives><element-citation id="ec-CR168" publication-type="journal"><person-group person-group-type="author"><name><surname>Kircaburun</surname><given-names>K</given-names></name></person-group><article-title>Self-esteem, daily internet use and social media addiction as predictors of depression among Turkish adolescents</article-title><source>J. Educ. Practice</source><year>2016</year><volume>7</volume><fpage>64</fpage><lpage>72</lpage></element-citation><mixed-citation id="mc-CR168" publication-type="journal">Kircaburun, K. Self-esteem, daily internet use and social media addiction as predictors of depression among Turkish adolescents. <italic>J. Educ. Practice</italic><bold>7</bold>, 64&#x02013;72 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR169"><label>169.</label><citation-alternatives><element-citation id="ec-CR169" publication-type="journal"><person-group person-group-type="author"><name><surname>Ricard</surname><given-names>BJ</given-names></name><name><surname>Marsch</surname><given-names>LA</given-names></name><name><surname>Crosier</surname><given-names>B</given-names></name><name><surname>Hassanpour</surname><given-names>S</given-names></name></person-group><article-title>Exploring the utility of community-generated social media content for detecting depression: an analytical study on Instagram</article-title><source>J. Med. Internet Res.</source><year>2018</year><volume>20</volume><fpage>e11817</fpage><pub-id pub-id-type="doi">10.2196/11817</pub-id><pub-id pub-id-type="pmid">30522991</pub-id>
</element-citation><mixed-citation id="mc-CR169" publication-type="journal">Ricard, B. J., Marsch, L. A., Crosier, B. &#x00026; Hassanpour, S. Exploring the utility of community-generated social media content for detecting depression: an analytical study on Instagram. <italic>J. Med. Internet Res.</italic><bold>20</bold>, e11817 (2018).<pub-id pub-id-type="pmid">30522991</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR170"><label>170.</label><citation-alternatives><element-citation id="ec-CR170" publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>Z</given-names></name><name><surname>Hu</surname><given-names>Q</given-names></name><name><surname>Dang</surname><given-names>J</given-names></name></person-group><article-title>Multi-kernel svm based depression recognition using social media data</article-title><source>Int. J. Mach. Learn. Cybernet.</source><year>2019</year><volume>10</volume><fpage>43</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1007/s13042-017-0697-1</pub-id></element-citation><mixed-citation id="mc-CR170" publication-type="journal">Peng, Z., Hu, Q. &#x00026; Dang, J. Multi-kernel svm based depression recognition using social media data. <italic>Int. J. Mach. Learn. Cybernet.</italic><bold>10</bold>, 43&#x02013;57 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR171"><label>171.</label><mixed-citation publication-type="other">Aldarwish, M. M. &#x00026; Ahmad, H. F. Predicting depression levels using social media posts. In <italic>2017 IEEE 13th international Symposium on Autonomous Decentralized System (ISADS)</italic>, 277&#x02013;280 (IEEE, 2017).</mixed-citation></ref><ref id="CR172"><label>172.</label><citation-alternatives><element-citation id="ec-CR172" publication-type="journal"><person-group person-group-type="author"><name><surname>Burdisso</surname><given-names>SG</given-names></name><name><surname>Errecalde</surname><given-names>M</given-names></name><name><surname>Montes-y G&#x000f3;mez</surname><given-names>M</given-names></name></person-group><article-title>A text classification framework for simple and effective early depression detection over social media streams</article-title><source>Expert Syst. Appl.</source><year>2019</year><volume>133</volume><fpage>182</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2019.05.023</pub-id></element-citation><mixed-citation id="mc-CR172" publication-type="journal">Burdisso, S. G., Errecalde, M. &#x00026; Montes-y G&#x000f3;mez, M. A text classification framework for simple and effective early depression detection over social media streams. <italic>Expert Syst. Appl.</italic><bold>133</bold>, 182&#x02013;197 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR173"><label>173.</label><citation-alternatives><element-citation id="ec-CR173" publication-type="journal"><person-group person-group-type="author"><name><surname>Smys</surname><given-names>S</given-names></name><name><surname>Raj</surname><given-names>JS</given-names></name></person-group><article-title>Analysis of deep learning techniques for early detection of depression on social media network-a comparative study</article-title><source>J. Trends Comput. Sci. Smart Technol. (TCSST)</source><year>2021</year><volume>3</volume><fpage>24</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.36548/jtcsst.2021.1.003</pub-id></element-citation><mixed-citation id="mc-CR173" publication-type="journal">Smys, S. &#x00026; Raj, J. S. Analysis of deep learning techniques for early detection of depression on social media network-a comparative study. <italic>J. Trends Comput. Sci. Smart Technol. (TCSST)</italic><bold>3</bold>, 24&#x02013;39 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR174"><label>174.</label><mixed-citation publication-type="other">Bucur, A.-M. &#x00026; Dinu, L. P. Detecting early onset of depression from social media text using learned confidence scores. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.01695">https://arxiv.org/abs/2011.01695</ext-link> (2020).</mixed-citation></ref><ref id="CR175"><label>175.</label><mixed-citation publication-type="other">Sampath, K. &#x00026; Durairaj, T. Data set creation and empirical analysis for detecting signs of depression from social media postings. In <italic>International Conference on Computational Intelligence in Data Science</italic>, 136&#x02013;151 (Springer, 2022).</mixed-citation></ref><ref id="CR176"><label>176.</label><mixed-citation publication-type="other">Mann, P., Paes, A. &#x00026; Matsushima, E. H. See and read: detecting depression symptoms in higher education students using multimodal social media data. In <italic>Proceedings of the International AAAI Conference on Web and Social Media</italic>, vol. 14, 440&#x02013;451 (AAAI, 2020).</mixed-citation></ref><ref id="CR177"><label>177.</label><mixed-citation publication-type="other">Sadeque, F., Xu, D. &#x00026; Bethard, S. Measuring the latency of depression detection in social media. In <italic>Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</italic>, 495&#x02013;503 (ACM, 2018).</mixed-citation></ref><ref id="CR178"><label>178.</label><citation-alternatives><element-citation id="ec-CR178" publication-type="journal"><person-group person-group-type="author"><name><surname>Fatima</surname><given-names>I</given-names></name><etal/></person-group><article-title>Prediction of postpartum depression using machine learning techniques from social media text</article-title><source>Expert Syst.</source><year>2019</year><volume>36</volume><fpage>e12409</fpage><pub-id pub-id-type="doi">10.1111/exsy.12409</pub-id></element-citation><mixed-citation id="mc-CR178" publication-type="journal">Fatima, I. et al. Prediction of postpartum depression using machine learning techniques from social media text. <italic>Expert Syst.</italic><bold>36</bold>, e12409 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR179"><label>179.</label><mixed-citation publication-type="other">Katchapakirin, K., Wongpatikaseree, K., Yomaboot, P. &#x00026; Kaewpitakkun, Y. Facebook social media for depression detection in the Thai community. In <italic>2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE)</italic>, 1&#x02013;6 (IEEE, 2018).</mixed-citation></ref><ref id="CR180"><label>180.</label><mixed-citation publication-type="other">Shen, G. et al. Depression detection via harvesting social media: a multimodal dictionary learning solution. <italic>IJCAI</italic>, 3838&#x02013;3844 (2017).</mixed-citation></ref><ref id="CR181"><label>181.</label><citation-alternatives><element-citation id="ec-CR181" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Chaudhary</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name></person-group><article-title>Modeling spatiotemporal pattern of depressive symptoms caused by covid-19 using social media data mining</article-title><source>Int. J. Environ. Res. Public Health</source><year>2020</year><volume>17</volume><fpage>4988</fpage><pub-id pub-id-type="doi">10.3390/ijerph17144988</pub-id><pub-id pub-id-type="pmid">32664388</pub-id>
</element-citation><mixed-citation id="mc-CR181" publication-type="journal">Li, D., Chaudhary, H. &#x00026; Zhang, Z. Modeling spatiotemporal pattern of depressive symptoms caused by covid-19 using social media data mining. <italic>Int. J. Environ. Res. Public Health</italic><bold>17</bold>, 4988 (2020).<pub-id pub-id-type="pmid">32664388</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR182"><label>182.</label><citation-alternatives><element-citation id="ec-CR182" publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>LY</given-names></name><etal/></person-group><article-title>Association between social media use and depression among us young adults</article-title><source>Depress. Anxiety</source><year>2016</year><volume>33</volume><fpage>323</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1002/da.22466</pub-id><pub-id pub-id-type="pmid">26783723</pub-id>
</element-citation><mixed-citation id="mc-CR182" publication-type="journal">Lin, L. Y. et al. Association between social media use and depression among us young adults. <italic>Depress. Anxiety</italic><bold>33</bold>, 323&#x02013;331 (2016).<pub-id pub-id-type="pmid">26783723</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR183"><label>183.</label><mixed-citation publication-type="other">Mohan, M., Abhinav, A., Ashok, A., Akhil, A. &#x00026; Achinth, P. Depression detection using facial expression and sentiment analysis. In <italic>2021 Asian Conference on Innovation in Technology (ASIANCON)</italic>, 1&#x02013;6 (IEEE, 2021).</mixed-citation></ref><ref id="CR184"><label>184.</label><citation-alternatives><element-citation id="ec-CR184" publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y-S</given-names></name><name><surname>Park</surname><given-names>W-H</given-names></name></person-group><article-title>Diagnosis of depressive disorder model on facial expression based on fast r-cnn</article-title><source>Diagnostics</source><year>2022</year><volume>12</volume><fpage>317</fpage><pub-id pub-id-type="doi">10.3390/diagnostics12020317</pub-id><pub-id pub-id-type="pmid">35204407</pub-id>
</element-citation><mixed-citation id="mc-CR184" publication-type="journal">Lee, Y.-S. &#x00026; Park, W.-H. Diagnosis of depressive disorder model on facial expression based on fast r-cnn. <italic>Diagnostics</italic><bold>12</bold>, 317 (2022).<pub-id pub-id-type="pmid">35204407</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR185"><label>185.</label><mixed-citation publication-type="other">Sumali, B., Mitsukura, Y., Tazawa, Y. &#x00026; Kishimoto, T. Facial landmark activity features for depression screening. In <italic>2019 58th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)</italic>, 1376&#x02013;1381 (IEEE, 2019).</mixed-citation></ref><ref id="CR186"><label>186.</label><mixed-citation publication-type="other">Dadiz, B. G. &#x00026; Ruiz, C. R. Detecting depression in videos using uniformed local binary pattern on facial features. In <italic>Computational Science and Technology: 5th ICCST 2018, Kota Kinabalu, Malaysia, 29-30 August 2018</italic>, 413&#x02013;422 (Springer, 2019).</mixed-citation></ref><ref id="CR187"><label>187.</label><mixed-citation publication-type="other">Stasak, B., Huang, Z., Joachim, D. &#x00026; Epps, J. Automatic elicitation compliance for short-duration speech based depression detection. In <italic>ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 7283&#x02013;7287 (IEEE, 2021).</mixed-citation></ref><ref id="CR188"><label>188.</label><mixed-citation publication-type="other">Huang, Z., Epps, J. &#x00026; Joachim, D. Speech landmark bigrams for depression detection from naturalistic smartphone speech. In <italic>ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 5856&#x02013;5860 (IEEE, 2019).</mixed-citation></ref><ref id="CR189"><label>189.</label><mixed-citation publication-type="other">Huang, Z., Epps, J., Joachim, D. &#x00026; Chen, M. Depression detection from short utterances via diverse smartphones in natural environmental conditions. <italic>INTERSPEECH</italic> 3393&#x02013;3397 (2018).</mixed-citation></ref><ref id="CR190"><label>190.</label><citation-alternatives><element-citation id="ec-CR190" publication-type="journal"><person-group person-group-type="author"><name><surname>Szabadi</surname><given-names>E</given-names></name><name><surname>Bradshaw</surname><given-names>C</given-names></name><name><surname>Besson</surname><given-names>J</given-names></name></person-group><article-title>Elongation of pause-time in speech: a simple, objective measure of motor retardation in depression</article-title><source>Br. J. Psychiatry</source><year>1976</year><volume>129</volume><fpage>592</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1192/bjp.129.6.592</pub-id><pub-id pub-id-type="pmid">1000144</pub-id>
</element-citation><mixed-citation id="mc-CR190" publication-type="journal">Szabadi, E., Bradshaw, C. &#x00026; Besson, J. Elongation of pause-time in speech: a simple, objective measure of motor retardation in depression. <italic>Br. J. Psychiatry</italic><bold>129</bold>, 592&#x02013;597 (1976).<pub-id pub-id-type="pmid">1000144</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR191"><label>191.</label><mixed-citation publication-type="other">He, L., Jiang, D. &#x00026; Sahli, H. Multimodal depression recognition with dynamic visual and audio cues. In <italic>2015 International Conference on Affective Computing and Intelligent Interaction (ACII)</italic>, 260&#x02013;266 (IEEE, 2015).</mixed-citation></ref><ref id="CR192"><label>192.</label><mixed-citation publication-type="other">P&#x000e9;rez Espinosa, H. et al. Fusing affective dimensions and audio-visual features from segmented video for depression recognition: Inaoe-buap&#x02019;s participation at avec&#x02019;14 challenge. In <italic>Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge</italic>, 49&#x02013;55 (Association for Computing Machinery (ACM), 2014).</mixed-citation></ref><ref id="CR193"><label>193.</label><mixed-citation publication-type="other">Malandrakis, N., Potamianos, A., Evangelopoulos, G. &#x00026; Zlatintsi, A. A supervised approach to movie emotion tracking. In <italic>2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic>, 2376&#x02013;2379 (IEEE, 2011).</mixed-citation></ref><ref id="CR194"><label>194.</label><citation-alternatives><element-citation id="ec-CR194" publication-type="journal"><person-group person-group-type="author"><name><surname>Semkovska</surname><given-names>M</given-names></name><name><surname>Noone</surname><given-names>M</given-names></name><name><surname>Carton</surname><given-names>M</given-names></name><name><surname>McLoughlin</surname><given-names>DM</given-names></name></person-group><article-title>Measuring consistency of autobiographical memory recall in depression</article-title><source>Psychiatry Res.</source><year>2012</year><volume>197</volume><fpage>41</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.psychres.2011.12.010</pub-id><pub-id pub-id-type="pmid">22397910</pub-id>
</element-citation><mixed-citation id="mc-CR194" publication-type="journal">Semkovska, M., Noone, M., Carton, M. &#x00026; McLoughlin, D. M. Measuring consistency of autobiographical memory recall in depression. <italic>Psychiatry Res.</italic><bold>197</bold>, 41&#x02013;48 (2012).<pub-id pub-id-type="pmid">22397910</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR195"><label>195.</label><citation-alternatives><element-citation id="ec-CR195" publication-type="journal"><person-group person-group-type="author"><name><surname>Saeb</surname><given-names>S</given-names></name><name><surname>Lattie</surname><given-names>EG</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Mohr</surname><given-names>DC</given-names></name><etal/></person-group><article-title>Mobile phone detection of semantic location and its relationship to depression and anxiety</article-title><source>JMIR mHealth uHealth</source><year>2017</year><volume>5</volume><fpage>e7297</fpage><pub-id pub-id-type="doi">10.2196/mhealth.7297</pub-id></element-citation><mixed-citation id="mc-CR195" publication-type="journal">Saeb, S., Lattie, E. G., Kording, K. P. &#x00026; Mohr, D. C. et al. Mobile phone detection of semantic location and its relationship to depression and anxiety. <italic>JMIR mHealth uHealth</italic><bold>5</bold>, e7297 (2017).</mixed-citation></citation-alternatives></ref></ref-list></back></article>