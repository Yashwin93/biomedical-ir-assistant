<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><?properties manuscript?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-journal-id">101604520</journal-id><journal-id journal-id-type="nlm-ta">IEEE J Biomed Health Inform</journal-id><journal-id journal-id-type="iso-abbrev">IEEE J Biomed Health Inform</journal-id><journal-title-group><journal-title>IEEE journal of biomedical and health informatics</journal-title></journal-title-group><issn pub-type="ppub">2168-2194</issn><issn pub-type="epub">2168-2208</issn></journal-meta><article-meta><article-id pub-id-type="pmid">32340967</article-id><article-id pub-id-type="pmc">PMC7618182</article-id><article-id pub-id-type="manuscript">EMS208285</article-id><article-id pub-id-type="doi">10.1109/JBHI.2020.2979608</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>PlethAugment: GAN-Based PPG Augmentation for Medical Diagnosis in Low-Resource Settings</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2898-1790</contrib-id><name><surname>Kiyasseh</surname><given-names>Dani</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2648-9102</contrib-id><name><surname>Tadesse</surname><given-names>Girmaw Abebe</given-names></name><email>girmaw.abebe@eng.ox.ac.uk</email></contrib><aff id="A1">Engineering Science Department, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>
<postal-code>OX3 7DQ</postal-code>, <country country="GB">U.K.</country></aff></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Nhan</surname><given-names>Le Nguyen Thanh</given-names></name><email>nhanlnt@oucru.org</email></contrib><contrib contrib-type="author"><name><surname>Van Tan</surname><given-names>Le</given-names></name><email>tanlv@oucru.org</email></contrib><contrib contrib-type="author"><name><surname>Thwaites</surname><given-names>Louise</given-names></name><email>lthwaites@oucru.org</email></contrib><aff id="A2"><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rehad94</institution-id><institution>Oxford University Clinical Research Unit</institution></institution-wrap>, <city>Ho Chi Minh City</city>
<postal-code>700000</postal-code>, <country country="VN">Vietnam</country></aff></contrib-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1552-5630</contrib-id><name><surname>Zhu</surname><given-names>Tingting</given-names></name><email>tingting.zhu@eng.ox.ac.uk</email></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Clifton</surname><given-names>David</given-names></name><email>davidc@robots.ox.ac.uk</email></contrib><aff id="A3">Engineering Science Department, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>
<postal-code>OX3 7DQ</postal-code>, <country country="GB">U.K.</country></aff></contrib-group><author-notes><corresp id="CR1">Corresponding author: Dani Kiyasseh. <email>dani.kiyasseh@eng.ox.ac.uk</email></corresp><fn id="FN1" fn-type="supported-by"><p id="P1">This work was supported by the EPRSC under Grants EP/P009824/1 and EP/N020774/1. The work of Tingting Zhu was supported by the Engineering for Development Research Fellowship provided by the Royal Academy of Engineering.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>28</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="ppub"><day>01</day><month>11</month><year>2020</year></pub-date><pub-date pub-type="epub"><day>04</day><month>11</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>29</day><month>9</month><year>2025</year></pub-date><volume>24</volume><issue>11</issue><fpage>3226</fpage><lpage>3235</lpage><permissions><ali:free_to_read xmlns:ali="http://www.niso.org/schemas/ali/1.0/"/><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions><abstract><p id="P2">The paucity of physiological time-series data collected from low-resource clinical settings limits the capabilities of modern machine learning algorithms in achieving high performance. Such performance is further hindered by class imbalance; datasets where a diagnosis is much more common than others. To overcome these two issues at low-cost while preserving privacy, data augmentation methods can be employed. In the time domain, the traditional method of time-warping could alter the underlying data distribution with detrimental consequences. This is prominent when dealing with physiological conditions that influence the frequency components of data. In this paper, we propose PlethAugment; three different conditional generative adversarial networks (CGANs) with an adapted diversity term for the generation of pathological photoplethysmogram (PPG) signals in order to boost medical classification performance. To evaluate and compare the GANs, we introduce a novel <italic toggle="yes">metric-agnostic</italic> method; the <italic toggle="yes">synthetic generalization curve</italic>. We validate this approach on two proprietary and two public datasets representing a diverse set of medical conditions. Compared to training on non-augmented class-balanced datasets, training on augmented datasets leads to an improvement of the AUROC by up to 29% when using cross validation. This illustrates the potential of the proposed CGANs to significantly improve classification performance.</p></abstract><kwd-group><title>Index Terms</title><kwd>Conditional generative adversarial networks</kwd><kwd>data-augmentation</kwd><kwd>time-series</kwd><kwd>photople-thysmogram</kwd><kwd>low-resource</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="S1"><label>I</label><title>Introduction</title><p id="P3">AUCITY of data and class imbalance drastically hinder the performance of modern machine learning algorithms [<xref rid="R1" ref-type="bibr">1</xref>], [<xref rid="R2" ref-type="bibr">2</xref>]. In the medical domain, the relatively low number of patients enrolled in experimental trials, among other reasons, limits the amount of data collected. This is even more pronounced in low-resource clinical settings where high financial and infrastructural constraints exist. To overcome this obstacle, the use of wearable sensors capable of continuous monitoring of physiological signals such as the photoplethysmogram (PPG) has experienced a rise [<xref rid="R3" ref-type="bibr">3</xref>]. The amount of data limits researchers from capitalizing on deep learning approaches which are known to be data-hungry [<xref rid="R4" ref-type="bibr">4</xref>] and which have produced promising results in cognate disciplines. Therefore, generating class-specific medical time-series data may help in alleviating some of the aforementioned obstacles.</p><p id="P4">Data augmentation, the process of generating new data from the existing data is common in computer vision [<xref rid="R5" ref-type="bibr">5</xref>] where images are flipped and rotated at various angles in order to augment the dataset and act as a form of regularization. Given its positive impact on classification performance [<xref rid="R6" ref-type="bibr">6</xref>], it has been used for various tasks involving deep learning for medical images such as segmentation [<xref rid="R7" ref-type="bibr">7</xref>] and liver lesion classification [<xref rid="R8" ref-type="bibr">8</xref>]. In the time-domain, on the other hand, the addition of noise and time-warping is performed [<xref rid="R9" ref-type="bibr">9</xref>]. Such approaches can lead to unwanted changes in the physiological signals, changing the underlying data distribution in a manner that might affect subsequent classification. This is especially problematic when dealing with medical conditions such as hand-foot-mouth (HFM) disease and tetanus (both of which are especially prevalent in low-resource settings) that impact the nervous system, and consequently, the frequency components of the physiological signal. Consequently, a generative process that accurately and <italic toggle="yes">realistically</italic> represents the data is needed.</p><p id="P5">Recently, generative adversarial networks (GANs) have been used for data augmentation purposes [<xref rid="R10" ref-type="bibr">10</xref>] given their ability to capture the underlying data distribution. Conditional GANs (CGANs) for data augmentation, however, have not been fully explored, let alone in the medical domain.</p><sec id="S2"><title>Contribution</title><p id="P6">In this paper, we follow the pipeline in <xref rid="F1" ref-type="fig">Fig. 1</xref> by proposing several CGANs inspired by work in other fields [<xref rid="R11" ref-type="bibr">11</xref>]&#x02013;[<xref rid="R13" ref-type="bibr">13</xref>] and adapt them to generate disease-severity-specific photoplethysmogram signals. We use the resulting synthetic signals to augment a dataset and improve upon the baseline performance. Finally, we introduce the <italic toggle="yes">synthetic generalization curve</italic>, a novel and generalizable method for evaluating and comparing the performance of GANs.</p></sec></sec><sec id="S3"><label>II</label><title>Related Work</title><p id="P7">GANs [<xref rid="R14" ref-type="bibr">14</xref>] were first introduced as a generative model based on a minimax formulation where two networks, the generator and discriminator, engage adversarially to outsmart one another. Shortly after, CGANs [<xref rid="R11" ref-type="bibr">11</xref>] were introduced as simple extensions to GANs where the generated data is conditioned on a certain variable such as a class, time-stamp, etc.</p><sec id="S4"><label>A</label><title>Conditional Generative Adversarial Networks (CGANs) for Time-Series</title><p id="P8">GANs have been successful in generating medical <italic toggle="yes">images</italic> for the purpose of augmenting datasets [<xref rid="R15" ref-type="bibr">15</xref>]. A recent review by Yi <italic toggle="yes">et al</italic>. [<xref rid="R16" ref-type="bibr">16</xref>] summarizes the state-of-the-art in that domain. Given that medical image synthesis is beyond the scope of this paper, we solely focus on applications to time-series data. Although in its infancy, the application of CGANs for time-series data has seen a recent rise in activity. CGANs have been used to generate weather data conditioned on specific scenarios [<xref rid="R17" ref-type="bibr">17</xref>] and to generate wind and solar energy production over time conditioned on environmental variables [<xref rid="R18" ref-type="bibr">18</xref>]. Others introduce MuseGAN [<xref rid="R19" ref-type="bibr">19</xref>] to generate track-specific polyphonic music. Although their task is temporal, their data representations lack high sampling rates usually experienced in physiological signals. Others [<xref rid="R20" ref-type="bibr">20</xref>] attempt to model the potential trajectories of humans over time using an LSTM-based generator and discriminator. In the medical domain, the work in [<xref rid="R21" ref-type="bibr">21</xref>] uses a 1D convolution-based GAN to generate electroencephalogram (EEG) brain signals. Inspired by this work, others generate synthetic epileptic brain activity signals [<xref rid="R22" ref-type="bibr">22</xref>] and EEG signals [<xref rid="R23" ref-type="bibr">23</xref>] specifically to improve classification models. Others [<xref rid="R24" ref-type="bibr">24</xref>] use a GAN to generate an open-source AND privacy-protected vital sign dataset. In [<xref rid="R25" ref-type="bibr">25</xref>], PPG and electrocardiogram (ECG) data are generated using the 2D convolution-based DCGAN. Here, time-series data are converted to images before being input into the GAN. Both of these works, however, do not aim to generate class-specific signals. Although the authors in [<xref rid="R26" ref-type="bibr">26</xref>] use a conditional DCGAN to generate EEG data, they perform their operations in the imaging domain and do not evaluate the representativeness of the synthetic EEG data. Closest to our work is that of [<xref rid="R27" ref-type="bibr">27</xref>] which uses an LSTM-based CGAN to produce various time-series data, including sine waves, some medical data, and sequential MNIST benchmark data. The medical time-series generated, however, is of summary numerics such as heart rate and oxygen saturation as opposed to high frequency medical data. Notably, they introduce an evaluation metric known as &#x0201c;Train on Synthetic, Test on Real&#x0201d; (TSTR) which we build upon in our work. Lastly, although not used for time-series, DSGAN [<xref rid="R28" ref-type="bibr">28</xref>] involves a diversity sensitivity term that rewards conditional GANs for diverse data generation.</p></sec><sec id="S5"><label>B</label><title>Data Augmentation for Time-Series</title><p id="P9">Given the improved results associated with data augmentation in computer vision [<xref rid="R29" ref-type="bibr">29</xref>], recent work converts time-series into image-representations [<xref rid="R30" ref-type="bibr">30</xref>], [<xref rid="R31" ref-type="bibr">31</xref>]. The work in [<xref rid="R9" ref-type="bibr">9</xref>] provides a good overview of data augmentation methods to employ on time-series data from wearable sensors. This includes random jitter, window-slicing, changing permutations, and time-warping. The latter is used before implementing a convolutional neural network [<xref rid="R32" ref-type="bibr">32</xref>] and to boost the performance of a deep ResNet classification network [<xref rid="R33" ref-type="bibr">33</xref>]. Unfortunately, the aforementioned approaches could be detrimental in our application especially when dealing with physiological conditions that impact a signal&#x02019;s frequency component. The addition of noise from a Gaussian distribution with varying standard deviations has been used to improve the classification performance of three different models (SVM, LeNet, ResNet) on various datasets [<xref rid="R18" ref-type="bibr">18</xref>]. While promising, the results are inconsistent and the methdology does not seem to generalize well. In the music domain, authors in [<xref rid="R34" ref-type="bibr">34</xref>]&#x02013;[<xref rid="R36" ref-type="bibr">36</xref>] leverage an audio degradation toolbox that introduces perturbations to the original data. To avoid domain-specific augmentation problems, additive noise is proposed [<xref rid="R37" ref-type="bibr">37</xref>], in addition to interpolation and extrapolation in the feature space as a form of data augmentation before data are fed into a classifier. In contrast to traditional augmentation approaches, an end-to-end model that learns invariant transformations to apply to the original data is proposed [<xref rid="R38" ref-type="bibr">38</xref>]. Although this resulted in minor classification improvement, their approach was limited to low-frequency data (1 sample per hour).</p></sec></sec><sec sec-type="methods" id="S6"><label>III</label><title>Experimental Methods</title><p id="P10">We are focused on a conditional GAN-based time-series data augmentation methodology in an effort to improve the performance of classification models. To achieve this purpose, we have chosen and adapted three various conditional GAN models that have had success in generating diverse images. When training such models, we leveraged advice pertaining to improving training stability and performance [<xref rid="R39" ref-type="bibr">39</xref>].</p><sec id="S7"><label>A</label><title>Encouraging Intraclass Diversity</title><p id="P11">The importance of generating sufficiently diverse class-specific data motivated us to adapt a reward term introduced in [<xref rid="R28" ref-type="bibr">28</xref>] by making it class-specific. Classes can be defined arbitrarily and may depend on the dataset used. In the context of this paper, for instance, the classes represent various disease severity levels. <disp-formula id="FD1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>]</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula> where the outer expectation is with respect to all classes, <italic toggle="yes">G</italic> represents the generator network, and <italic toggle="yes">z</italic><sub>1</sub> and <italic toggle="yes">z</italic><sub>2</sub> represent any two input noise vectors belonging to the same class <italic toggle="yes">c</italic>. Intuitively, this term rewards the generator according to how sensitive it is to a change in input. Extreme mode-collapse, for instance, results in a sensitivity of zero because the same output would be generated for two different noise inputs (<italic toggle="yes">G</italic>(<italic toggle="yes">z</italic><sub>2</sub>|c<italic toggle="yes">c</italic>) = <italic toggle="yes">G</italic>(<italic toggle="yes">z</italic><sub>1</sub>|<italic toggle="yes">c</italic>)). Thus a null reward value is returned. We incorporate this term into our proposed CGAN models in the hope of encouraging intraclass diversity.</p></sec><sec id="S8"><label>B</label><title>Encouraging Interclass Diversity</title><sec id="S9"><label>1)</label><title>Vanilla CGAN With Diversity Sensitivity</title><p id="P12">The &#x0201c;vanilla CGAN&#x0201d; incorporates the conditional variable at any point within the generator <italic toggle="yes">G</italic> and/or discriminator <italic toggle="yes">D</italic> network. We opted to concatenate a one-hot encoding of the class of the PPG to the input of the generator. Our generator was trained using a loss function that consists of three terms; i) a Jensen-Shannon loss &#x02112;<sub><italic toggle="yes">JS</italic></sub> that penalizes the network for generating unrealistic synthetic data <inline-formula><mml:math id="M2" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>, ii) an auxiliary cross-entropy loss that penalizes the network for generating data that cannot be correctly classified as the ground truth <italic toggle="yes">k</italic>, and iii) our proposed class-specific diversity sensitivity loss (1) that penalizes the network for not generating synthetic data that is diverse. <disp-formula id="FD2"><label>(2)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>~</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3"><label>(3)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>~</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula> where <italic toggle="yes">P</italic><sub><italic toggle="yes">g</italic></sub> represents the distribution of synthetic data and <italic toggle="yes">&#x003bb;</italic><sub><italic toggle="yes">div</italic></sub> is a hyperparameter that determines the degree of diversity sensitivity. Independently of the generator, the discriminator was trained using a loss function that also consists of three terms; i) a Wasserstein loss [<xref rid="R40" ref-type="bibr">40</xref>] that penalizes the network for classifying the synthetic data as realistic and the real data as synthetic, ii) a gradient penalty of zero [<xref rid="R41" ref-type="bibr">41</xref>] that was found to improve training stability, and iii) an auxiliary cross-entropy loss that penalizes the network for incorrectly classifying the real data. <disp-formula id="FD4"><label>(4)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtable columnalign="right"><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>~</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>~</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mo>&#x02207;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover></mml:msub><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>~</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where <italic toggle="yes">P</italic><sub><italic toggle="yes">r</italic></sub> represents the distribution of the real data, &#x02207; represents the gradient operator and <inline-formula><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is a linear combination of the real and synthetic data with <italic toggle="yes">&#x003b1;</italic> ~ <italic toggle="yes">U</italic> (0, 1), as suggested by the original authors.</p></sec><sec id="S10"><label>2)</label><title>DeLiGAN With Diversity Sensitivity</title><p id="P13">DeLiGAN [<xref rid="R12" ref-type="bibr">12</xref>] is proposed to deal with diverse and limited data regimes. As part of the generative model, the parameters of a Gaussian Mixture Model are learned through training. We remove the variance regularization term originally introduced and replace it with the diversity sensitivity term mentioned earlier. Furthermore, we revert to the traditional Jensen-Shannon loss term. Our generator and discriminator loss are represented by (2) and (4), respectively.</p></sec><sec id="S11"><label>3)</label><title>MADGAN</title><p id="P14">MADGAN [<xref rid="R13" ref-type="bibr">13</xref>] is proposed as a way to explicitly generate data from different classes. In order to do this, as many generators as there are classes are introduced. Our generator loss consists of a Jensen-Shannon loss term of the form in (3) for each generator and is as follows: <disp-formula id="FD5"><label>(5)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x02112;</mml:mi><mml:mrow><mml:mi>J</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p id="P15">The discriminator is tasked with identifying whether the data is real or synthetic, and if it is the latter, to further identify the generator from which it came. Our discriminator loss is the same as that suggested in the original paper.</p></sec></sec></sec><sec id="S12"><label>VI</label><title>Evaluation Methods</title><p id="P16">There are many ways to evaluate GANs as summarized in [<xref rid="R42" ref-type="bibr">42</xref>]. Although we take inspiration from some of these techniques, our focus does not lie here. Given our desire to quantify the potential improvement in medical diagnosis offered by data augmentation, we build upon the work introduced in [<xref rid="R27" ref-type="bibr">27</xref>] and further propose a novel evaluation method.</p><sec id="S13"><label>A</label><title>GAN-Specific Evaluation</title><p id="P17">With time-series, in contrast to computer vision, assessing the quality and representativeness of synthetic data is not straight-forward. Moreover, a common pitfall of such networks is mode collapse where the generator fails to produce diverse samples; i.e., there exists a many-to-one or many-to-few mapping of random variable <italic toggle="yes">z</italic> to synthetic image <inline-formula><mml:math id="M8" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>. This is especially problematic in the conditional GAN case where some diversity is expected in the generated data. Thus, we evaluate our GANs by measuring the following:</p><sec id="S14"><label>1)</label><title>Representativeness of Synthetic Data</title><p id="P18">We use the kernel maximum mean discrepancy (MMD) [<xref rid="R43" ref-type="bibr">43</xref>], a common evaluation method for GANs that compares the similarity of synthetic data and real data. This similarity is quantified using a kernel function <italic toggle="yes">K</italic>, and in our case, we use the exponentiated quadratic. <disp-formula id="FD6"><label>(6)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where <italic toggle="yes">x</italic> and <italic toggle="yes">x</italic>&#x02032; are two vectors to be compared. If they are exactly the same, then the kernel function evaluates to one. The more dissimilar they are from one another, the smaller the value is, which is lower-bounded by zero. Since the original MMD metric fails to illustrate the more granular class-specific similarities, we introduce <italic toggle="yes">M M D</italic><sub><italic toggle="yes">c</italic></sub>; a conditional MMD metric that allows us to compare class-specific performance across different GANs as shown below <disp-formula id="FD7"><label>(7)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mspace width="0.1em"/><mml:mi>M</mml:mi><mml:mspace width="0.1em"/><mml:msub><mml:mi>D</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>&#x02211;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>&#x02211;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>&#x02211;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>j</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> where <italic toggle="yes">K</italic> is a kernel function that measures the similarity between its inputs, <inline-formula><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">c</italic> is a particular class, and <inline-formula><mml:math id="M12" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> and <italic toggle="yes">x</italic> represent the synthetic and original data, respectively.</p></sec><sec id="S15"><label>2)</label><title>Class Diversity</title><p id="P19">Variation in the generated data within and across the classes is important to detect, where the latter helps evaluate the <italic toggle="yes">conditional</italic> component of the CGAN. Since the MMD obscures this calculation, we explicitly calculate it through exponentiated quadratic kernels.</p></sec></sec><sec id="S16"><label>B)</label><title>Train on Synthetic and Real, Test on Real</title><p id="P20">We call the process of training on a dataset augmented with synthetic data and testing on the real dataset &#x0201c;Train on Synthetic and Real, Test on Real&#x0201d; (TSRTR). The outcome of this, when compared to a baseline, &#x0201c;Train on Real and Test on Real&#x0201d; (TRTR), allows us to see the effect of the data augmentation policy, which could be negative as observed in [<xref rid="R44" ref-type="bibr">44</xref>]. We define a data augmentation policy as a set of three parameters that dictate how to augment the original data: i) choice of class to imbalance, ii) degree of synthetic imbalance, and iii) ratio of synthetic to real data. The complete list of policies can be found in Section-VII of the Supplementary Material. Such an evaluation is performed using leave N-patients-out cross validation on 10 diverse classification models; Naive Bayes, Linear and Quadratic Discriminant Analysis, k-Nearest Neighbours, Logistic Regression, Support Vector Machines, Decision Tree, Random Forest, Adaboost, and Multilayer Perceptron. Mathematically, for a certain augmentation policy and for all classification models <italic toggle="yes">M</italic>, we calculate the percent change in a metric of interest. <disp-formula id="FD8"><label>(8)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mtext>%&#x00394;</mml:mtext></mml:mrow><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x022c5;</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></disp-formula> where <italic toggle="yes">X</italic> can be any desired metric such as AUROC, and <italic toggle="yes">X</italic><sub><italic toggle="yes">TSRTR</italic></sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">T</italic>
<italic toggle="yes">RTR</italic></sub> represent the metric value on a validation set when <italic toggle="yes">training</italic> on an augmented dataset and a non-augmented dataset, respectively.</p></sec><sec id="S17"><label>C</label><title>Synthetic Generalization Curve</title><p id="P21">The above evaluation method is limited and simply provides us with the performance of an individual classification model for a particular augmentation policy. To obtain a holistic evaluation of all classification models for all augmentation policies, and thus provide a more realistic evaluation of any GAN, we propose the <italic toggle="yes">Synthetic Generalization Curve</italic>. Such a metric quantifies the extent to which all classification models <italic toggle="yes">M</italic> are over-or underperforming relative to a baseline. Mathematically, a point on the curve, which we call the synthetic generalization (<italic toggle="yes">SG</italic>), can be calculated as follows: <disp-formula id="FD9"><label>(9)</label><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:mo>&#x02200;</mml:mo><mml:mi>&#x003b5;</mml:mi><mml:mspace width="0.2em"/><mml:mi>S</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b5;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>&#x02211;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b5;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic toggle="yes">&#x003b4;</italic> is the Kronecker delta function which evaluates to one if its argument is true and zero otherwise. The <italic toggle="yes">SG</italic> is performed for a particular augmentation policy <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> from the pool of policies <italic toggle="yes">P</italic>, and <italic toggle="yes">&#x003b5;</italic> &#x02260; 1 dictates the comparison of the classification model in the augmented scheme <italic toggle="yes">X</italic><sub><italic toggle="yes">TSRTR</italic></sub> to the baseline (1 &#x02212; <italic toggle="yes">&#x003b5;</italic>)<italic toggle="yes">X</italic><sub><italic toggle="yes">T</italic>
<italic toggle="yes">RTR</italic></sub> and varies according to user needs. For instance, when <italic toggle="yes">&#x003b5; &#x0003c;</italic> 0, the SG represents the percentage of classification models in the augmented scheme that outperform those in the baseline by at least &#x02212;<italic toggle="yes">&#x003b5;</italic> &#x000b7; 100 percentage points. From this curve, a novel metric naturally follows: the <italic toggle="yes">Area Under the Synthetic Generalization Curve</italic> or <italic toggle="yes">AUSGC</italic>. This curve can be averaged over many augmentation polices to allow for a more realistic comparison of the performance of different types of GANs.</p></sec></sec><sec id="S18"><label>V</label><title>Experimental Setup</title><sec id="S19"><label>A</label><title>Dataset Description</title><sec id="S20"><label>1)</label><title>PPG From Patients in Vietnam With Hand-Foot-Mouth Disease</title><p id="P22">The PPG data were collected using a pulse oximeter (SmartCare Analytics Ltd., Oxford, UK) placed on the major toe of HFM-afflicted children between the ages of 3 and 6. Such data, sampled at a rate of 100 Hz, were collected from 74 patients upon admission to the pediatric intesive care unit, 6 hours after admission, and one day before discharge. Each data collection period was approximately 10 minutes in duration. Typically, HFMD severity is diagnosed based on medical criteria [<xref rid="R45" ref-type="bibr">45</xref>], [<xref rid="R46" ref-type="bibr">46</xref>]. For this dataset, diagnoses are performed by ICU physicians independently of the PPG waveform and consist of 3 classes in total.</p></sec><sec id="S21"><label>2)</label><title>PPG From Patients in Vietnam With Tetanus</title><p id="P23">The PPG data were collected using a pulse oximeter (SmartCare Analytics Ltd., Oxford, UK) placed on the index finger of tetanus-afflicted adults. Such data were collected from 19 patients upon admission to the intesive care unit and one day before discharge. We only use the data from the first day of ICU admission. Each data collection period was approximately 24 hours in duration. Typically, tetanus severity is diagnosed based on clinical features outlined in the Ablett score [<xref rid="R47" ref-type="bibr">47</xref>]. For this dataset, diagnoses are performed by ICU physicians independently of the PPG waveform and consist of 3 classes in total.</p></sec><sec id="S22"><label>3)</label><title>PPG From Patients in China With Cardiovascular Disease</title><p id="P24">The PPG data introduced in [<xref rid="R48" ref-type="bibr">48</xref>] were collected via a sensor used on CVD-afflicted patients between the ages of 21 and 86 and may be accessed in [<xref rid="R49" ref-type="bibr">49</xref>]. Such data, sampled at a rate of 1KHz, are collected from 219 patients in a clinical environment. Each patient has three data collection periods each of which is 2.1 s in duration. The 4-class diagnosis of hypertension includes; normotension, prehypertension, stage I, and stage II hypertension. In order to better compare results across datasets and architectures, we keep the number of classes consistent at 3 by combining the more similar data labelled normotension and prehypertension together. We did this to keep the network architecture consistent across datasets and because the aforementioned two classes are on the lower end of the severity of the medical condition.</p></sec><sec id="S23"><label>4)</label><title>PPG From Physionet 2015 Challenge</title><p id="P25">The PPG data were the training data offered by the Physionet Challenge 2015 [<xref rid="R50" ref-type="bibr">50</xref>]. It consisted of recordings from 750 patients that suffered either of the following cardiac conditions; asystole, extreme bradycardia, extreme tachycardia, ventricular tachycardia, and ventricular flutter. The data, originally resampled to 250 Hz by the organizers of the challenge, were downsampled to 100 Hz for consistency with our other datasets. In effort to enable a fair comparison across datasets, we ensure that all tasks are a 3-way classification. Therefore, even though 5 cardiac classes exist, we choose to only distinguish between asystole, extreme bradycardia, and ventricular flutter.</p></sec></sec><sec id="S24"><label>B</label><title>PPG Data Representation</title><p id="P26">Medical conditions that are associated with autonomic nervous system dysfunction and heart rate variability can manifest themselves in the photoplethysmographic wave. A task force set up in 1996 [<xref rid="R51" ref-type="bibr">51</xref>] decided that five minutes of ECG data would be sufficient for a physician to discern such medical conditions. However, due to the lack of sufficient data and to avoid the curse of dimensionality [<xref rid="R52" ref-type="bibr">52</xref>], a shorter window was chosen to allow for an increased number of frames. Consequently, the PPG time-series data in this work is split into frames of <italic toggle="yes">t</italic> = 10 second duration. Given a sampling rate of <italic toggle="yes">F</italic><sub><italic toggle="yes">s</italic></sub>, the length of each frame in samples becomes <italic toggle="yes">F</italic><sub><italic toggle="yes">s</italic></sub>
<italic toggle="yes">&#x000b7; t</italic>.</p></sec><sec id="S25"><label>C</label><title>CGAN Model Data</title><p id="P27">The discriminator of each CGAN model was fed a PPG frame of length <italic toggle="yes">F</italic><sub><italic toggle="yes">s</italic></sub>
<italic toggle="yes">&#x000b7; t</italic> where <italic toggle="yes">F</italic><sub><italic toggle="yes">s</italic></sub> is 100 Hz and <italic toggle="yes">t</italic> is 5 seconds. These 500-dimensional frames were then reshaped according to the packing degree <italic toggle="yes">p</italic> used. Packing the frames consists of simply concatenating several frames along the time dimension and has been previously shown to improve the discriminator&#x02019;s performance [<xref rid="R53" ref-type="bibr">53</xref>]. We found that a packing degree of 3 helped produce visually-realistic PPG data. A subset (20%) of the PPG frames from each dataset was used for training the CGANs.</p></sec><sec id="S26"><label>D</label><title>Classification Models Data</title><p id="P28">PPG data were split into 5-second frames with 50% overlap. In other words, each frame was of length 500 and overlapped with the latter 250 points from the previous frame. Guided by the importance of the frequency components of the PPG signal, we used as input to the classification models the log of the one-sided power spectrum of the PPG frames. Therefore, the length of each input becomes <italic toggle="yes">F</italic><sub><italic toggle="yes">s</italic></sub>/2, which in our case was 50.</p></sec><sec id="S27"><label>E</label><title>Cross-Validation</title><p id="P29">For the evaluation of the classification models, we perform leave-3-patients-out cross-validation. Even though frames were split into 5-second segments and treated as independent from one another, the training and test folds were always split according to patients. This avoids patient-related data leakage. Moreover, each test fold consisted of PPG frames from exactly one patient from each of the three classes. Consequently, the total number of folds was equivalent to the lowest number of patients belonging to each class. In <xref rid="T1" ref-type="table">Table I</xref>, we outline the specific input data sizes <italic toggle="yes">N</italic> x<italic toggle="yes">D</italic> for each of the medical diagnosis tasks, where <italic toggle="yes">N</italic> and <italic toggle="yes">D</italic> represent the number of frames and dimensionality of the data, respectively. We also illustrate the degree of imbalance in the class labels.</p></sec><sec id="S28"><label>F</label><title>Proposed CGAN Models Specification</title><p id="P30">We primarily use fully-connected layers for the generator and discriminator for all of the CGAN models implemented. The input to the generator is a 50-dimensional noise vector sampled from a standard Gaussian distribution. To reduce the noise present in the synthetic data, we add a 1D convolutional layer which acts as a low-pass filter before the final output of the generator, as described in [<xref rid="R54" ref-type="bibr">54</xref>]. We ultimately generate PPG signals with 500 time-steps. The specific network architecture can be found in <xref rid="T2" ref-type="table">Table II</xref>.</p><p id="P31">To make the discriminator more robust, we reshape batch outputs of the generator according to the packing degree <italic toggle="yes">p</italic> described in [<xref rid="R53" ref-type="bibr">53</xref>]. The DeLiGAN network consists of a random variable with three Gaussian mixture model components, one for each class. After experimentation, we found that when the mean vectors and covariance matrices were initialized randomly and isotropically (<italic toggle="yes">&#x003c3;</italic> = 0.3), respectively, training was stabilized. Lastly, the MADGAN architecture has three generators with the same structure as that in <xref rid="T2" ref-type="table">Table II</xref>. We note the presence of two heads at the end of the discriminator; one for determining whether the data sample is fake or real, and another for predicting the appropriate class. Lastly, we choose <italic toggle="yes">&#x003bb;</italic><sub><italic toggle="yes">div</italic></sub> = 1<italic toggle="yes">e</italic> &#x02212; 6 as that appeared to stabilize training. By varying <italic toggle="yes">&#x003bb;</italic><sub><italic toggle="yes">div</italic></sub>, we briefly illustrate its effect on the GAN evaluation metrics in <xref rid="SD1" ref-type="supplementary-material">Section-I of the Supplemetary Material</xref>.</p><p id="P32">Sample outputs from each CGAN model are shown in <xref rid="F2" ref-type="fig">Fig. 2</xref> for the HFM dataset. Synthetic data for the remaining datasets can be found in <xref rid="SD2" ref-type="supplementary-material">Section-II of the Supplementary Material</xref>. In addition to the similarity in shape between the real and synthetic data, we draw the reader&#x02019;s attention to a more subtle characteristic: amplitude modulation. Such low frequency changes in the PPG amplitude are hypothesized to represent respiratory sinus arrythmia [<xref rid="R55" ref-type="bibr">55</xref>], a naturally-experienced physiologial phenomenon. In some cases, our CGANs are able to capture this behaviour.</p></sec><sec id="S29"><label>G</label><title>Baselines</title><p id="P33">The evaluation methods discussed earlier require a comparison to a baseline. Below is a description of the various baselines used. In all cases, the same training used for TSRTR is used for TRTR.</p><sec id="S30"><label>1)</label><title>Class Imbalanced Original Data</title><p id="P34">We employ TRTR while maintaining the original imbalance present in the dataset.</p></sec><sec id="S31"><label>2)</label><title>Class Balanced Original Data</title><p id="P35">We employ TRTR while <italic toggle="yes">balancing</italic> the original imbalanced dataset. The balancing procedure is done by removing extra frames from the overpopulated classes. Motivation for this arises from improved performance due to a balanced dataset. Therefore, we report our augmentation results relative to this stronger baseline.</p></sec><sec id="S32"><label>3)</label><title>SpecAugment</title><p id="P36">We implement the technique in [<xref rid="R31" ref-type="bibr">31</xref>] which focuses on the augmentation of time-series by masking randomly-chosen time and/or frequency bands in a spectrogram representation. Whereas the original work stops here, we then convert the spectrograms back into the time-domain using an inverse short time Fourier transform.</p></sec></sec><sec id="S33"><label>H</label><title>Effect of Data Augmentation - Hypotheses</title><p id="P37">In effort to find the ideal augmentation policy and whether that generalized across CGAN models and/or datasets, we formulated four different hypotheses. The sample size and normality of the data (supported via a Shapiro test) associated with such hypotheses motivated our use of the statistical t-test and ANOVA. Nevertheless, for extra precaution, their corresponding <italic toggle="yes">non-parametric</italic> statistical tests (Wilcoxon and Kruskal-Wallis) were also performed.</p><sec id="S34"><label>1)</label><title>CGAN Models</title><p id="P38">Without any prior knowledge, and given that the current CGANs have not been implemented on time-series data, there is no reason to believe one model should outperform the other. Therefore, we hypothesize that the results of the CGAN models will be similar.</p></sec><sec id="S35"><label>2)</label><title>Training Set Imbalance</title><p id="P39">Given the work in [<xref rid="R1" ref-type="bibr">1</xref>], class imbalance is shown to degrade classifier performance. Therefore, we hypothesize that balanced training sets will outperform their unbalanced counterparts.</p></sec><sec id="S36"><label>3)</label><title>Ratio of Synthetic to Real Data</title><p id="P40">Deep learning models are notorious for being data-hungry. Authors in [<xref rid="R56" ref-type="bibr">56</xref>], [<xref rid="R57" ref-type="bibr">57</xref>] illustrate the importance of training set size on non-parametric and deep learning models, respectively. However, a significant addition of synthetic data may result in a plateau [<xref rid="R8" ref-type="bibr">8</xref>] or even a worsening in performance [<xref rid="R44" ref-type="bibr">44</xref>]. While the first effect could be due to a lack of sufficient diversity in the synthetic data, the latter is a consequence of unrepresentative synthetic data. Therefore, we hypothesize that performance will increase up to a certain ratio of synthetic to real data.</p></sec><sec id="S37"><label>4)</label><title>Class-Specific Imbalance</title><p id="P41">There is no reason to believe that introducing an imbalance to one of the classes should outperform that introduced to any of the other classes. Therefore, we hypothesize that the results will be similar regardless of class-specific imbalance.</p></sec></sec></sec><sec sec-type="results" id="S38"><label>VI</label><title>Results</title><sec id="S39"><label>A</label><title>Performance of Proposed CGANs</title><p id="P42">We quantify the representativeness of the synthetic data via the MMD values in <xref rid="T3" ref-type="table">Table III</xref>, where a lower value implies that the synthetic data is more realistic. An average is taken over 10 seeds with each seed containing 30 (15) randomly sampled datapoints from the appropriate distributions of the HFM (CVD) dataset. Fewer samples are chosen for the CVD dataset due to the small sample size in the original dataset. We also propose the use of cMMD values in order to discern interclass differences. Such a granular approach facilitates the identification of potential causal relationships between network/hyperparameter changes and representativeness of synthetic data. This can ultimately guide researchers working with <italic toggle="yes">conditional</italic> GANs. When considering all classes, we can observe that MADGAN generates data that most resembles the true underlying distribution for both datasets. A closer look at the HFM cMMD values, however, indicates that CGAN+DS is able to produce the most realistic class 1 data. Conversely, DeLiGAN+DS appears to generate the least realistic synthetic data as observed by its relatively high cMMD and MMD values. We believe such a situation may arise due to the over-powering effect of the constraints placed on the DeLiGAN+DS network such as the diversity-sensitivity loss. In other words, the network could have placed greater emphasis on generating diverse classes compared to generating realistic classes. We also compare the real and synthetic data by visualizing them in a 2-dimensional t-SNE [<xref rid="R58" ref-type="bibr">58</xref>] subspace and calculating the pairwise L2 distance between them. More details can be found in <xref rid="SD3" ref-type="supplementary-material">Section-III of the Supplementary Material</xref>.</p><p id="P43">In addition to representativeness of the synthetic data, we must ensure that the CGANs are not suffering from mode-collapse i.e. data generated from each class must be sufficiently diverse. This diversity is illustrated in <xref rid="F3" ref-type="fig">Fig. 3</xref> where the exponentiated quadratic kernel is applied to 30 randomly sampled synthetic datapoints from each class and model combination. For each such combination, the resulting symmetric matrix is truncated to only show its lower triangular region. Darker elements indicate synthetic datapoints that are quite similar to one another; a potential sign of class-specific mode-collapse. Conversely, lighter values indicate datapoints that are dissimilar from one another. Although this hints at the existence of intra-class diversity, it could also be a sign that the synthetic datapoint should not even belong to that class. This latter case would confuse classification models and negatively impact their performance. The intraclass similarity matrices belonging to the remaining datasets can be found in <xref rid="SD4" ref-type="supplementary-material">Section-IV of the Supplementary Material</xref>.</p><p id="P44">We calculate an intra-class similarity score in <xref rid="T4" ref-type="table">Table IV</xref> by taking the average of the off-diagonal elements of the 30 &#x000d7; 30 kernel matrices. Moreover, we mitigate the impact of a small sample size by averaging this across 10 different sets of 30 randomly sampled synthetic datapoints. Since we are aiming for high diversity, or equivalently low similarity, the lower the value the better. Based on this intuition, we can observe that CGAN+DS on the HFM dataset suffers the least from mode-collapse when generating data from class 1. The poorer diversity observed in class 3 implies that its generator became more focused on the conditional component of the input than on the random variable. It is worthwhile to note the correlation of the results in <xref rid="T3" ref-type="table">Table III</xref> and <xref rid="T3" ref-type="table">Table IV</xref>. We observe that the most diverse scenarios are the ones that correspond to the most representative synthetic data. Such a finding supports the notion that encouraging diversity can be advantageous.</p></sec><sec id="S40"><label>B</label><title>Effect of Data Augmentation - Results</title><sec id="S41"><title>Augmentation Methods</title><p id="P45">GAN-based data augmentation can improve classification performance relative to a balanced sub-sampled baseline by up to 29% as illustrated in <xref rid="F4" ref-type="fig">Fig. 4</xref>. The absolute classification performance before and after augmentation can be found in Section-VII of the Supplementary Material.</p><p id="P46">Firstly, we observe that the ranking of the three GAN-based methods are consistent across the four datasets, with CGAN+DS outperforming the others (<italic toggle="yes">p</italic> &#x0003c; 0.05). Such consistency is promising and is indicative of the robustness of these models. We explain the relatively poorer behaviour of the remaining GANs by noting the potential limitations of artificially inducing interclass diversity when originally present to a minimal extent. Furthermore, for three of the four datasets (HFM, Tetanus, and CVD), our GAN-based data augmentation outperforms that of SpecAugment in a statistically significant manner (<italic toggle="yes">p</italic> &#x0003c; 0.05). On the Physionet dataset, the difference between the SpecAugment and CGAN+DS results are not statistically significant. We attribute the strong performance of the GANs to their ability to generate representative and sufficiently diverse synthetic data. When performance is relatively worse than SpecAugment, as in the case of Physionet, we attribute this to the high degree of noise present within the dataset and also to the inability of the GANs to generate realistic datapoints (see <xref rid="SD2" ref-type="supplementary-material">Section-II of Supplementary Material</xref>). Lastly, the CGAN+DS and MADGAN appear to produce more consistent outcomes across datasets. This increased reliability may be a positive trait among practitioners.</p></sec><sec id="S42"><title>Training Set Imbalance</title><p id="P47">For all datasets except Physionet, we were somewhat surprised to observe that there is no significant difference in the results generated by balanced and unbalanced training sets. This could be explained by certain synthetic classes being less diverse and informative than others, a finding supported by the intraclass diversity plots. Therefore, more samples from only that class would be needed to improve performance.</p></sec><sec id="S43"><title>Ratio of Synthetic to Real Data</title><p id="P48">After performing an ANOVA and a Kruskal-Wallis test, we observe that there is no significant difference between the results generated by a variety of synthetic to real data ratios. This implies that the utility of the synthetic data is limited, at least for the range of ratios chosen. The improvement in classification performance, however, indicates that only a small amount of synthetic data can have a strong positive impact. Such a finding was most prominent for the Physionet dataset.</p></sec><sec id="S44"><title>Class Imbalance</title><p id="P49">On the HFM dataset, we observe that the classification improvement caused by introducing an imbalance in class 1 significantly outperforms (<italic toggle="yes">p</italic> &#x0003c; 0.05) that when imbalances are introduced in other classes. Anticipating such a potential outcome, based on work in [<xref rid="R59" ref-type="bibr">59</xref>], the CGANs were trained with a balanced dataset to avoid class favouritism. Nevertheless, this effect is still observed and can be partly explained by the relatively strong class 1 cMMD values relative to the others as seen in <xref rid="T3" ref-type="table">Table III</xref>. This phenomenon, however, is not observed with the other datasets. The figures associated with the aforementioned hypotheses can be found in <xref rid="SD5" ref-type="supplementary-material">Section-V of the Supplementary Material</xref>.</p><p id="P50">Data augmentation, although sometimes beneficial, can also be detrimental. To better understand the potential improvement <italic toggle="yes">and</italic> worsening of classification due to data augmentation, we illustrate our novel synthetic generalization curve in <xref rid="F5" ref-type="fig">Fig. 5</xref>. Analagous to an ROC curve, the higher it is, the stronger the outcome. Moreover, increased mass when <italic toggle="yes">&#x003b5;</italic> &#x0003c; 0 is indicative of classification improvement relative to the chosen baseline. For instance, the black dot indicates that when using MADGAN to augment the dataset, 40% of the classification models on average perform equivalent to or better than 1.10 of the baseline performance. We also observe that all methods are upper-bounded by the MADGAN method, indicating the latter&#x02019;s superiority. The synthetic generalization curves for the remaining datasets can be found in Section-VI of the Supplementary Material.</p><p id="P51">Building on the analogy to the ROC, we introduce the AUSGC values in <xref rid="T5" ref-type="table">Table V</xref>. Given the range of values chosen for epsilon, the closer the AUSGC is to 1, the better the conditional GAN is in improving classification. Moreover, the smaller the standard deviation, the more consistent the conitional GAN is across the chosen augmentation policies. In other words, it is not producing highly varying behaviour. Ultimately, no statistical difference was found between the AUSGC values of the various augmentation methods. Nonetheless, we would like to emphasize that although we have used AUROC as the comparative performance metric in (9), this curve is inherently <italic toggle="yes">metric agnostic</italic>; i.e., one can use any performance metric. This allows researchers to choose their metric of interest based on the task at hand.</p></sec></sec></sec><sec sec-type="conclusions" id="S45"><label>VII</label><title>Conclusion</title><p id="P52">Challenges posed by insufficient medical time-series data which are class-imbalanced can limit the potential of clinical decision support algorithms. To overcome such challenges, we modify and compare various conditional generative adversarial networks in their ability to synthesize pathological photoplethysmogram data. If researchers are solely aiming to generate the most realistic PPG data, then we recommend the DeLiGAN+DS and MADGAN methods in light of the relatively lower maximum mean discrepancy and L2 distance values. If, however, researchers are <italic toggle="yes">also</italic> aiming to boost the performance of their classification task, then we recommend CGAN+DS. Using this method, we show a statistically significant improvement of the AUROC by up to 29%.</p><p id="P53">For researchers working with time-series in low-data regimes, our proposed models offer them an opportunity to expand their dataset and improve their classification performance. Unfortunately, in pursuit of rules of thumb for augmentation policies, we were unable to find significant patterns. Future work would involve the evaluation of such augmentation methods on more complex neural networks and the <italic toggle="yes">simultaneous</italic> generation of different pathological medical time-series data. Also, the merger of generative modelling with self-supervised learning can be leveraged to obtain clinically acceptable classification performance.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material id="SD1" position="float" content-type="local-data"><label>supp1-2979608</label><media xlink:href="EMS208285-supplement-supp1_2979608.mp4" id="d67e1894" position="anchor" mimetype="video" mime-subtype="mp4"/></supplementary-material><supplementary-material id="SD2" position="float" content-type="local-data"><label>supp2-2979608</label><media xlink:href="EMS208285-supplement-supp2_2979608.mp4" id="d67e1897" position="anchor" mimetype="video" mime-subtype="mp4"/></supplementary-material><supplementary-material id="SD3" position="float" content-type="local-data"><label>supp3-2979608</label><media xlink:href="EMS208285-supplement-supp3_2979608.mp4" id="d67e1900" position="anchor" mimetype="video" mime-subtype="mp4"/></supplementary-material><supplementary-material id="SD4" position="float" content-type="local-data"><label>supp4-2979608</label><media xlink:href="EMS208285-supplement-supp4_2979608.pdf" id="d67e1903" position="anchor"/></supplementary-material><supplementary-material id="SD5" position="float" content-type="local-data"><label>supp5-2979608</label><media xlink:href="EMS208285-supplement-supp5_2979608.xlsx" id="d67e1906" position="anchor"/></supplementary-material></sec></body><back><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jeni</surname><given-names>LA</given-names></name>
<name><surname>Cohn</surname><given-names>JF</given-names></name>
<name><surname>De La Torre</surname><given-names>F</given-names></name>
</person-group><source>Facing imbalanced data&#x02013;recommendations for the use of performance metrics</source><conf-name>Proc Humaine Assoc Conf Affect Comput Intell Interact</conf-name><year>2013</year><fpage>245</fpage><lpage>251</lpage></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mazurowski</surname><given-names>MA</given-names></name>
<name><surname>Habas</surname><given-names>PA</given-names></name>
<name><surname>Zurada</surname><given-names>JM</given-names></name>
<name><surname>Lo</surname><given-names>JY</given-names></name>
<name><surname>Baker</surname><given-names>JA</given-names></name>
<name><surname>Tourassi</surname><given-names>GD</given-names></name>
</person-group><article-title>Training neural network classifiers for medical decision making: The effects of imbalanced datasets on classification performance</article-title><source>Neural Netw</source><year>2008</year><volume>21</volume><issue>2</issue><fpage>427</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2007.12.031</pub-id><!--<pub-id pub-id-type="pmcid">PMC2346433</pub-id>--><pub-id pub-id-type="pmid">18272329</pub-id>
</element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Elgendi</surname><given-names>M</given-names></name>
<name><surname>Liang</surname><given-names>Y</given-names></name>
<name><surname>Ward</surname><given-names>R</given-names></name>
</person-group><article-title>Toward generating more diagnostic features from photoplethysmogram waveforms</article-title><source>Diseases</source><year>2018</year><volume>6</volume><issue>1</issue><fpage>20</fpage><pub-id pub-id-type="doi">10.3390/diseases6010020</pub-id><!--<pub-id pub-id-type="pmcid">PMC5871966</pub-id>--><pub-id pub-id-type="pmid">29534495</pub-id>
</element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Marcus</surname><given-names>G</given-names></name>
</person-group><source>Deep learning: A critical appraisal</source><year>2018</year><elocation-id>arXiv:1801.00631</elocation-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J</given-names></name>
<name><surname>Perez</surname><given-names>L</given-names></name>
</person-group><article-title>The effectiveness of data augmentation in image classification using deep learning</article-title><source>Convolutional Neural Networks Vis Recognit</source><year>2017</year></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Russakovsky</surname><given-names>O</given-names></name>
<etal/>
</person-group><article-title>Imagenet large scale visual recognition challenge</article-title><source>Int J Comput Vision</source><year>2015</year><volume>115</volume><issue>3</issue><fpage>211</fpage><lpage>252</lpage></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ronneberger</surname><given-names>O</given-names></name>
<name><surname>Fischer</surname><given-names>P</given-names></name>
<name><surname>Brox</surname><given-names>T</given-names></name>
</person-group><source>U-net: Convolutional networks for biomedical image segmentation</source><conf-name>Proc Int Conf Med Image Comput Comput-Assisted Intervention</conf-name><year>2015</year><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Frid-Adar</surname><given-names>M</given-names></name>
<name><surname>Klang</surname><given-names>E</given-names></name>
<name><surname>Amitai</surname><given-names>M</given-names></name>
<name><surname>Goldberger</surname><given-names>J</given-names></name>
<name><surname>Greenspan</surname><given-names>H</given-names></name>
</person-group><source>Synthetic data augmentation using GAN for improved liver lesion classification</source><conf-name>Proc IEEE 15th Int Symp Biomed Imag</conf-name><year>2018</year><fpage>289</fpage><lpage>293</lpage></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Um</surname><given-names>TT</given-names></name>
<etal/>
</person-group><source>Data augmentation of wearable sensor data for parkinson&#x02019;s disease monitoring using convolutional neural networks</source><conf-name>Proc 19th ACM Int Conf Multimodal Interact</conf-name><year>2017</year><fpage>216</fpage><lpage>220</lpage></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Antoniou</surname><given-names>A</given-names></name>
<name><surname>Storkey</surname><given-names>A</given-names></name>
<name><surname>Edwards</surname><given-names>H</given-names></name>
</person-group><source>Data augmentation generative adversarial networks</source><year>2017</year><month>Nov</month><elocation-id>arXiv:1711.04340</elocation-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mirza</surname><given-names>M</given-names></name>
<name><surname>Osindero</surname><given-names>S</given-names></name>
</person-group><source>Conditional generative adversarial nets</source><year>2014</year><month>Nov</month><elocation-id>arXiv:1411.1784</elocation-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gurumurthy</surname><given-names>S</given-names></name>
<name><surname>Kiran Sarvadevabhatla</surname><given-names>R</given-names></name>
<name><surname>Venkatesh Babu</surname><given-names>R</given-names></name>
</person-group><source>Deligan: Generative adversarial networks for diverse and limited data</source><conf-name>Proc IEEE Conf Comput Vision Pattern Recognit</conf-name><year>2017</year><fpage>166</fpage><lpage>174</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>A</given-names></name>
<name><surname>Kulharia</surname><given-names>V</given-names></name>
<name><surname>Namboodiri</surname><given-names>VP</given-names></name>
<name><surname>Torr</surname><given-names>PH</given-names></name>
<name><surname>Dokania</surname><given-names>PK</given-names></name>
</person-group><source>Multi-agent diverse generative adversarial networks</source><conf-name>Proc IEEE Conf Comput Vision Pattern Recognit</conf-name><year>2018</year><fpage>8513</fpage><lpage>8521</lpage></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Goodfellow</surname><given-names>IJ</given-names></name>
<etal/>
</person-group><source>Generative adversarial networks</source><conf-name>Proc Adv Neural Inf Process Syst</conf-name><year>2014</year><fpage>2672</fpage><lpage>2680</lpage></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Salehinejad</surname><given-names>H</given-names></name>
<name><surname>Colak</surname><given-names>E</given-names></name>
<name><surname>Dowdell</surname><given-names>T</given-names></name>
<name><surname>Barfett</surname><given-names>J</given-names></name>
<name><surname>Valaee</surname><given-names>S</given-names></name>
</person-group><article-title>Synthesizing chest x-ray pathology for training deep convolutional neural networks</article-title><source>IEEE Trans Med Imag</source><year>2019</year><month>May</month><volume>38</volume><issue>5</issue><fpage>1197</fpage><lpage>1206</lpage><pub-id pub-id-type="pmid">30442603</pub-id>
</element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yi</surname><given-names>X</given-names></name>
<name><surname>Walia</surname><given-names>E</given-names></name>
<name><surname>Babyn</surname><given-names>P</given-names></name>
</person-group><article-title>Generative adversarial network in medical imaging: A review</article-title><source>Med Image Anal</source><year>2019</year><volume>58</volume><comment>Art. no. 101552</comment><pub-id pub-id-type="pmid">31521965</pub-id>
</element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>Y</given-names></name>
<name><surname>Wang</surname><given-names>Y</given-names></name>
<name><surname>Kirschen</surname><given-names>D</given-names></name>
<name><surname>Zhang</surname><given-names>B</given-names></name>
</person-group><article-title>Model-free renewable scenario generation using generative adversarial networks</article-title><source>IEEE Trans Power Syst</source><year>2018</year><month>May</month><volume>33</volume><fpage>3265</fpage><lpage>3275</lpage></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>F</given-names></name>
<name><surname>Zhong</surname><given-names>S-H</given-names></name>
<name><surname>Peng</surname><given-names>J</given-names></name>
<name><surname>Jiang</surname><given-names>J</given-names></name>
<name><surname>Liu</surname><given-names>Y</given-names></name>
</person-group><part-title>Data augmentation for EEG-based emotion recognition with deep convolutional neural network</part-title><person-group person-group-type="editor">
<name><surname>Schoeffmann</surname><given-names>K</given-names></name>
<name><surname>Chalidabhongse</surname><given-names>TH</given-names></name>
<name><surname>Ngo</surname><given-names>CW</given-names></name>
<name><surname>Aramvith</surname><given-names>S</given-names></name>
<name><surname>Ho</surname><given-names>Y-S</given-names></name>
<name><surname>Gabbouj</surname><given-names>M</given-names></name>
<name><surname>Elgammal</surname><given-names>A</given-names></name>
</person-group><source>MultiMedia Modeling</source><year>2018</year><fpage>82</fpage><lpage>93</lpage><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Dong</surname><given-names>H-W</given-names></name>
<name><surname>Hsiao</surname><given-names>W-Y</given-names></name>
<name><surname>Yang</surname><given-names>L-C</given-names></name>
<name><surname>Yang</surname><given-names>Y-H</given-names></name>
</person-group><source>Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment</source><conf-name>Proc 32nd AAAI Conf Artif Intell</conf-name><year>2018</year></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gupta</surname><given-names>A</given-names></name>
<name><surname>Johnson</surname><given-names>J</given-names></name>
<name><surname>Fei-Fei</surname><given-names>L</given-names></name>
<name><surname>Savarese</surname><given-names>S</given-names></name>
<name><surname>Alahi</surname><given-names>A</given-names></name>
</person-group><source>Social GAN: Socially acceptable trajectories with generative adversarial networks</source><conf-name>Proc IEEE Conf Comput Vision Pattern Recognit</conf-name><year>2018</year><fpage>2255</fpage><lpage>2264</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gregor Hartmann</surname><given-names>K</given-names></name>
<name><surname>Tibor Schirrmeister</surname><given-names>R</given-names></name>
<name><surname>Ball</surname><given-names>T</given-names></name>
</person-group><source>EEG-GAN: Generative adversarial networks for electroencephalograhic (EEG) brain signals</source><year>2018</year><month>Jun</month><elocation-id>arXiv:1806.01875</elocation-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pascual</surname><given-names>D</given-names></name>
<name><surname>Aminifar</surname><given-names>A</given-names></name>
<name><surname>Atienza</surname><given-names>D</given-names></name>
<name><surname>Ryvlin</surname><given-names>P</given-names></name>
<name><surname>Wattenhofer</surname><given-names>R</given-names></name>
</person-group><source>Synthetic epileptic brain activities using generative adversarial networks</source><year>2019</year><elocation-id>arXiv:1907.10518</elocation-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Aznan</surname><given-names>NKN</given-names></name>
<name><surname>Atapour-Abarghouei</surname><given-names>A</given-names></name>
<name><surname>Bonner</surname><given-names>S</given-names></name>
<name><surname>Connolly</surname><given-names>J</given-names></name>
<name><surname>Moubayed</surname><given-names>NA</given-names></name>
<name><surname>Breckon</surname><given-names>T</given-names></name>
</person-group><source>Simulating brain signals: Creating synthetic EEG data via neural-based generative models for improved SSVEP classification</source><conf-name>Proc Int Joint Conf Neural Netw</conf-name><year>2019</year><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Severo</surname><given-names>D</given-names></name>
<name><surname>Amaro</surname><given-names>F</given-names></name>
<name><surname>Hruschka</surname><given-names>ER</given-names><suffix>Jr</suffix></name>
<name><surname>Costa</surname><given-names>ASdM</given-names></name>
</person-group><source>Ward2icu: A vital signs dataset of inpatients from the general ward</source><year>2019</year><elocation-id>arXiv:1910.00752</elocation-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brophy</surname><given-names>E</given-names></name>
<name><surname>Wang</surname><given-names>Z</given-names></name>
<name><surname>Ward</surname><given-names>TE</given-names></name>
</person-group><source>Quick and easy time series generation with established image-based GANs</source><year>2019</year><elocation-id>arXiv:1806.07108</elocation-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Q</given-names></name>
<name><surname>Liu</surname><given-names>Y</given-names></name>
</person-group><source>Improving brain computer interface performance by data augmentation with conditional deep convolutional generative adversarial networks</source><year>2018</year><elocation-id>arXiv:1806.07108</elocation-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Esteban</surname><given-names>C</given-names></name>
<name><surname>Hyland</surname><given-names>SL</given-names></name>
<name><surname>R&#x000e4;tsch</surname><given-names>G</given-names></name>
</person-group><source>Real-valued (Medical) time series generation with recurrent conditional GANs</source><year>2017</year><month>Jun</month><elocation-id>arXiv:1706.02633</elocation-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>D</given-names></name>
<name><surname>Hong</surname><given-names>S</given-names></name>
<name><surname>Jang</surname><given-names>Y</given-names></name>
<name><surname>Zhao</surname><given-names>T</given-names></name>
<name><surname>Lee</surname><given-names>H</given-names></name>
</person-group><source>Diversity-sensitive conditional generative adversarial networks</source><year>2019</year><month>Jan</month><elocation-id>arXiv:1901.09024</elocation-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Krizhevsky</surname><given-names>A</given-names></name>
<name><surname>Sutskever</surname><given-names>I</given-names></name>
<name><surname>Hinton</surname><given-names>GE</given-names></name>
</person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Commun ACM</source><year>2017</year><month>May</month><volume>60</volume><fpage>84</fpage><lpage>90</lpage></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Z</given-names></name>
<name><surname>Qu</surname><given-names>Y</given-names></name>
<name><surname>Tao</surname><given-names>J</given-names></name>
<name><surname>Song</surname><given-names>Y</given-names></name>
</person-group><source>Image-mediated data augmentation for low-resource human activity recognition</source><conf-name>Proc 3rd Int Conf Comput Data Anal</conf-name><year>2019</year><fpage>49</fpage><lpage>54</lpage><conf-loc>New York, NY, USA</conf-loc></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>DS</given-names></name>
<etal/>
</person-group><source>SpecAugment: A simple data augmentation method for automatic speech recognition</source><conf-name>Proc Interspeech</conf-name><year>2019</year><month>Apr</month><fpage>2613</fpage><lpage>2617</lpage><pub-id pub-id-type="doi">10.21437/Interspeech.2019-2680</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Guennec</surname><given-names>AL</given-names></name>
<name><surname>Malinowski</surname><given-names>S</given-names></name>
<name><surname>Tavenard</surname><given-names>R</given-names></name>
</person-group><source>Data augmentation for time series classification using convolutional neural networks</source><conf-name>Proc ECML/PKDD Workshop Adv Anal Learn Temporal Data</conf-name><year>2016</year><month>Sep</month><conf-loc>Riva Del Garda, Italy</conf-loc></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fawaz</surname><given-names>HI</given-names></name>
<name><surname>Forestier</surname><given-names>G</given-names></name>
<name><surname>Weber</surname><given-names>J</given-names></name>
<name><surname>Idoumghar</surname><given-names>L</given-names></name>
<name><surname>Muller</surname><given-names>P-A</given-names></name>
</person-group><source>Data augmentation using synthetic data for time series classification with deep residual networks</source><conf-name>Proc Int Workshop Adv Anal Learn Temporal Data ECML PKDD</conf-name><year>2018</year></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Thickstun</surname><given-names>J</given-names></name>
<name><surname>Harchaoui</surname><given-names>Z</given-names></name>
<name><surname>Foster</surname><given-names>D</given-names></name>
<name><surname>Kakade</surname><given-names>SM</given-names></name>
</person-group><source>Invariances and data augmentation for supervised music transcription</source><conf-name>Proc IEEE Int Conf Acoust, Speech Signal Process</conf-name><year>2018</year><fpage>2241</fpage><lpage>2245</lpage></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>McFee</surname><given-names>B</given-names></name>
<name><surname>Humphrey</surname><given-names>EJ</given-names></name>
<name><surname>Bello</surname><given-names>JP</given-names></name>
</person-group><source>A software framework for musical data augmentation</source><conf-name>Proc 16th Int Soc Music Inf Retr Conf</conf-name><year>2015</year><fpage>248</fpage><lpage>254</lpage></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mauch</surname><given-names>M</given-names></name>
<name><surname>Ewert</surname><given-names>S</given-names></name>
</person-group><source>The audio degradation toolbox and its application to robustness evaluation</source><conf-name>Proc Int Soc Music Inf Retr Conf</conf-name><year>2013</year><fpage>83</fpage><lpage>88</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>DeVries</surname><given-names>T</given-names></name>
<name><surname>Taylor</surname><given-names>GW</given-names></name>
</person-group><source>Dataset Augmentation in Feature Space</source><conf-name>Proc Int Conf Learn Representations</conf-name><year>2017</year><pub-id pub-id-type="arxiv">1702.05538</pub-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Oh</surname><given-names>J</given-names></name>
<name><surname>Wang</surname><given-names>J</given-names></name>
<name><surname>Wiens</surname><given-names>J</given-names></name>
</person-group><source>Learning to exploit invariances in clinical time-series data using sequence transformer networks</source><conf-name>Proc 3rd Mach Learn Healthcare Conf</conf-name><year>2018</year><conf-date>17&#x02013;18</conf-date><fpage>332</fpage><lpage>347</lpage><conf-loc>Palo Alto, CA, USA</conf-loc></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Salimans</surname><given-names>T</given-names></name>
<name><surname>Goodfellow</surname><given-names>I</given-names></name>
<name><surname>Zaremba</surname><given-names>W</given-names></name>
<name><surname>Cheung</surname><given-names>V</given-names></name>
<name><surname>Radford</surname><given-names>A</given-names></name>
<name><surname>Chen</surname><given-names>X</given-names></name>
</person-group><source>Improved techniques for training GANs</source><conf-name>Proc Adv Neural Inf Process Sys</conf-name><year>2016</year><fpage>2234</fpage><lpage>2242</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Arjovsky</surname><given-names>M</given-names></name>
<name><surname>Chintala</surname><given-names>S</given-names></name>
<name><surname>Bottou</surname><given-names>L</given-names></name>
</person-group><source>Wasserstein generative adversarial networks</source><conf-name>Proc Int Conf Mach Learn</conf-name><year>2017</year><fpage>214</fpage><lpage>223</lpage></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Thanh-Tung</surname><given-names>H</given-names></name>
<name><surname>Tran</surname><given-names>T</given-names></name>
<name><surname>Venkatesh</surname><given-names>S</given-names></name>
</person-group><source>Improving generalization and stability of generative adversarial networks</source><conf-name>Proc Int Conf Learn Representations</conf-name><year>2019</year></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Borji</surname><given-names>A</given-names></name>
</person-group><article-title>Pros and cons of GAN evaluation measures</article-title><source>Comput Vision Image Understanding</source><year>2019</year><volume>179</volume><fpage>41</fpage><lpage>65</lpage></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gretton</surname><given-names>A</given-names></name>
<name><surname>Borgwardt</surname><given-names>KM</given-names></name>
<name><surname>Rasch</surname><given-names>MJ</given-names></name>
<name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name>
<name><surname>Smola</surname><given-names>A</given-names></name>
</person-group><article-title>A kernel two-sample test</article-title><source>J Mach Learn Res</source><year>2012</year><month>Mar</month><volume>13</volume><fpage>723</fpage><lpage>773</lpage></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tran</surname><given-names>T</given-names></name>
<name><surname>Pham</surname><given-names>T</given-names></name>
<name><surname>Carneiro</surname><given-names>G</given-names></name>
<name><surname>Palmer</surname><given-names>L</given-names></name>
<name><surname>Reid</surname><given-names>I</given-names></name>
</person-group><source>A bayesian data augmentation approach for learning deep models</source><conf-name>Proc Advances Neural Inf Process Syst</conf-name><year>2017</year><fpage>2797</fpage><lpage>2806</lpage></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Khanh</surname><given-names>TH</given-names></name>
<etal/>
</person-group><article-title>Enterovirus 71-associated hand, foot, and mouth disease, Southern Vietnam, 2011</article-title><source>Emerg Infectious Diseases</source><year>2012</year><month>Dec</month><volume>18</volume><fpage>2002</fpage><lpage>2005</lpage><pub-id pub-id-type="doi">10.3201/eid1812.120929</pub-id><!--<pub-id pub-id-type="pmcid">PMC3557876</pub-id>--><pub-id pub-id-type="pmid">23194699</pub-id>
</element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hoang</surname><given-names>MTV</given-names></name>
<etal/>
</person-group><article-title>Clinical and aetiological study of hand, foot and mouth disease in Southern Vietnam, 2013&#x02013;2015: Inpatients and outpatients</article-title><source>Int J Infectious Diseases</source><year>2019</year><volume>80</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.ijid.2018.12.004</pub-id><!--<pub-id pub-id-type="pmcid">PMC6403263</pub-id>--><pub-id pub-id-type="pmid">30550944</pub-id>
</element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ablett</surname><given-names>J</given-names></name>
</person-group><source>Analysis and main experience in 82 patients treated in leeds tetanus unit</source><conf-name>Proc Symp Tetanus Great Britain</conf-name><year>1967</year><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liang</surname><given-names>Y</given-names></name>
<name><surname>Chen</surname><given-names>Z</given-names></name>
<name><surname>Liu</surname><given-names>G</given-names></name>
<name><surname>Elgendi</surname><given-names>M</given-names></name>
</person-group><article-title>A new, short-recorded photoplethysmogram dataset for blood pressure monitoring in china</article-title><source>Scientific Data</source><year>2018</year><volume>5</volume><comment>Art. no. 180020</comment><pub-id pub-id-type="doi">10.1038/sdata.2018.20</pub-id><!--<pub-id pub-id-type="pmcid">PMC5827692</pub-id>--><pub-id pub-id-type="pmid">29485624</pub-id>
</element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liang</surname><given-names>Y</given-names></name>
<name><surname>Liu</surname><given-names>G</given-names></name>
<name><surname>Chen</surname><given-names>Z</given-names></name>
<name><surname>Elgendi</surname><given-names>M</given-names></name>
</person-group><source>PPG-BP database</source><year>2018</year></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Goldberger</surname><given-names>AL</given-names></name>
<etal/>
</person-group><article-title>Physiobank, physiotoolkit, and physionet: Components of a new research resource for complex physiologic signals</article-title><source>Circulation</source><year>2000</year><volume>101</volume><issue>23</issue><fpage>e215</fpage><lpage>e220</lpage><pub-id pub-id-type="pmid">10851218</pub-id>
</element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><collab>T. F. of the European Society of Cardiology, the North American Society of Pacing, and Electrophysiology</collab><article-title>Heart rate variability: Standards of measurement, physiological interpretation and clinical use</article-title><source>Circulation</source><year>1996</year><volume>93</volume><fpage>1043</fpage><lpage>1065</lpage><pub-id pub-id-type="pmid">8598068</pub-id>
</element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Bellman</surname><given-names>RE</given-names></name>
</person-group><source>Adaptive Control Processes: A Guided Tour</source><year>1961</year><publisher-name>Princeton Univ. Press</publisher-name><publisher-loc>Princeton, NJ, USA</publisher-loc></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>Z</given-names></name>
<name><surname>Khetan</surname><given-names>A</given-names></name>
<name><surname>Fanti</surname><given-names>G</given-names></name>
<name><surname>Oh</surname><given-names>S</given-names></name>
</person-group><source>PacGAN: The power of two samples in generative adversarial networks</source><conf-name>Proc Advances Neural Inf Process Syst</conf-name><year>2018</year><fpage>1498</fpage><lpage>1507</lpage></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Donahue</surname><given-names>C</given-names></name>
<name><surname>McAuley</surname><given-names>J</given-names></name>
<name><surname>Puckette</surname><given-names>M</given-names></name>
</person-group><source>Adversarial audio synthesis</source><year>2018</year><month>Feb</month><elocation-id>arXiv:1802.04208</elocation-id></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Charlton</surname><given-names>PH</given-names></name>
<etal/>
</person-group><article-title>Extraction of respiratory signals from the electro-cardiogram and photoplethysmogram: Technical and physiological determinants</article-title><source>Physiological Meas</source><year>2017</year><volume>38</volume><issue>5</issue><fpage>669</fpage><lpage>690</lpage><pub-id pub-id-type="pmid">28296645</pub-id>
</element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>van der Walt</surname><given-names>CM</given-names></name>
<name><surname>Barnard</surname><given-names>E</given-names></name>
</person-group><article-title>Data characteristics that determine classifier performance</article-title><source>SAIEE Africa Research J</source><year>2006</year><volume>98</volume><issue>3</issue><fpage>87</fpage><lpage>93</lpage></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pinto</surname><given-names>N</given-names></name>
<name><surname>Cox</surname><given-names>DD</given-names></name>
<name><surname>DiCarlo</surname><given-names>JJ</given-names></name>
</person-group><article-title>Why is real-world visual object recognition hard?</article-title><source>PLoS Comput Biol</source><year>2008</year><volume>4</volume><issue>1</issue><fpage>e27</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.0040027</pub-id><!--<pub-id pub-id-type="pmcid">PMC2211529</pub-id>--><pub-id pub-id-type="pmid">18225950</pub-id>
</element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Maaten</surname><given-names>Lvd</given-names></name>
<name><surname>Hinton</surname><given-names>G</given-names></name>
</person-group><article-title>Visualizing data using T-SNE</article-title><source>J Mach Learn Res</source><year>2008</year><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mariani</surname><given-names>G</given-names></name>
<name><surname>Scheidegger</surname><given-names>F</given-names></name>
<name><surname>Istrate</surname><given-names>R</given-names></name>
<name><surname>Bekas</surname><given-names>C</given-names></name>
<name><surname>Malossi</surname><given-names>C</given-names></name>
</person-group><article-title>BaGAN: Data augmentation with balancing GAN</article-title><source>CoRR</source><year>2018</year><elocation-id>abs/1803.09655</elocation-id><comment>[Online]. Available: <ext-link xlink:href="http://arxiv.org/abs/1803.09655" ext-link-type="uri">http://arxiv.org/abs/1803.09655</ext-link></comment></element-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Fig. 1</label><caption><title>Illustration of Pipeline. Synthetic data generated by the three different CGAN models are used to augment the original dataset for a 3-way classification problem.</title></caption><graphic xlink:href="EMS208285-f001" position="float"/></fig><fig position="float" id="F2"><label>Fig. 2</label><caption><p>Randomly sampled class-specific real (HFM) and synthetic PPG data generated by each of the CGAN models. Samples are 5 s in duration. Note the ability of the CGANs to capture respiratory sinus arrythmia-induced amplitude modulation.</p></caption><graphic xlink:href="EMS208285-f002" position="float"/></fig><fig position="float" id="F3"><label>Fig. 3</label><caption><p>Lower triangular exponentiated quadratic kernel matrices representing the intraclass similarity of 30 randomly sampled synthetic datapoints generated by the three different CGANs (columns) for each of the three classes of HFM (rows). Results are shown for one seed.</p></caption><graphic xlink:href="EMS208285-f003" position="float"/></fig><fig position="float" id="F4"><label>Fig. 4</label><caption><title>Average best percent change in AUROC as a function of the different augmentation methods used on each dataset. Error bars represent one standard error.</title></caption><graphic xlink:href="EMS208285-f004" position="float"/></fig><fig position="float" id="F5"><label>Fig. 5</label><caption><p>Synthetic generalization curve averaged across all 54 augmentation policies for each augmentation method when tested on the CVD dataset. Shaded area represents one standard deviation from the mean.</p></caption><graphic xlink:href="EMS208285-f005" position="float"/></fig><table-wrap position="float" id="T1"><label>Table I</label><caption><title>Dataset-Specific Cross-Validation Summary</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="center" style="border-top: 2px solid #000000" rowspan="1" colspan="1">Dataset</th><th valign="top" align="center" style="border-top: 2px solid #000000" rowspan="1" colspan="1">nFolds</th><th valign="top" align="center" style="border-top: 2px solid #000000" rowspan="1" colspan="1">Input Dimensionality</th><th valign="top" align="center" style="border-top: 2px solid #000000" rowspan="1" colspan="1">Class Ratios</th></tr></thead><tbody><tr><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">HFM - Vietnam</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">16</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1980 x 50</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1.35 : 1.04 : 1</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">Tetanus - Vietnam</td><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">5978 x 50</td><td valign="top" align="center" rowspan="1" colspan="1">1 : 4.75 : 2.76</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">CVD - China [<xref rid="R48" ref-type="bibr">48</xref>]</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">219 x 50</td><td valign="top" align="center" rowspan="1" colspan="1">8.25 : 1.7 : 1</td></tr><tr><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">Physionet [<xref rid="R50" ref-type="bibr">50</xref>]</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">7</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">2202 x 50</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">4.49 : 3.64 : 1</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T2"><label>Table II</label><caption><title>Network Architecture Common to All 3 CGAN Models</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">Network</th><th valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">Layer</th><th valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">Dimension</th><th valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">Activation</th></tr></thead><tbody><tr><td valign="middle" align="center" rowspan="6" style="border-top: 1.5px solid #000000" colspan="1">Generator</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">Input</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">50</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">-</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC1</td><td valign="top" align="center" rowspan="1" colspan="1">100</td><td valign="top" align="center" rowspan="1" colspan="1">tanh</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC2</td><td valign="top" align="center" rowspan="1" colspan="1">200</td><td valign="top" align="center" rowspan="1" colspan="1">tanh</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC3</td><td valign="top" align="center" rowspan="1" colspan="1">400</td><td valign="top" align="center" rowspan="1" colspan="1">tanh</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC4</td><td valign="top" align="center" rowspan="1" colspan="1">500</td><td valign="top" align="center" rowspan="1" colspan="1">tanh</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">Conv1d</td><td valign="top" align="center" rowspan="1" colspan="1">500</td><td valign="top" align="center" rowspan="1" colspan="1">-</td></tr><tr><td valign="middle" align="center" rowspan="7" style="border-top: 1.5px solid #000000;border-bottom: 2px solid #000000" colspan="1">Discriminator</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">Input</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">500*<sup><xref rid="TFN1" ref-type="table-fn">p</xref></sup></td><td valign="top" align="center" style="border-top:solid 1.5px #000000" rowspan="1" colspan="1">-</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC1</td><td valign="top" align="center" rowspan="1" colspan="1">400</td><td valign="top" align="center" rowspan="1" colspan="1">tanh</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC2</td><td valign="top" align="center" rowspan="1" colspan="1">200</td><td valign="top" align="center" rowspan="1" colspan="1">tanh</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC3</td><td valign="top" align="center" rowspan="1" colspan="1">100</td><td valign="top" align="center" rowspan="1" colspan="1">tanh</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC4</td><td valign="top" align="center" rowspan="1" colspan="1">50</td><td valign="top" align="center" rowspan="1" colspan="1">tanh</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">FC5a</td><td valign="top" align="center" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">-</td></tr><tr><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">FC5b</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">3</td><td valign="top" align="center" style="border-bottom:solid 2px" rowspan="1" colspan="1">-</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><label>p</label><p id="P54">represents the packing degree introduced in [<xref rid="R53" ref-type="bibr">53</xref>]. FC and Conv1d represent Fully Connected and 1d Convolution operations, respectively.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="T3"><label>Table III</label><caption><title>Average Maximum Mean Discrepancy of Synthetic Data</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">Dataset</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">Class</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">CGAN+DS</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">DeLiGAN+DS</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">MADGAN</th></tr></thead><tbody><tr><td valign="middle" align="center" rowspan="4" style="border-top: 1.5px solid #000000" colspan="1">HFM</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">
<bold>0.84&#x000b1;0.089</bold>
</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.87&#x000b1;0.066</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.89&#x000b1;0.089</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.85&#x000b1;0.085</td><td valign="top" align="center" rowspan="1" colspan="1">0.97&#x000b1;0.087</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.85&#x000b1;0.066</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">0.94&#x000b1;0.046</td><td valign="top" align="center" rowspan="1" colspan="1">1.03&#x000b1;0.033</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.85&#x000b1;0.034</bold>
</td></tr><tr><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">All</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">0.87&#x000b1;0.048</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">0.90&#x000b1;0.032</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">
<bold>0.69&#x000b1;0.034</bold>
</td></tr><tr><td valign="middle" align="center" rowspan="4" style="border-top: 1.5px solid #000000" colspan="1">Tetanus</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.50&#x000b1;0.040</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.52&#x000b1;0.029</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">
<bold>0.25&#x000b1;0.023</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.53&#x000b1;0.040</td><td valign="top" align="center" rowspan="1" colspan="1">0.69&#x000b1;0.022</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.36&#x000b1;0.023</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">0.50&#x000b1;0.038</td><td valign="top" align="center" rowspan="1" colspan="1">0.69&#x000b1;0.041</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.44&#x000b1;0.027</bold>
</td></tr><tr><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">All</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">0.50&#x000b1;0.017</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">0.60&#x000b1;0.031</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">
<bold>0.26&#x000b1;0.011</bold>
</td></tr><tr><td valign="middle" align="center" rowspan="4" style="border-top: 1.5px solid #000000;border-bottom: 1.5px solid #000000" colspan="1">CVD [<xref rid="R48" ref-type="bibr">48</xref>]</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.88&#x000b1;0.082</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1.05&#x000b1;0.089</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1"><bold>0.81</bold>&#x000b1;<bold>0.078</bold></td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.92&#x000b1;0.089</td><td valign="top" align="center" rowspan="1" colspan="1">1.11&#x000b1;0.120</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.89&#x000b1;0.100</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.85&#x000b1;0.086</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.87&#x000b1;0.067</td><td valign="top" align="center" rowspan="1" colspan="1">0.91&#x000b1;0.099</td></tr><tr><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">All</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">0.88&#x000b1;0.038</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">0.91&#x000b1;0.054</td><td valign="top" align="center" style="border-top: 1px solid #000000" rowspan="1" colspan="1">
<bold>0.66&#x000b1;0.034</bold>
</td></tr><tr><td valign="middle" align="center" rowspan="4" style="border-top: 1px solid #000000;border-bottom: 2px solid #000000" colspan="1">Physionet [<xref rid="R50" ref-type="bibr">50</xref>]</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.69&#x000b1;0.053</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.48&#x000b1;0.048</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">
<bold>0.40&#x000b1;0.050</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.76&#x000b1;0.043</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.51&#x000b1;0.036</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.51&#x000b1;0.040</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">0.73&#x000b1;0.060</td><td valign="top" align="center" rowspan="1" colspan="1">0.58&#x000b1;0.051</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.47&#x000b1;0.070</bold>
</td></tr><tr><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 2px solid #000000" rowspan="1" colspan="1">All</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 2px solid #000000" rowspan="1" colspan="1">0.72&#x000b1;0.031</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 2px solid #000000" rowspan="1" colspan="1">0.49&#x000b1;0.019</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 2px solid #000000" rowspan="1" colspan="1">
<bold>0.40&#x000b1;0.031</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T4"><label>Table IV</label><caption><title>Average Intraclass Similarity of Synthetic Data</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">Dataset</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">Class</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">CGAN+DS</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">DeLiGAN+DS</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">MADGAN</th></tr></thead><tbody><tr><td valign="middle" align="center" rowspan="3" style="border-top: 1.5px solid #000000" colspan="1">HFM</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">
<bold>0.46&#x000b1;0.044</bold>
</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.49&#x000b1;0.039</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.51&#x000b1;0.034</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.47&#x000b1;0.046</td><td valign="top" align="center" rowspan="1" colspan="1">0.59&#x000b1;0.043</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.47&#x000b1;0.024</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">0.55&#x000b1;0.051</td><td valign="top" align="center" rowspan="1" colspan="1">0.64&#x000b1;0.025</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.46&#x000b1;0.019</bold>
</td></tr><tr><td valign="middle" align="center" rowspan="3" style="border-top: 1.5px solid #000000" colspan="1">Tetanus</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.44&#x000b1;0.025</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.38&#x000b1;0.019</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">
<bold>0.31&#x000b1;0.021</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.45&#x000b1;0.026</td><td valign="top" align="center" rowspan="1" colspan="1">0.49&#x000b1;0.018</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.42&#x000b1;0.022</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">0.44&#x000b1;0.020</td><td valign="top" align="center" rowspan="1" colspan="1">0.50&#x000b1;0.021</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.43&#x000b1;0.036</bold>
</td></tr><tr><td valign="middle" align="center" rowspan="3" style="border-top: 1.5px solid #000000;border-bottom: 1.5px solid #000000" colspan="1">CVD [<xref rid="R48" ref-type="bibr">48</xref>]</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.53&#x000b1;0.081</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.70&#x000b1;0.089</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">
<bold>0.47&#x000b1;0.070</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.51&#x000b1;0.084</td><td valign="top" align="center" rowspan="1" colspan="1">0.70&#x000b1;0.099</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.48&#x000b1;0.081</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.43&#x000b1;0.044</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.45&#x000b1;0.045</td><td valign="top" align="center" rowspan="1" colspan="1">0.50&#x000b1;0.087</td></tr><tr><td valign="middle" align="center" rowspan="3" style="border-top: 1px solid #000000;border-bottom: 2px solid #000000" colspan="1">Physionet [<xref rid="R50" ref-type="bibr">50</xref>]</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">1</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.42&#x000b1;0.036</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.42&#x000b1;0.027</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">
<bold>0.34&#x000b1;0.027</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">0.42&#x000b1;0.033</td><td valign="top" align="center" rowspan="1" colspan="1">0.47&#x000b1;0.025</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.41&#x000b1;0.031</bold>
</td></tr><tr><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">3</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">0.42&#x000b1;0.044</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">0.50&#x000b1;0.031</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">
<bold>0.42&#x000b1;0.024</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T5"><label>Table V</label><caption><title>AUSGC Averaged Across All 54 Augmentation Policies</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">Dataset</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">CGAN+DS</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">DeLiGAN+DS</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">MADGAN</th><th valign="top" align="center" style="border-top:solid 2px #000000" rowspan="1" colspan="1">Spec Augment [<xref rid="R31" ref-type="bibr">31</xref>]</th></tr></thead><tbody><tr><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">HFM</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.511&#x000b1;0.014</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.521&#x000b1;0.014</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">
<bold>0.525&#x000b1;0.018</bold>
</td><td valign="top" align="center" style="border-top: 1.5px solid #000000" rowspan="1" colspan="1">0.516&#x000b1;0.014</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">Tetanus</td><td valign="top" align="center" rowspan="1" colspan="1">0.522&#x000b1;0.011</td><td valign="top" align="center" rowspan="1" colspan="1">0.509&#x000b1;0.012</td><td valign="top" align="center" rowspan="1" colspan="1">0.510&#x000b1;0.010</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.540&#x000b1;0.010</bold>
</td></tr><tr><td valign="top" align="center" rowspan="1" colspan="1">CVD [<xref rid="R48" ref-type="bibr">48</xref>]</td><td valign="top" align="center" rowspan="1" colspan="1">0.521&#x000b1;0.014</td><td valign="top" align="center" rowspan="1" colspan="1">0.512&#x000b1;0.015</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.534&#x000b1;0.016</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.490&#x000b1;0.021</td></tr><tr><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">Physionet [<xref rid="R50" ref-type="bibr">50</xref>]</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">0.521&#x000b1;0.013</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">0.517&#x000b1;0.012</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">0.500&#x000b1;0.012</td><td valign="top" align="center" style="border-bottom: 2px solid #000000" rowspan="1" colspan="1">
<bold>0.581 &#x000b1;0.027</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>